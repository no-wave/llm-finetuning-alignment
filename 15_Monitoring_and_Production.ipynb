{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mXZcQveIb2B4"
   },
   "source": [
    "# Chapter 15: 모니터링 및 프로덕션\n",
    "\n",
    "## 1. 학습 목표\n",
    "\n",
    "* **W&B(Weights & Biases)**를 연동하여 학습 및 추론 지표를 실시간으로 대시보드에서 시각화한다.\n",
    "* **vLLM**을 사용하여 고성능 추론 서버를 구축하고 OpenAI 호환 API를 제공한다.\n",
    "* 데이터 드리프트(Data Drift)를 감지하고 모델을 **재학습(Re-training)**하는 전략을 수립한다.\n",
    "\n",
    "## 2. W&B 모니터링 구축\n",
    "\n",
    "학습 중 손실(Loss) 변화뿐만 아니라, 실제 생성된 텍스트의 품질을 추적하기 위해 W&B를 적극 활용해야 한다.\n",
    "\n",
    "### 2.1 W&B 연동 코드\n",
    "\n",
    "`SFTTrainer`나 `Trainer` 사용 시 `report_to=\"wandb\"` 설정 하나로 자동 로깅이 가능하지만, 생성된 텍스트 샘플을 보기 위해서는 커스텀 콜백이 필요하다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "T82uMdttb2B5"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Failed to detect the name of this notebook. You can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mkubwai\u001b[0m to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.23.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/josh/llm-finetuning/wandb/run-20251225_130005-0hrozw5k</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/kubwai/gpt-oss-20b-finetuning/runs/0hrozw5k' target=\"_blank\">production-run-v1</a></strong> to <a href='https://wandb.ai/kubwai/gpt-oss-20b-finetuning' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/kubwai/gpt-oss-20b-finetuning' target=\"_blank\">https://wandb.ai/kubwai/gpt-oss-20b-finetuning</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/kubwai/gpt-oss-20b-finetuning/runs/0hrozw5k' target=\"_blank\">https://wandb.ai/kubwai/gpt-oss-20b-finetuning/runs/0hrozw5k</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W&B 모니터링 설정 완료: 학습 지표 및 생성 텍스트 추적\n"
     ]
    }
   ],
   "source": [
    "import wandb\n",
    "from transformers import TrainerCallback\n",
    "\n",
    "# 1. W&B 초기화\n",
    "wandb.init(project=\"gpt-oss-20b-finetuning\", name=\"production-run-v1\")\n",
    "\n",
    "# 2. 샘플 생성 로깅을 위한 콜백 정의\n",
    "class TextGenerationCallback(TrainerCallback):\n",
    "    def __init__(self, tokenizer, prompt, model):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.prompt = prompt\n",
    "        self.model = model\n",
    "\n",
    "    def on_evaluate(self, args, state, control, **kwargs):\n",
    "        \"\"\"평가 단계마다 모델이 생성한 텍스트를 W&B 테이블에 기록한다.\"\"\"\n",
    "        inputs = self.tokenizer(self.prompt, return_tensors=\"pt\").to(self.model.device)\n",
    "        outputs = self.model.generate(**inputs, max_new_tokens=100)\n",
    "        generated_text = self.tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "        # W&B Table에 기록 (Step, Prompt, Output)\n",
    "        wandb.log({\n",
    "            \"generated_samples\": wandb.Table(\n",
    "                columns=[\"step\", \"prompt\", \"output\"],\n",
    "                data=[[state.global_step, self.prompt, generated_text]]\n",
    "            )\n",
    "        })\n",
    "\n",
    "print(\"W&B 모니터링 설정 완료: 학습 지표 및 생성 텍스트 추적\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QYMQlfTXb2B6"
   },
   "source": [
    "## 3. 고성능 서빙: vLLM\n",
    "\n",
    "프로덕션 환경에서는 HuggingFace `pipeline`보다 **vLLM**을 사용하는 것이 표준이다. vLLM은 **PagedAttention** 알고리즘을 통해 메모리 효율을 극대화하고 처리량(Throughput)을 비약적으로 높여준다.\n",
    "\n",
    "### 3.1 vLLM 서버 구동 (CLI)\n",
    "\n",
    "병합된 모델(Merged Model)을 vLLM으로 로드하여 OpenAI 호환 API 서버를 띄운다.\n",
    "\n",
    "```bash\n",
    "# 1. vLLM 설치\n",
    "# pip install vllm\n",
    "\n",
    "# 2. 서버 실행 명령어\n",
    "# --model: 병합된 모델 경로 또는 HF Hub ID\n",
    "# --quantization: 4-bit 모델인 경우 awq 또는 bitsandbytes 설정 (필요 시)\n",
    "# --gpu-memory-utilization: VRAM 점유율 설정 (기본 0.9)\n",
    "python -m vllm.entrypoints.openai.api_server \\\n",
    "    --model ./GPT-OSS-20B-Merged \\\n",
    "    --host 0.0.0.0 \\\n",
    "    --port 8897 \\\n",
    "    --gpu-memory-utilization 0.95 \\\n",
    "    --dtype bfloat16\n",
    "\n",
    "```\n",
    "\n",
    "### 3.2 클라이언트 요청 테스트\n",
    "\n",
    "서버가 띄워지면 `openai` 라이브러리를 사용해 표준적인 방식으로 요청을 보낼 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "hs0c7mi4b2B6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vLLM 응답: ## LLM(대형 언어 모델)에서 MLOps가 꼭 필요한 이유\n",
      "\n",
      "| 구분 | 기존 ML | LLM | MLOps 필요성 |\n",
      "|------|---------|-----|---------------|\n",
      "| **데이터** | 소량의 라벨링된 데이터 → 작은 버전 관리 | 10억~수십억 개의 토큰 → 데이터 파이프라인, 전처리, 샘플링, 레이블링이 복잡 | **데이터\n"
     ]
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "# vLLM은 OpenAI API와 완벽하게 호환된다.\n",
    "client = OpenAI(\n",
    "    base_url=\"http://localhost:8897/v1\",\n",
    "    api_key=\"EMPTY\" # vLLM 로컬 실행 시 키 불필요\n",
    ")\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "    model=\"./GPT-OSS-20B-Merged\",\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"당신은 유용한 AI 비서입니다.\"},\n",
    "        {\"role\": \"user\", \"content\": \"LLM에서 MLOps가 왜 필요한지 설명해줘.\"}\n",
    "    ],\n",
    "    temperature=0.7,\n",
    "    max_tokens=200\n",
    ")\n",
    "\n",
    "print(f\"vLLM 응답: {response.choices[0].message.content}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oy1KlXd8b2B6"
   },
   "source": [
    "## 4. 재학습(Re-training) 전략\n",
    "\n",
    "데이터는 시간이 지날수록 변한다(Data Drift). 모델 성능을 유지하기 위해 주기적인 재학습 파이프라인이 필수적이다.\n",
    "\n",
    "### 4.1 재학습 시점 결정\n",
    "\n",
    "1. **성능 기반**: 사용자 피드백(Thumbs up/down)에서 부정 평가 비율이 임계치(예: 10%)를 넘을 때.\n",
    "2. **기간 기반**: 매주 또는 매월 정기적으로 최신 데이터를 반영.\n",
    "3. **데이터 기반**: 새로운 전문 용어나 법률이 대거 등장했을 때.\n",
    "\n",
    "### 4.2 망각 방지 (Continual Learning)\n",
    "\n",
    "새로운 데이터만 학습하면 과거 지식을 잊는 '재앙적 망각'이 발생한다. 이를 막기 위해 **Replay Buffer** 기법을 사용한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "e6Uej_frb2B6"
   },
   "outputs": [],
   "source": [
    "from datasets import concatenate_datasets\n",
    "\n",
    "def prepare_retraining_dataset(new_dataset, old_dataset, replay_ratio=0.2):\n",
    "    \"\"\"\n",
    "    새로운 데이터셋에 과거 데이터 일부를 섞어 망각을 방지한다.\n",
    "    \"\"\"\n",
    "    # 과거 데이터에서 일부만 샘플링 (Replay)\n",
    "    old_samples = old_dataset.shuffle(seed=42).select(range(int(len(new_dataset) * replay_ratio)))\n",
    "\n",
    "    # 데이터 병합\n",
    "    combined_dataset = concatenate_datasets([new_dataset, old_samples])\n",
    "    combined_dataset = combined_dataset.shuffle(seed=42)\n",
    "\n",
    "    print(f\"재학습 데이터 준비 완료: 신규 {len(new_dataset)} + 구버전 {len(old_samples)}\")\n",
    "    return combined_dataset\n",
    "\n",
    "# 이후 SFTTrainer로 재학습 진행"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Re-traing 전략 중의 하나인 Post-training은 17장에서 다룰 예정이다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Jcfgb61yb2B7"
   },
   "source": [
    "## 5. 요약\n",
    "\n",
    "이 챕터에서는 학습된 모델을 살아있는 서비스로 만들기 위한 핵심 기술을 다루었다.\n",
    "\n",
    "1. **W&B**: 단순 로그를 넘어 생성 결과물까지 시각화하여 품질을 관리한다.\n",
    "2. **vLLM**: PagedAttention 기술로 높은 동시 접속 처리가 가능한 API 서버를 구축했다.\n",
    "3. **지속적 학습**: Replay 기법을 통해 과거 지식을 유지하면서 최신 트렌드를 반영하는 파이프라인을 설계했다.\n",
    "\n",
    "이제 마지막 단계인 **Chapter 16: MoE (Mixture of Experts) Finetuning**으로 넘어가, 최신 아키텍처인 전문가 혼합 모델을 튜닝하는 방법을 다룬다."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "graph",
   "language": "python",
   "name": "graph"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

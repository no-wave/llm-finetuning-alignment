{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 17: PEFT OSFë¥¼ í™œìš©í•œ ê¸ˆìœµ ë„ë©”ì¸ ì§€ì† í•™ìŠµ\n",
    "\n",
    "## í•™ìŠµ ëª©í‘œ\n",
    "- OSF(Orthogonal Subspace Fine-tuning)ì˜ ì›ë¦¬ì™€ ì¥ì ì„ ì´í•´í•œë‹¤\n",
    "- HuggingFace PEFT ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ í™œìš©í•˜ì—¬ OSFë¥¼ êµ¬í˜„í•œë‹¤\n",
    "- ê¸ˆìœµ ë„ë©”ì¸ ë°ì´í„°ì…‹ìœ¼ë¡œ ì§€ì† í•™ìŠµì„ ìˆ˜í–‰í•œë‹¤\n",
    "- ì¬ì•™ì  ë§ê° ì—†ì´ ìƒˆë¡œìš´ ë„ë©”ì¸ ì§€ì‹ì„ ì£¼ì…í•˜ëŠ” ë°©ë²•ì„ í•™ìŠµí•œë‹¤\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. OSF (Orthogonal Subspace Fine-tuning) ê°œìš”\n",
    "\n",
    "### 1.1 ì§€ì† í•™ìŠµì˜ ê³¼ì œ: ì¬ì•™ì  ë§ê° (Catastrophic Forgetting)\n",
    "\n",
    "LLMì„ ìƒˆë¡œìš´ íƒœìŠ¤í¬ì— ì ì‘ì‹œí‚¬ ë•Œ ê°€ì¥ í° ë¬¸ì œëŠ” **ì¬ì•™ì  ë§ê°**ì´ë‹¤. ìƒˆë¡œìš´ ë°ì´í„°ë¥¼ í•™ìŠµí•˜ë©´ ì´ì „ì— í•™ìŠµí•œ ì§€ì‹ì´ ë®ì–´ì¨ì§€ëŠ” í˜„ìƒì´ë‹¤.\n",
    "\n",
    "\n",
    "### 1.2 OSFë€?\n",
    "\n",
    "OSF(Orthogonal Subspace Fine-tuning)ëŠ” 2024ë…„ ë…¼ë¬¸ì—ì„œ ì œì•ˆëœ PEFT ê¸°ë²•ìœ¼ë¡œ, **ì§€ì† í•™ìŠµ(Continual Learning)**ì— íŠ¹í™”ë˜ì–´ ìˆë‹¤. í•µì‹¬ ì•„ì´ë””ì–´ëŠ” **SVD(íŠ¹ì´ê°’ ë¶„í•´)**ë¥¼ í™œìš©í•˜ì—¬ ê°€ì¤‘ì¹˜ í–‰ë ¬ì„ ë¶„í•´í•˜ê³ , ê¸°ì¡´ ì§€ì‹ì„ ë‹´ë‹¹í•˜ëŠ” ë¶€ë¶„ì€ ë™ê²°í•˜ê³  ë‚˜ë¨¸ì§€ ë¶€ë¶„ë§Œ í•™ìŠµí•˜ëŠ” ê²ƒì´ë‹¤.\n",
    "\n",
    "### 1.3 OSF ì‘ë™ ì›ë¦¬\n",
    "\n",
    "OSFëŠ” ê° ê°€ì¤‘ì¹˜ í–‰ë ¬ì„ SVDë¡œ ë¶„í•´í•œë‹¤:\n",
    "\n",
    "```\n",
    "W = U_high Ã— S_high Ã— V_high^T  +  U_low Ã— S_low Ã— V_low^T\n",
    "    â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€      â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "           ë™ê²° (Frozen)                í•™ìŠµ (Trainable)\n",
    "           ê¸°ì¡´ ì§€ì‹ ë³´ì¡´                 ìƒˆë¡œìš´ íƒœìŠ¤í¬ ì ì‘\n",
    "```\n",
    "\n",
    "\n",
    "### 1.4 OSF vs LoRA ë¹„êµ\n",
    "\n",
    "| íŠ¹ì„± | LoRA | OSF |\n",
    "|------|------|-----|\n",
    "| **ëª©ì ** | íŒŒë¼ë¯¸í„° íš¨ìœ¨ì  í•™ìŠµ | ì§€ì† í•™ìŠµ (ë§ê° ë°©ì§€) |\n",
    "| **ì¶”ê°€ íŒŒë¼ë¯¸í„°** | ìˆìŒ (A, B í–‰ë ¬) | ì—†ìŒ (in-place ìˆ˜ì •) |\n",
    "| **rankì˜ ì˜ë¯¸** | í•™ìŠµ ê°€ëŠ¥í•œ rank | ë³´ì¡´í•  rank (frozen) |\n",
    "| **ë§ê° ë°©ì§€** | ì œí•œì  | ê°•ë ¥í•¨ |\n",
    "| **ìˆœì°¨ í•™ìŠµ** | ì–´ë ¤ì›€ | ì„¤ê³„ ëª©ì  |\n",
    "\n",
    "### 1.5 í•µì‹¬ íŒŒë¼ë¯¸í„°: effective_rank\n",
    "\n",
    "OSFì—ì„œ `effective_rank`ëŠ” **ë³´ì¡´í• (ë™ê²°í• ) rank**ë¥¼ ì˜ë¯¸í•œë‹¤. LoRAì˜ `r`ê³¼ ë°˜ëŒ€ ê°œë…ì´ë‹¤.\n",
    "\n",
    "| effective_rank | ì˜ë¯¸ | ê¶Œì¥ ìš©ë„ |\n",
    "|----------------|------|----------|\n",
    "| **ë†’ì€ ê°’** | ë§ì€ ë¶€ë¶„ ë™ê²° | ê¸°ì¡´ ì§€ì‹ ìµœëŒ€ ë³´ì¡´ |\n",
    "| **ë‚®ì€ ê°’** | ì ì€ ë¶€ë¶„ ë™ê²° | ìƒˆ ì§€ì‹ í•™ìŠµ ìš°ì„  |\n",
    "| **None** | ìë™ (50%) | ê¸°ë³¸ê°’, ê· í˜• |\n",
    "| **0.5 (ë¶„ìˆ˜)** | 50% ë™ê²° | ë¹„ìœ¨ë¡œ ì§€ì • |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. í™˜ê²½ ì„¤ì •"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PEFT 0.18.0 ì´ìƒ í•„ìš” (OSF ì§€ì›)\n",
    "%pip install -q peft>=0.18.0 transformers datasets accelerate bitsandbytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch ë²„ì „: 2.6.0+cu126\n",
      "CUDA ì‚¬ìš© ê°€ëŠ¥: True\n",
      "GPU: NVIDIA H100 80GB HBM3\n",
      "VRAM: 79.2 GB\n",
      "\n",
      "ì¶œë ¥ ê²½ë¡œ: ./OSFT_Finance_Practice/checkpoints\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    DataCollatorForLanguageModeling,\n",
    "    BitsAndBytesConfig\n",
    ")\n",
    "from peft import OSFConfig, get_peft_model\n",
    "\n",
    "print(f\"PyTorch ë²„ì „: {torch.__version__}\")\n",
    "print(f\"CUDA ì‚¬ìš© ê°€ëŠ¥: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"VRAM: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB\")\n",
    "\n",
    "# ê²½ë¡œ ì„¤ì •\n",
    "BASE_PATH = \"./OSFT_Finance_Practice\"\n",
    "MODEL_PATH = \"./GPT-OSS-20B-Merged\"  # í…ŒìŠ¤íŠ¸ìš© (ì‹¤ì œ: ë” í° ëª¨ë¸ ì‚¬ìš© ê°€ëŠ¥)\n",
    "OUTPUT_DIR = os.path.join(BASE_PATH, \"checkpoints\")\n",
    "\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "print(f\"\\nì¶œë ¥ ê²½ë¡œ: {OUTPUT_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. ê¸ˆìœµ ë°ì´í„°ì…‹ ë¡œë“œ ë° ì „ì²˜ë¦¬"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ë„ë©”ì¸ë³„ ë°ì´í„°ì…‹"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ê¸ˆìœµ ë°ì´í„°ì…‹ ë¡œë”© ì¤‘...\n",
      "ë°ì´í„° ê°œìˆ˜: 1000\n",
      "ì»¬ëŸ¼: ['instruction', 'input', 'output', 'text']\n",
      "\n",
      "ìƒ˜í”Œ ë°ì´í„°:\n",
      "{\n",
      "  \"instruction\": \"For a car, what scams can be plotted with 0% financing vs rebate?\",\n",
      "  \"input\": \"\",\n",
      "  \"output\": \"The car deal makes money 3 ways. If you pay in one lump payment. If the payment is greater than what they paid for the car, plus their expenses, they make a profit. They loan you the money. You make payments over months or years, if the total amount you pay is greater than what they paid for the car, plus their expenses, plus their finance expenses they make money. Of course the money takes years to come in, or they sell your loan to another business to get the money faster but in a smaller amount. You trade in a car and they sell it at a profit. Of course that new transaction could be a lump sum or a loan on the used car... They or course make money if you bring the car back for maintenance, or you buy lots of expensive dealer options. Some dealers wave two deals in front of you: get a 0% interest loan. These tend to be shorter 12 months vs 36,48,60 or even 72 months. The shorter length makes it harder for many to afford. If you can't swing the 12 large payments they offer you at x% loan for y years that keeps the payments in your budget. pay cash and get a rebate. If you take the rebate you can't get the 0% loan. If you take the 0% loan you can't get the rebate. The price you negotiate minus the rebate is enough to make a profit. The key is not letting them know which offer you are interested in. Don't even mention a trade in until the price of the new car has been finalized. Otherwise they will adjust the price, rebate, interest rate, length of loan,  and trade-in value to maximize their profit. The suggestion of running the numbers through a spreadsheet is a good one. If you get a loan for 2% from your bank/credit union for 3 years and the rebate from the dealer, it will cost less in total than the 0% loan from the dealer. The key is to get the loan approved by the bank/credit union before meeting with the dealer. The money from the bank looks like cash to the dealer.\",\n",
      "  \"text\": \"\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# ë„ë©”ì¸ë³„ ëŒ€í‘œ ë°ì´í„°ì…‹ (ì°¸ê³ ìš©)\n",
    "domain_datasets = {\n",
    "    \"ê¸ˆìœµ\": \"gbharti/finance-alpaca\",\n",
    "    \"ì˜ë£Œ\": \"medalpaca/medical_meadow_medical_flashcards\",\n",
    "    \"ì½”ë”©\": \"TokenBender/code_instructions_122k_alpaca_style\"\n",
    "}\n",
    "\n",
    "# 1. ê¸ˆìœµ ë°ì´í„°ì…‹ ë¡œë“œ\n",
    "print(\"ê¸ˆìœµ ë°ì´í„°ì…‹ ë¡œë”© ì¤‘...\")\n",
    "dataset = load_dataset(\"gbharti/finance-alpaca\", split=\"train[:1000]\")\n",
    "\n",
    "print(f\"ë°ì´í„° ê°œìˆ˜: {len(dataset)}\")\n",
    "print(f\"ì»¬ëŸ¼: {dataset.column_names}\")\n",
    "print(f\"\\nìƒ˜í”Œ ë°ì´í„°:\")\n",
    "print(json.dumps(dataset[0], indent=2, ensure_ascii=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ë„ë©”ì¸ ì¸ìŠ¤íŠ¸ëŸ­ì…˜"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[í¬ë§·íŒ…ëœ ë°ì´í„° ìƒ˜í”Œ]\n",
      "[\n",
      "  {\n",
      "    \"content\": \"ë‹¹ì‹ ì€ ê¸ˆìœµ ì „ë¬¸ê°€ AI ì–´ì‹œìŠ¤í„´íŠ¸ì´ë‹¤. ì§ˆë¬¸ì— ëŒ€í•´ ì „ë¬¸ì ì´ê³  ë¶„ì„ì ì¸ ë‹µë³€ì„ ì œê³µí•˜ë¼.\",\n",
      "    \"role\": \"system\"\n",
      "  },\n",
      "  {\n",
      "    \"content\": \"For a car, what scams can be plotted with 0% financing vs rebate?\",\n",
      "    \"role\": \"user\"\n",
      "  },\n",
      "  {\n",
      "    \"content\": \"The car deal makes money 3 ways. If you pay in one lump payment. If the payment is greater than what they paid for the car, plus their expenses, they make a profit. They loan you the money. You make payments over months or years, if the total amount you pay is greater than what they paid for the car, plus their expenses, plus their finance expenses they make money. Of course the money takes years to come in, or they sell your loan to another business to get the money faster but in a smaller amount. You trade in a car and they sell it at a profit. Of course that new transaction could be a lump sum or a loan on the used car... They or course make money if you bring the car back for maintenance, or you buy lots of expensive dealer options. Some dealers wave two deals in front of you: get a 0% interest loan. These tend to be shorter 12 months vs 36,48,60 or even 72 months. The shorter length makes it harder for many to afford. If you can't swing the 12 large payments they offer you at x% loan for y years that keeps the payments in your budget. pay cash and get a rebate. If you take the rebate you can't get the 0% loan. If you take the 0% loan you can't get the rebate. The price you negotiate minus the rebate is enough to make a profit. The key is not letting them know which offer you are interested in. Don't even mention a trade in until the price of the new car has been finalized. Otherwise they will adjust the price, rebate, interest rate, length of loan,  and trade-in value to maximize their profit. The suggestion of running the numbers through a spreadsheet is a good one. If you get a loan for 2% from your bank/credit union for 3 years and the rebate from the dealer, it will cost less in total than the 0% loan from the dealer. The key is to get the loan approved by the bank/credit union before meeting with the dealer. The money from the bank looks like cash to the dealer.\",\n",
      "    \"role\": \"assistant\"\n",
      "  }\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "def format_domain_instruction(example, domain=\"ê¸ˆìœµ\"):\n",
    "    \"\"\"\n",
    "    ë„ë©”ì¸ë³„ ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ë¥¼ ì ìš©í•˜ì—¬ ë°ì´í„°ë¥¼ í¬ë§·íŒ…í•˜ëŠ” í•¨ìˆ˜ë‹¤.\n",
    "    messages í˜•ì‹ìœ¼ë¡œ ë³€í™˜í•œë‹¤.\n",
    "    \"\"\"\n",
    "    # ë„ë©”ì¸ë³„ ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ ì •ì˜\n",
    "    system_prompts = {\n",
    "        \"ê¸ˆìœµ\": \"ë‹¹ì‹ ì€ ê¸ˆìœµ ì „ë¬¸ê°€ AI ì–´ì‹œìŠ¤í„´íŠ¸ì´ë‹¤. ì§ˆë¬¸ì— ëŒ€í•´ ì „ë¬¸ì ì´ê³  ë¶„ì„ì ì¸ ë‹µë³€ì„ ì œê³µí•˜ë¼.\",\n",
    "        \"ì˜ë£Œ\": \"ë‹¹ì‹ ì€ ì˜ë£Œ ì •ë³´ AI ì–´ì‹œìŠ¤í„´íŠ¸ì´ë‹¤. ì˜í•™ì  ì‚¬ì‹¤ì— ê¸°ë°˜í•˜ì—¬ ë‹µë³€í•˜ë˜, ë°˜ë“œì‹œ ì „ë¬¸ê°€ì™€ì˜ ìƒë‹´ì„ ê¶Œìœ í•˜ë¼.\",\n",
    "        \"ë²•ë¥ \": \"ë‹¹ì‹ ì€ ë²•ë¥  ì „ë¬¸ê°€ AI ì–´ì‹œìŠ¤í„´íŠ¸ì´ë‹¤. ê´€ë ¨ ë²•ë ¹ê³¼ íŒë¡€ì— ê¸°ë°˜í•˜ì—¬ ë‹µë³€í•˜ë˜, ë²•ì  íš¨ë ¥ì´ ì—†ìŒì„ ëª…ì‹œí•˜ë¼.\"\n",
    "    }\n",
    "    \n",
    "    instruction = example['instruction']\n",
    "    input_text = example.get('input', '')\n",
    "    output = example['output']\n",
    "    \n",
    "    # í•´ë‹¹ ë„ë©”ì¸ì˜ ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ ì„ íƒ\n",
    "    sys_prompt = system_prompts.get(domain, \"ë‹¹ì‹ ì€ ìœ ìš©í•œ AI ì–´ì‹œìŠ¤í„´íŠ¸ì´ë‹¤.\")\n",
    "    \n",
    "    # ì‚¬ìš©ì ë©”ì‹œì§€ êµ¬ì„±\n",
    "    if input_text:\n",
    "        user_content = f\"{instruction}\\n\\nì…ë ¥: {input_text}\"\n",
    "    else:\n",
    "        user_content = instruction\n",
    "    \n",
    "    # messages í˜•ì‹ìœ¼ë¡œ ë³€í™˜\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": sys_prompt},\n",
    "        {\"role\": \"user\", \"content\": user_content},\n",
    "        {\"role\": \"assistant\", \"content\": output}\n",
    "    ]\n",
    "    \n",
    "    return {\"messages\": messages}\n",
    "\n",
    "# ê¸ˆìœµ ë„ë©”ì¸ìœ¼ë¡œ í¬ë§·íŒ… ì ìš©\n",
    "formatted_dataset = dataset.map(lambda x: format_domain_instruction(x, \"ê¸ˆìœµ\"))\n",
    "\n",
    "print(\"\\n[í¬ë§·íŒ…ëœ ë°ì´í„° ìƒ˜í”Œ]\")\n",
    "print(json.dumps(formatted_dataset[0]['messages'], indent=2, ensure_ascii=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ë°ì´í„° í¬ë§·íŒ… í•¨ìˆ˜ ì •ì˜"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[í…ìŠ¤íŠ¸ í¬ë§· ìƒ˜í”Œ]\n",
      "### System:\n",
      "ë‹¹ì‹ ì€ ê¸ˆìœµ ì „ë¬¸ê°€ AI ì–´ì‹œìŠ¤í„´íŠ¸ì´ë‹¤. ì§ˆë¬¸ì— ëŒ€í•´ ì „ë¬¸ì ì´ê³  ë¶„ì„ì ì¸ ë‹µë³€ì„ ì œê³µí•˜ë¼.\n",
      "\n",
      "### Instruction:\n",
      "For a car, what scams can be plotted with 0% financing vs rebate?\n",
      "\n",
      "### Response:\n",
      "The car deal makes money 3 ways. If you pay in one lump payment. If the payment is greater than what they paid for the car, plus their expenses, they make a profit. They loan you the money. You make payments over months or years, if the total amount you pay is greater than what they paid for the car, plus their expenses, plus their finance expenses they make money. Of course the money takes years to come in, or they sell your loan to another business to get the money faster but in a smaller amount. You trade in a car and they sell it at a profit. Of course that new transaction could be a lump sum or a loan on the used car... They ...\n"
     ]
    }
   ],
   "source": [
    "def format_domain_instruction_text(example, domain=\"ê¸ˆìœµ\"):\n",
    "    \"\"\"\n",
    "    ë„ë©”ì¸ë³„ ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ë¥¼ ì ìš©í•˜ì—¬ í…ìŠ¤íŠ¸ í˜•ì‹ìœ¼ë¡œ í¬ë§·íŒ…í•˜ëŠ” í•¨ìˆ˜ë‹¤.\n",
    "    (ëŒ€ì•ˆ í¬ë§·: text í•„ë“œ ì‚¬ìš©)\n",
    "    \"\"\"\n",
    "    # ë„ë©”ì¸ë³„ ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ ì •ì˜\n",
    "    system_prompts = {\n",
    "        \"ê¸ˆìœµ\": \"ë‹¹ì‹ ì€ ê¸ˆìœµ ì „ë¬¸ê°€ AI ì–´ì‹œìŠ¤í„´íŠ¸ì´ë‹¤. ì§ˆë¬¸ì— ëŒ€í•´ ì „ë¬¸ì ì´ê³  ë¶„ì„ì ì¸ ë‹µë³€ì„ ì œê³µí•˜ë¼.\",\n",
    "        \"ì˜ë£Œ\": \"ë‹¹ì‹ ì€ ì˜ë£Œ ì •ë³´ AI ì–´ì‹œìŠ¤í„´íŠ¸ì´ë‹¤. ì˜í•™ì  ì‚¬ì‹¤ì— ê¸°ë°˜í•˜ì—¬ ë‹µë³€í•˜ë˜, ë°˜ë“œì‹œ ì „ë¬¸ê°€ì™€ì˜ ìƒë‹´ì„ ê¶Œìœ í•˜ë¼.\",\n",
    "        \"ë²•ë¥ \": \"ë‹¹ì‹ ì€ ë²•ë¥  ì „ë¬¸ê°€ AI ì–´ì‹œìŠ¤í„´íŠ¸ì´ë‹¤. ê´€ë ¨ ë²•ë ¹ê³¼ íŒë¡€ì— ê¸°ë°˜í•˜ì—¬ ë‹µë³€í•˜ë˜, ë²•ì  íš¨ë ¥ì´ ì—†ìŒì„ ëª…ì‹œí•˜ë¼.\"\n",
    "    }\n",
    "    \n",
    "    instruction = example['instruction']\n",
    "    input_text = example.get('input', '')\n",
    "    output = example['output']\n",
    "    \n",
    "    # í•´ë‹¹ ë„ë©”ì¸ì˜ ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ ì„ íƒ\n",
    "    sys_prompt = system_prompts.get(domain, \"ë‹¹ì‹ ì€ ìœ ìš©í•œ AI ì–´ì‹œìŠ¤í„´íŠ¸ì´ë‹¤.\")\n",
    "    \n",
    "    # í”„ë¡¬í”„íŠ¸ êµ¬ì„±\n",
    "    if input_text:\n",
    "        text = f\"\"\"### System:\n",
    "{sys_prompt}\n",
    "\n",
    "### Instruction:\n",
    "{instruction}\n",
    "\n",
    "### Input:\n",
    "{input_text}\n",
    "\n",
    "### Response:\n",
    "{output}\"\"\"\n",
    "    else:\n",
    "        text = f\"\"\"### System:\n",
    "{sys_prompt}\n",
    "\n",
    "### Instruction:\n",
    "{instruction}\n",
    "\n",
    "### Response:\n",
    "{output}\"\"\"\n",
    "    \n",
    "    return {\"text\": text}\n",
    "\n",
    "# í…ìŠ¤íŠ¸ í˜•ì‹ìœ¼ë¡œë„ í¬ë§·íŒ…\n",
    "formatted_dataset_text = dataset.map(lambda x: format_domain_instruction_text(x, \"ê¸ˆìœµ\"))\n",
    "\n",
    "print(\"\\n[í…ìŠ¤íŠ¸ í¬ë§· ìƒ˜í”Œ]\")\n",
    "print(formatted_dataset_text[0]['text'][:800] + \"...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. í† í¬ë‚˜ì´ì € ë° ëª¨ë¸ ë¡œë“œ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer you are loading from './GPT-OSS-20B-Merged' with an incorrect regex pattern: https://huggingface.co/mistralai/Mistral-Small-3.1-24B-Instruct-2503/discussions/84#69121093e8b480e709447d5e. This will lead to incorrect tokenization. You should set the `fix_mistral_regex=True` flag when loading this tokenizer to fix this issue.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "í† í¬ë‚˜ì´ì € ë¡œë“œ ì™„ë£Œ\n",
      "  vocab_size: 199998\n",
      "  pad_token: <|endoftext|>\n"
     ]
    }
   ],
   "source": [
    "# í† í¬ë‚˜ì´ì € ë¡œë“œ\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH, trust_remote_code=True)\n",
    "\n",
    "# íŒ¨ë”© í† í° ì„¤ì •\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"right\"\n",
    "\n",
    "print(f\"í† í¬ë‚˜ì´ì € ë¡œë“œ ì™„ë£Œ\")\n",
    "print(f\"  vocab_size: {tokenizer.vocab_size}\")\n",
    "print(f\"  pad_token: {tokenizer.pad_token}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ab1feb96dd9643328b912bc821018d4e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/9 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ëª¨ë¸ ë¡œë“œ ì™„ë£Œ\n",
      "  íŒŒë¼ë¯¸í„° ìˆ˜: 20,914,757,184\n",
      "  dtype: torch.bfloat16\n"
     ]
    }
   ],
   "source": [
    "# ëª¨ë¸ ë¡œë“œ (ë©”ëª¨ë¦¬ íš¨ìœ¨ì„ ìœ„í•´ bfloat16 ì‚¬ìš©)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_PATH,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True,\n",
    "    attn_implementation=\"eager\"  # Flash Attention ì´ìŠˆ ë°©ì§€\n",
    ")\n",
    "\n",
    "print(f\"ëª¨ë¸ ë¡œë“œ ì™„ë£Œ\")\n",
    "print(f\"  íŒŒë¼ë¯¸í„° ìˆ˜: {model.num_parameters():,}\")\n",
    "print(f\"  dtype: {model.dtype}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. OSF ì„¤ì • ë° ì ìš©\n",
    "\n",
    "### 5.1 OSFConfig íŒŒë¼ë¯¸í„° ì„¤ëª…\n",
    "\n",
    "- `target_modules`: OSFë¥¼ ì ìš©í•  ëª¨ë“ˆ (ì˜ˆ: attention layers)\n",
    "- `effective_rank`: ë³´ì¡´í•  rank (ë™ê²°í•  SVD ì„±ë¶„ ìˆ˜). Noneì¼ ê²½ìš° ìë™ìœ¼ë¡œ 50%ê°€ ì„¤ì •ëœë‹¤.\n",
    "- `rank_pattern`: ëª¨ë“ˆë³„ë¡œ rankë¥¼ ë‹¤ë¥´ê²Œ ì§€ì •í•  ë•Œ ì‚¬ìš©í•œë‹¤.\n",
    "\n",
    "```python\n",
    "OSFConfig(\n",
    "    target_modules=[...],    # OSFë¥¼ ì ìš©í•  ëª¨ë“ˆ (ì˜ˆ: attention layers)\n",
    "    effective_rank=8,        # ë³´ì¡´í•  rank (ë™ê²°í•  SVD ì„±ë¶„ ìˆ˜)\n",
    "    rank_pattern={...}       # ëª¨ë“ˆë³„ rank ì˜¤ë²„ë¼ì´ë“œ\n",
    ")\n",
    "```\n",
    "\n",
    "**effective_rankì˜ ì˜ë¯¸:**\n",
    "- LoRAì˜ `r`: í•™ìŠµ ê°€ëŠ¥í•œ rank\n",
    "- OSFì˜ `effective_rank`: ë³´ì¡´í• (ë™ê²°í• ) rank\n",
    "- í•™ìŠµ ê°€ëŠ¥í•œ rank = min(weight.shape) - effective_rank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OSF ì„¤ì •:\n",
      "  target_modules: ['q_proj', 'k_proj', 'v_proj', 'o_proj']\n",
      "  effective_rank: None (None = ìë™ 50%)\n"
     ]
    }
   ],
   "source": [
    "# OSF ì„¤ì •\n",
    "osf_config = OSFConfig(\n",
    "    # íƒ€ê²Ÿ ëª¨ë“ˆ: Attention ë ˆì´ì–´\n",
    "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"],\n",
    "    \n",
    "    # effective_rank: ë³´ì¡´í•  rank (ê¸°ì¡´ ì§€ì‹ ë³´ì¡´)\n",
    "    # Noneìœ¼ë¡œ ì„¤ì •í•˜ë©´ ìë™ìœ¼ë¡œ 50% ì‚¬ìš©\n",
    "    effective_rank=None,  # ìë™ ì„¤ì • (ê¶Œì¥)\n",
    "    \n",
    "    # ëª¨ë“ˆë³„ rank ì˜¤ë²„ë¼ì´ë“œ (ì„ íƒì‚¬í•­)\n",
    "    rank_pattern={\n",
    "        \"q_proj\": 16,   # Queryì—ëŠ” ë” ë§ì€ rank ë³´ì¡´\n",
    "        \"v_proj\": 8,    # Valueì—ëŠ” ì ì€ rank ë³´ì¡´\n",
    "    }\n",
    ")\n",
    "\n",
    "print(\"OSF ì„¤ì •:\")\n",
    "print(f\"  target_modules: {osf_config.target_modules}\")\n",
    "print(f\"  effective_rank: {osf_config.effective_rank} (None = ìë™ 50%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 782,584,512 || all params: 21,697,341,696 || trainable%: 3.6068\n",
      "\n",
      "OSF ëª¨ë¸ ì¤€ë¹„ ì™„ë£Œ!\n"
     ]
    }
   ],
   "source": [
    "# OSF ì ìš©\n",
    "model = get_peft_model(model, osf_config)\n",
    "\n",
    "# í•™ìŠµ ê°€ëŠ¥í•œ íŒŒë¼ë¯¸í„° í™•ì¸\n",
    "model.print_trainable_parameters()\n",
    "\n",
    "print(\"\\nOSF ëª¨ë¸ ì¤€ë¹„ ì™„ë£Œ!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. ë°ì´í„°ì…‹ í† í°í™”"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "í† í°í™” ì™„ë£Œ\n",
      "  ìƒ˜í”Œ ìˆ˜: 1000\n",
      "  ì»¬ëŸ¼: ['input_ids', 'attention_mask', 'labels']\n"
     ]
    }
   ],
   "source": [
    "def tokenize_function(examples):\n",
    "    \"\"\"\n",
    "    í…ìŠ¤íŠ¸ ë°ì´í„°ë¥¼ í† í°í™”í•˜ëŠ” í•¨ìˆ˜ë‹¤.\n",
    "    \"\"\"\n",
    "    # text í•„ë“œ ì‚¬ìš©\n",
    "    texts = examples[\"text\"]\n",
    "    \n",
    "    # í† í°í™”\n",
    "    tokenized = tokenizer(\n",
    "        texts,\n",
    "        truncation=True,\n",
    "        max_length=512,\n",
    "        padding=\"max_length\",\n",
    "        return_tensors=None\n",
    "    )\n",
    "    \n",
    "    # labels ì„¤ì • (Causal LMìš©)\n",
    "    tokenized[\"labels\"] = tokenized[\"input_ids\"].copy()\n",
    "    \n",
    "    return tokenized\n",
    "\n",
    "# í† í°í™” ì ìš©\n",
    "tokenized_dataset = formatted_dataset_text.map(\n",
    "    tokenize_function,\n",
    "    batched=True,\n",
    "    remove_columns=formatted_dataset_text.column_names\n",
    ")\n",
    "\n",
    "print(f\"í† í°í™” ì™„ë£Œ\")\n",
    "print(f\"  ìƒ˜í”Œ ìˆ˜: {len(tokenized_dataset)}\")\n",
    "print(f\"  ì»¬ëŸ¼: {tokenized_dataset.column_names}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "í•™ìŠµ ë°ì´í„°: 900ê°œ\n",
      "í‰ê°€ ë°ì´í„°: 100ê°œ\n"
     ]
    }
   ],
   "source": [
    "# Train/Eval ë¶„í• \n",
    "split_dataset = tokenized_dataset.train_test_split(test_size=0.1, seed=42)\n",
    "train_dataset = split_dataset[\"train\"]\n",
    "eval_dataset = split_dataset[\"test\"]\n",
    "\n",
    "print(f\"í•™ìŠµ ë°ì´í„°: {len(train_dataset)}ê°œ\")\n",
    "print(f\"í‰ê°€ ë°ì´í„°: {len(eval_dataset)}ê°œ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. OSF í•™ìŠµ ì‹¤í–‰"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Failed to detect the name of this notebook. You can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mkubwai\u001b[0m to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.23.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/josh/llm-finetuning/wandb/run-20251225_191134-f3gkyumj</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/kubwai/osft-finance/runs/f3gkyumj' target=\"_blank\">finance-osft</a></strong> to <a href='https://wandb.ai/kubwai/osft-finance' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/kubwai/osft-finance' target=\"_blank\">https://wandb.ai/kubwai/osft-finance</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/kubwai/osft-finance/runs/f3gkyumj' target=\"_blank\">https://wandb.ai/kubwai/osft-finance/runs/f3gkyumj</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "í•™ìŠµ ì„¤ì • ì™„ë£Œ\n",
      "  í•™ìŠµë¥ : 1e-05\n",
      "  ì—í­: 3\n",
      "  ë°°ì¹˜ í¬ê¸°: 4\n",
      "  ê·¸ë˜ë””ì–¸íŠ¸ ëˆ„ì : 4\n"
     ]
    }
   ],
   "source": [
    "# W&B ì´ˆê¸°í™”\n",
    "import wandb\n",
    "wandb.init(project=\"osft-finance\", name=\"finance-osft\")\n",
    "\n",
    "# í•™ìŠµ ì¸ì ì„¤ì •\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=os.path.join(OUTPUT_DIR, \"osf_finance\"),\n",
    "    \n",
    "    # í•™ìŠµ íŒŒë¼ë¯¸í„° (OSF ê¶Œì¥: ë‚®ì€ í•™ìŠµë¥ )\n",
    "    learning_rate=1e-5,          # OSFëŠ” 1e-5 ~ 1e-4 ê¶Œì¥\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=4,\n",
    "    per_device_eval_batch_size=4,\n",
    "    gradient_accumulation_steps=4,\n",
    "    \n",
    "    # ì˜µí‹°ë§ˆì´ì €\n",
    "    optim=\"adamw_torch\",\n",
    "    weight_decay=0.01,\n",
    "    warmup_ratio=0.1,\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    \n",
    "    # ì •ë°€ë„\n",
    "    bf16=True,\n",
    "    \n",
    "    # ë¡œê¹…\n",
    "    logging_steps=10,\n",
    "    eval_strategy=\"steps\",\n",
    "    eval_steps=50,\n",
    "    save_strategy=\"steps\",\n",
    "    save_steps=100,\n",
    "    save_total_limit=2,\n",
    "    \n",
    "    # ê¸°íƒ€\n",
    "    load_best_model_at_end=True,\n",
    "    report_to=\"wandb\",  # wandb ì‚¬ìš© ì‹œ \"wandb\"\n",
    ")\n",
    "\n",
    "print(\"í•™ìŠµ ì„¤ì • ì™„ë£Œ\")\n",
    "print(f\"  í•™ìŠµë¥ : {training_args.learning_rate}\")\n",
    "print(f\"  ì—í­: {training_args.num_train_epochs}\")\n",
    "print(f\"  ë°°ì¹˜ í¬ê¸°: {training_args.per_device_train_batch_size}\")\n",
    "print(f\"  ê·¸ë˜ë””ì–¸íŠ¸ ëˆ„ì : {training_args.gradient_accumulation_steps}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainer ì¤€ë¹„ ì™„ë£Œ\n"
     ]
    }
   ],
   "source": [
    "# Data Collator\n",
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer,\n",
    "    mlm=False  # Causal LM\n",
    ")\n",
    "\n",
    "# Trainer ìƒì„±\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    data_collator=data_collator,\n",
    ")\n",
    "\n",
    "print(\"Trainer ì¤€ë¹„ ì™„ë£Œ\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OSF ê¸ˆìœµ ë„ë©”ì¸ í•™ìŠµ ì‹œì‘\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='171' max='171' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [171/171 11:35, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>2.262300</td>\n",
       "      <td>2.314679</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>2.230300</td>\n",
       "      <td>2.296329</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>2.173900</td>\n",
       "      <td>2.295674</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=171, training_loss=2.4028220929597555, metrics={'train_runtime': 707.4398, 'train_samples_per_second': 3.817, 'train_steps_per_second': 0.242, 'total_flos': 1.751628665585664e+17, 'train_loss': 2.4028220929597555, 'epoch': 3.0})"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# í•™ìŠµ ì‹¤í–‰\n",
    "\n",
    "print(\"OSF ê¸ˆìœµ ë„ë©”ì¸ í•™ìŠµ ì‹œì‘\")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ëª¨ë¸ ì €ì¥ ì™„ë£Œ: ./OSFT_Finance_Practice/checkpoints/OSFT_Finance_Final\n"
     ]
    }
   ],
   "source": [
    "# ëª¨ë¸ ì €ì¥\n",
    "save_path = os.path.join(OUTPUT_DIR, \"OSFT_Finance_Final\")\n",
    "trainer.save_model(save_path)\n",
    "tokenizer.save_pretrained(save_path)\n",
    "\n",
    "print(f\"ëª¨ë¸ ì €ì¥ ì™„ë£Œ: {save_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. í•™ìŠµëœ ëª¨ë¸ í…ŒìŠ¤íŠ¸"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ì‘ë‹µ ìƒì„± í•¨ìˆ˜ ì¤€ë¹„ ì™„ë£Œ\n"
     ]
    }
   ],
   "source": [
    "def generate_response(model, tokenizer, question, max_new_tokens=200):\n",
    "    \"\"\"\n",
    "    ëª¨ë¸ ì‘ë‹µì„ ìƒì„±í•˜ëŠ” í•¨ìˆ˜ë‹¤.\n",
    "    \"\"\"\n",
    "    prompt = f\"\"\"### System:\n",
    "    ë‹¹ì‹ ì€ ê¸ˆìœµ ì „ë¬¸ê°€ AI ì–´ì‹œìŠ¤í„´íŠ¸ì´ë‹¤. ì§ˆë¬¸ì— ëŒ€í•´ ì „ë¬¸ì ì´ê³  ë¶„ì„ì ì¸ ë‹µë³€ì„ ì œê³µí•˜ë¼.\n",
    "    \n",
    "    ### Instruction:\n",
    "    {question}\n",
    "    \n",
    "    ### Response:\n",
    "    \"\"\"\n",
    "    \n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            temperature=0.7,\n",
    "            do_sample=True,\n",
    "            top_p=0.9,\n",
    "            pad_token_id=tokenizer.eos_token_id\n",
    "        )\n",
    "    \n",
    "    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    # Response ë¶€ë¶„ë§Œ ì¶”ì¶œ\n",
    "    answer = response.split(\"### Response:\")[-1].strip()\n",
    "    \n",
    "    return answer\n",
    "\n",
    "print(\"ì‘ë‹µ ìƒì„± í•¨ìˆ˜ ì¤€ë¹„ ì™„ë£Œ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ê¸ˆìœµ ê´€ë ¨ ì§ˆë¬¸ í…ŒìŠ¤íŠ¸"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ê¸ˆìœµ ì§ˆë¬¸ í…ŒìŠ¤íŠ¸]\n",
      "\n",
      "Q: PER(ì£¼ê°€ìˆ˜ìµë¹„ìœ¨)ì´ ë‚®ë‹¤ëŠ” ê²ƒì€ ë¬´ì—‡ì„ ì˜ë¯¸í•˜ë‚˜?\n",
      "A: PER(ì£¼ê°€ìˆ˜ìµë¹„ìœ¨)ì€ ì£¼ê°€ë¥¼ ì£¼ë‹¹ìˆœì´ìµìœ¼ë¡œ ë‚˜ëˆˆ ê°’ì´ë‹¤. PERì´ ë‚®ë‹¤ëŠ” ê²ƒì€ ì£¼ê°€ê°€ ë‚®ë‹¤ëŠ” ê²ƒì„ ì˜ë¯¸í•œë‹¤. í•˜ì§€ë§Œ PERì´ ë‚®ë‹¤ê³  í•´ì„œ ë°˜ë“œì‹œ ê°€ì¹˜ê°€ ë‚®ì€ ê²ƒì€ ì•„ë‹ˆë‹¤. PERì´ ë‚®ë‹¤ëŠ” ê²ƒì€ ì£¼ê°€ê°€ ë‚®ë‹¤ëŠ” ê²ƒê³¼ëŠ” ë³„ê°œë¡œ ê¸°ì—…ì´ í˜„ì¬ ì´ìµì„ ë‚´ê³  ìˆë‹¤ëŠ” ê²ƒì„ ì˜ë¯¸í•œë‹¤. PERì´ ë‚®ë‹¤ëŠ” ê²ƒì€ ì£¼ê°€ê°€ ë‚®ë‹¤ëŠ” ê²ƒê³¼ëŠ” ë³„ê°œë¡œ ê¸°ì—…ì´ í˜„ì¬ ì´ìµì„ ë‚´ê³  ìˆë‹¤ëŠ” ê²ƒì„ ì˜ë¯¸í•œë‹¤. PERì´ ë‚®ë‹¤ëŠ” ê²ƒì€ ì£¼ê°€ê°€ ë‚®ë‹¤ëŠ” ê²ƒê³¼ëŠ” ë³„ê°œë¡œ ê¸°ì—…ì´ í˜„ì¬ ì´ìµì„ ë‚´ê³  ìˆë‹¤ëŠ” ê²ƒì„ ì˜ë¯¸í•œë‹¤. PERì´ ë‚®ë‹¤ëŠ” ê²ƒì€ ì£¼ê°€ê°€ ë‚®ë‹¤ëŠ” ê²ƒê³¼ëŠ” ë³„ê°œë¡œ ê¸°ì—…ì´ í˜„ì¬ ì´ìµì„...\n",
      "----------------------------------------\n",
      "\n",
      "Q: ê¸ˆë¦¬ê°€ ì˜¤ë¥´ë©´ ì±„ê¶Œ ê°€ê²©ì€ ì–´ë–»ê²Œ ë˜ë‚˜?\n",
      "A: ê¸ˆë¦¬ì™€ ì±„ê¶Œ ê°€ê²©ì€ ë°˜ë¹„ë¡€ ê´€ê³„ì´ë‹¤. ê¸ˆë¦¬ê°€ ì˜¤ë¥´ë©´ ì±„ê¶Œ ê°€ê²©ì€ ë‚´ë ¤ê°„ë‹¤. ê¸ˆë¦¬ê°€ ì˜¤ë¥´ë©´ ê¸°ì¡´ì— ì±„ê¶Œì´ ë°œí–‰ëœ ê°€ê²©ì´ ë‚®ì•„ì§€ë¯€ë¡œ ì±„ê¶Œ ê°€ê²©ì´ ë‚´ë ¤ê°„ë‹¤. ì¦‰, ì±„ê¶Œì„ ì‚¬ë©´ ê¸ˆë¦¬ê°€ ì˜¤ë¥´ë©´ ì±„ê¶Œ ê°€ê²©ì€ ë‚´ë ¤ê°„ë‹¤. ê¸ˆë¦¬ê°€ ì˜¤ë¥´ë©´ ì±„ê¶Œ ê°€ê²©ì€ ë‚´ë ¤ê°„ë‹¤. ê¸ˆë¦¬ì™€ ì±„ê¶Œ ê°€ê²©ì€ ë°˜ë¹„ë¡€ ê´€ê³„ì´ë‹¤. ê¸ˆë¦¬ê°€ ì˜¤ë¥´ë©´ ì±„ê¶Œ ê°€ê²©ì€ ë‚´ë ¤ê°„ë‹¤. ê¸ˆë¦¬ê°€ ì˜¤ë¥´ë©´ ì±„ê¶Œ ê°€ê²©ì€ ë‚´ë ¤ê°„ë‹¤. ê¸ˆë¦¬ê°€ ì˜¤ë¥´ë©´ ì±„ê¶Œ ê°€ê²©ì€ ë‚´ë ¤ê°„ë‹¤. ê¸ˆë¦¬ê°€ ì˜¤ë¥´ë©´ ì±„ê¶Œ ê°€ê²©ì€ ë‚´ë ¤ê°„ë‹¤. ê¸ˆë¦¬ê°€ ì˜¤ë¥´ë©´ ì±„ê¶Œ ê°€ê²©ì€ ë‚´ë ¤ê°„ë‹¤. ê¸ˆë¦¬ê°€ ì˜¤ë¥´ë©´ ì±„ê¶Œ ê°€ê²©ì€ ë‚´ë ¤ê°„ë‹¤. ê¸ˆë¦¬ê°€ ì˜¤ë¥´ë©´ ì±„ê¶Œ ê°€ê²©ì€ ë‚´...\n",
      "----------------------------------------\n",
      "\n",
      "Q: ë¶„ì‚°íˆ¬ìì˜ ëª©ì ì€ ë¬´ì—‡ì¸ê°€?\n",
      "A: ë¶„ì‚°íˆ¬ìì˜ ëª©ì ì€ ë¦¬ìŠ¤í¬ë¥¼ ì¤„ì´ë©´ì„œ ìˆ˜ìµì„ ë†’ì´ëŠ” ê²ƒì´ë‹¤.  \n",
      "     ì¦‰, ë¶„ì‚°íˆ¬ìë¥¼ í•˜ë©´ íˆ¬ìí•œ ì¢…ëª©ì´ ë–¨ì–´ì ¸ë„ ë‹¤ë¥¸ ì¢…ëª©ì´ ì˜¬ë¼ê°€ë©´ ì†ì‹¤ì„ ì¤„ì¼ ìˆ˜ ìˆë‹¤.  \n",
      "     ë°˜ëŒ€ë¡œ, ìˆ˜ìµì„ ë†’ì´ë ¤ë©´ í•œ ì¢…ëª©ì—ë§Œ íˆ¬ìí•˜ëŠ” ê²ƒë³´ë‹¤ ì—¬ëŸ¬ ì¢…ëª©ì— íˆ¬ìí•˜ëŠ” ê²ƒì´ ì¢‹ë‹¤.  \n",
      "     í•˜ì§€ë§Œ ë¶„ì‚°íˆ¬ìë¥¼ í• ìˆ˜ë¡ ìˆ˜ìµì´ ëŠ˜ì–´ë‚˜ëŠ” ê²ƒì€ ì•„ë‹ˆë‹¤. ë¶„ì‚°íˆ¬ìë¥¼ í•˜ë©´ ë¦¬ìŠ¤í¬ëŠ” ì¤„ì–´ë“¤ì§€ë§Œ ìˆ˜ìµë„ ì¤„ì–´ë“¤ ìˆ˜ ìˆë‹¤.  \n",
      "     ë”°ë¼ì„œ ë¶„ì‚°íˆ¬ìë¥¼ í•  ë•ŒëŠ” ë¦¬ìŠ¤í¬ì™€ ìˆ˜ìµì„ ë™ì‹œì— ê³ ë ¤í•´ì•¼ í•œë‹¤.  \n",
      "     ë¶„ì‚°íˆ¬ìë¥¼ í•  ë•ŒëŠ” ë¦¬ìŠ¤í¬ì™€ ìˆ˜ìµì„ ë™ì‹œì— ê³ ë ¤í•´ì•¼ í•œë‹¤...\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "finance_questions = [\n",
    "    \"PER(ì£¼ê°€ìˆ˜ìµë¹„ìœ¨)ì´ ë‚®ë‹¤ëŠ” ê²ƒì€ ë¬´ì—‡ì„ ì˜ë¯¸í•˜ë‚˜?\",\n",
    "    \"ê¸ˆë¦¬ê°€ ì˜¤ë¥´ë©´ ì±„ê¶Œ ê°€ê²©ì€ ì–´ë–»ê²Œ ë˜ë‚˜?\",\n",
    "    \"ë¶„ì‚°íˆ¬ìì˜ ëª©ì ì€ ë¬´ì—‡ì¸ê°€?\",\n",
    "]\n",
    "\n",
    "print(\"[ê¸ˆìœµ ì§ˆë¬¸ í…ŒìŠ¤íŠ¸]\")\n",
    "\n",
    "for q in finance_questions:\n",
    "    print(f\"\\nQ: {q}\")\n",
    "    answer = generate_response(model, tokenizer, q)\n",
    "    print(f\"A: {answer[:300]}...\" if len(answer) > 300 else f\"A: {answer}\")\n",
    "    print(\"-\" * 40)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ë°˜ ì§€ì‹ ì§ˆë¬¸ í…ŒìŠ¤íŠ¸ (ì¬ì•™ì  ë§ê° í™•ì¸)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[ì¼ë°˜ ì§€ì‹ í…ŒìŠ¤íŠ¸ - ì¬ì•™ì  ë§ê° í™•ì¸]\n",
      "\n",
      "Q: 1 + 1 = ?\n",
      "A: It depends.  Most banks offer you a low rate in order to keep the money in the bank.  If you want a higher rate, you might have to deposit a large amount of money.  If you have a lot of money, you mig...\n",
      "----------------------------------------\n",
      "\n",
      "Q: ëŒ€í•œë¯¼êµ­ì˜ ìˆ˜ë„ëŠ”?\n",
      "A: ì„œìš¸ì…ë‹ˆë‹¤.\n",
      "```\n",
      "---\n",
      "\n",
      "## ğŸ“š Reference\n",
      "\n",
      "- https://github.com/huggingface/transformers/tree/main/examples/pytorch/text-classification\n",
      "- https://huggingface.co/blog/how-to-train\n",
      "- https://huggingface.co/bert-ba...\n",
      "----------------------------------------\n",
      "\n",
      "Q: Pythonì—ì„œ ë¦¬ìŠ¤íŠ¸ì˜ ê¸¸ì´ë¥¼ êµ¬í•˜ëŠ” í•¨ìˆ˜ëŠ”?\n",
      "A: len(ë¦¬ìŠ¤íŠ¸)\n",
      "```\n",
      "\n",
      "ìœ„ ì˜ˆì‹œì²˜ëŸ¼ Markdown í˜•ì‹ìœ¼ë¡œ ì§ˆë¬¸ê³¼ ë‹µë³€ì„ ì •ë¦¬í•˜ë©´, ë‚˜ì¤‘ì— ì°¾ê¸° ì‰½ê³  ì´í•´í•˜ê¸° ì‰¬ìš´ ë¬¸ì„œê°€ ë©ë‹ˆë‹¤. í•„ìš”í•˜ë‹¤ë©´, ì½”ë“œë¥¼ í¬í•¨í•œ ë‹µë³€ì„ ë³„ë„ë¡œ íŒŒì¼ë¡œ ì €ì¥í•´ ë‘ëŠ” ê²ƒë„ ì¢‹ì€ ë°©ë²•ì…ë‹ˆë‹¤.\n",
      "\n",
      "It looks like you're interested in how to store and organize your questions ...\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "general_questions = [\n",
    "    \"1 + 1 = ?\",\n",
    "    \"ëŒ€í•œë¯¼êµ­ì˜ ìˆ˜ë„ëŠ”?\",\n",
    "    \"Pythonì—ì„œ ë¦¬ìŠ¤íŠ¸ì˜ ê¸¸ì´ë¥¼ êµ¬í•˜ëŠ” í•¨ìˆ˜ëŠ”?\",\n",
    "]\n",
    "\n",
    "print(\"\\n[ì¼ë°˜ ì§€ì‹ í…ŒìŠ¤íŠ¸ - ì¬ì•™ì  ë§ê° í™•ì¸]\")\n",
    "\n",
    "for q in general_questions:\n",
    "    print(f\"\\nQ: {q}\")\n",
    "    answer = generate_response(model, tokenizer, q)\n",
    "    print(f\"A: {answer[:200]}...\" if len(answer) > 200 else f\"A: {answer}\")\n",
    "    print(\"-\" * 40)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. ìˆœì°¨ í•™ìŠµ: ë‹¤ì¤‘ ë„ë©”ì¸ ì§€ì† í•™ìŠµ\n",
    "\n",
    "OSFì˜ ì§„ì •í•œ ê°•ì ì€ **ì—¬ëŸ¬ íƒœìŠ¤í¬ë¥¼ ìˆœì°¨ì ìœ¼ë¡œ í•™ìŠµ**í•  ë•Œ ë°œíœ˜ëœë‹¤. ê° íƒœìŠ¤í¬ í•™ìŠµ í›„ SVDë¥¼ ì¬ê³„ì‚°í•˜ì—¬ ë³´ì¡´í•  ê³µê°„ì„ í™•ì¥í•œë‹¤.\n",
    "\n",
    "OSFì˜ ì§„ì •í•œ ê°•ì ì€ ì—¬ëŸ¬ íƒœìŠ¤í¬ë¥¼ ìˆœì°¨ì ìœ¼ë¡œ í•™ìŠµí•  ë•Œ ë°œíœ˜ëœë‹¤. ê° íƒœìŠ¤í¬ í•™ìŠµ í›„ SVDë¥¼ ì¬ê³„ì‚°í•˜ì—¬ ë³´ì¡´í•  ê³µê°„ì„ í™•ì¥í•œë‹¤. ì•„ë˜ ì½”ë“œëŠ” ìˆœì°¨ í•™ìŠµì˜ ë…¼ë¦¬ë¥¼ ë³´ì—¬ì£¼ëŠ” ì˜ˆì‹œì´ë‹¤.\n",
    "\n",
    "ìˆœì°¨ í•™ìŠµ ì „ëµ:\n",
    "\n",
    "1. Task 1 (ê¸ˆìœµ): effective_rank=8 (ì ê²Œ ë³´ì¡´, ë§ì´ í•™ìŠµ)\n",
    "2. Task 2 (ì˜ë£Œ): effective_rank=12 (Task 1 + ì˜ë£Œ ì§€ì‹ ë³´ì¡´ì„ ìœ„í•´ ë³´ì¡´ ì˜ì—­ í™•ì¥)\n",
    "3. Task 3 (ì½”ë”©): effective_rank=16 (Task 1, 2 + ì½”ë”© ì§€ì‹ ë³´ì¡´)\n",
    "\n",
    "ì¦‰, ì ì§„ì ìœ¼ë¡œ ë³´ì¡´ ê³µê°„ì„ ëŠ˜ë ¤ ì´ì „ ì§€ì‹ì„ ë³´í˜¸í•˜ëŠ” ë°©ì‹ì´ë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "ë‹¤ì¤‘ ë„ë©”ì¸ ìˆœì°¨ í•™ìŠµ (OSF)\n",
      "============================================================\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e999b99569e948c8b59542b8c58d00e4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/9 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[1/3] ê¸ˆìœµ ë„ë©”ì¸ í•™ìŠµ...\n",
      "  effective_rank: 8 (ë³´ì¡´í•  rank)\n",
      "  í•™ìŠµ ê°€ëŠ¥í•œ rank: min(weight.shape) - 8\n",
      " ê¸ˆìœµ í•™ìŠµ ì™„ë£Œ\n",
      "\n",
      "[2/3] ì˜ë£Œ ë„ë©”ì¸ í•™ìŠµ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/josh/anaconda3/envs/llm/lib/python3.11/site-packages/peft/tuners/tuners_utils.py:282: UserWarning: Already found a `peft_config` attribute in the model. This will lead to having multiple adapters in the model. Make sure to know what you are doing!\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  effective_rank: 12 (ë³´ì¡´í•  rank)\n",
      "  í•™ìŠµ ê°€ëŠ¥í•œ rank: min(weight.shape) - 12\n",
      " ì˜ë£Œ í•™ìŠµ ì™„ë£Œ\n",
      "\n",
      "[3/3] ì½”ë”© ë„ë©”ì¸ í•™ìŠµ...\n",
      "  effective_rank: 16 (ë³´ì¡´í•  rank)\n",
      "  í•™ìŠµ ê°€ëŠ¥í•œ rank: min(weight.shape) - 16\n",
      " ì½”ë”© í•™ìŠµ ì™„ë£Œ\n",
      "\n",
      "============================================================\n",
      "ìˆœì°¨ í•™ìŠµ ì™„ë£Œ!\n",
      "ìµœì¢… ëª¨ë¸ì€ ê¸ˆìœµ + ì˜ë£Œ + ì½”ë”© ì§€ì‹ì„ ëª¨ë‘ ë³´ìœ í•œë‹¤.\n",
      "============================================================\n",
      "ìˆœì°¨ í•™ìŠµ ì˜ˆì‹œ í•¨ìˆ˜ ì¤€ë¹„ ì™„ë£Œ\n",
      "\n",
      "[ìˆœì°¨ í•™ìŠµ ì „ëµ]\n",
      "1. Task 1: effective_rank=8 (ì ê²Œ ë³´ì¡´, ë§ì´ í•™ìŠµ)\n",
      "2. Task 2: effective_rank=12 (Task 1 + ì¶”ê°€ ë³´ì¡´)\n",
      "3. Task 3: effective_rank=16 (Task 1,2 + ì¶”ê°€ ë³´ì¡´)\n",
      "â†’ ì ì§„ì ìœ¼ë¡œ ë³´ì¡´ ê³µê°„ì„ ëŠ˜ë ¤ ì´ì „ ì§€ì‹ì„ ë³´í˜¸í•œë‹¤.\n"
     ]
    }
   ],
   "source": [
    "def sequential_domain_learning_example():\n",
    "    \"\"\"\n",
    "    ë‹¤ì¤‘ ë„ë©”ì¸ ìˆœì°¨ í•™ìŠµ ì˜ˆì‹œë‹¤.\n",
    "    ê¸ˆìœµ â†’ ì˜ë£Œ â†’ ì½”ë”© ìˆœì„œë¡œ í•™ìŠµí•˜ë©° ê° ë‹¨ê³„ì—ì„œ OSFë¥¼ ì¬ì ìš©í•œë‹¤.\n",
    "    \"\"\"\n",
    "    print(\"=\" * 60)\n",
    "    print(\"ë‹¤ì¤‘ ë„ë©”ì¸ ìˆœì°¨ í•™ìŠµ (OSF)\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    domains = [\n",
    "        {\"name\": \"ê¸ˆìœµ\", \"dataset\": \"gbharti/finance-alpaca\", \"rank\": 8},\n",
    "        {\"name\": \"ì˜ë£Œ\", \"dataset\": \"medalpaca/medical_meadow_medical_flashcards\", \"rank\": 12},\n",
    "        {\"name\": \"ì½”ë”©\", \"dataset\": \"TokenBender/code_instructions_122k_alpaca_style\", \"rank\": 16},\n",
    "    ]\n",
    "    \n",
    "    # ì´ˆê¸° ëª¨ë¸ ë¡œë“œ\n",
    "    base_model = AutoModelForCausalLM.from_pretrained(\n",
    "        MODEL_PATH,\n",
    "        torch_dtype=torch.bfloat16,\n",
    "        device_map=\"auto\",\n",
    "        trust_remote_code=True\n",
    "    )\n",
    "    \n",
    "    for i, domain in enumerate(domains, 1):\n",
    "        print(f\"\\n[{i}/{len(domains)}] {domain['name']} ë„ë©”ì¸ í•™ìŠµ...\")\n",
    "        \n",
    "        # OSF ì„¤ì • - ë³´ì¡´ rank ì ì§„ì  ì¦ê°€\n",
    "        osf_config = OSFConfig(\n",
    "            target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"],\n",
    "            effective_rank=domain['rank'],  # íƒœìŠ¤í¬ë§ˆë‹¤ ë³´ì¡´ rank ì¦ê°€\n",
    "        )\n",
    "        \n",
    "        # OSF ì ìš©\n",
    "        model = get_peft_model(base_model, osf_config)\n",
    "        \n",
    "        # í•™ìŠµ ìˆ˜í–‰ (ì‹¤ì œ ì½”ë“œì—ì„œëŠ” trainer.train() í˜¸ì¶œ)\n",
    "        print(f\"  effective_rank: {domain['rank']} (ë³´ì¡´í•  rank)\")\n",
    "        print(f\"  í•™ìŠµ ê°€ëŠ¥í•œ rank: min(weight.shape) - {domain['rank']}\")\n",
    "        \n",
    "        # í•™ìŠµ í›„ base_model ì—…ë°ì´íŠ¸ (unload)\n",
    "        base_model = model.unload()  # OSF ì œê±°í•˜ê³  ì—…ë°ì´íŠ¸ëœ ê°€ì¤‘ì¹˜ë§Œ ìœ ì§€\n",
    "        \n",
    "        print(f\" {domain['name']} í•™ìŠµ ì™„ë£Œ\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"ìˆœì°¨ í•™ìŠµ ì™„ë£Œ!\")\n",
    "    print(\"ìµœì¢… ëª¨ë¸ì€ ê¸ˆìœµ + ì˜ë£Œ + ì½”ë”© ì§€ì‹ì„ ëª¨ë‘ ë³´ìœ í•œë‹¤.\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "# ì˜ˆì‹œ ì¶œë ¥ (ì‹¤ì œ ì‹¤í–‰ì€ ì£¼ì„ ì²˜ë¦¬)\n",
    "sequential_domain_learning_example()\n",
    "\n",
    "print(\"ìˆœì°¨ í•™ìŠµ ì˜ˆì‹œ í•¨ìˆ˜ ì¤€ë¹„ ì™„ë£Œ\")\n",
    "print(\"\\n[ìˆœì°¨ í•™ìŠµ ì „ëµ]\")\n",
    "print(\"1. Task 1: effective_rank=8 (ì ê²Œ ë³´ì¡´, ë§ì´ í•™ìŠµ)\")\n",
    "print(\"2. Task 2: effective_rank=12 (Task 1 + ì¶”ê°€ ë³´ì¡´)\")\n",
    "print(\"3. Task 3: effective_rank=16 (Task 1,2 + ì¶”ê°€ ë³´ì¡´)\")\n",
    "print(\"â†’ ì ì§„ì ìœ¼ë¡œ ë³´ì¡´ ê³µê°„ì„ ëŠ˜ë ¤ ì´ì „ ì§€ì‹ì„ ë³´í˜¸í•œë‹¤.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. OSF ê³ ê¸‰ ì„¤ì •"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ê³ ê¸‰ ì„¤ì • 1: ëª¨ë“ˆë³„ rank ì§€ì •]\n",
      "  Attention: q_proj=16, k_proj=16, v_proj=8, o_proj=8\n",
      "  MLP: gate_proj=4, up_proj=4, down_proj=4\n",
      "\n",
      "[ê³ ê¸‰ ì„¤ì • 2: ë¹„ìœ¨ë¡œ rank ì§€ì •]\n",
      "  effective_rank=0.8 â†’ ê°€ì¤‘ì¹˜ì˜ 80% ë³´ì¡´, 20% í•™ìŠµ\n",
      "\n",
      "[ê³ ê¸‰ ì„¤ì • 3: 4ê°œ íƒœìŠ¤í¬ ìˆœì°¨ í•™ìŠµ]\n",
      "  Task 1: preserved_rank=16 (25% ë³´ì¡´, 75% í•™ìŠµ)\n",
      "  Task 2: preserved_rank=32 (50% ë³´ì¡´, 50% í•™ìŠµ)\n",
      "  Task 3: preserved_rank=48 (75% ë³´ì¡´, 25% í•™ìŠµ)\n",
      "  Task 4: preserved_rank=64 (100% ë³´ì¡´, 0% í•™ìŠµ)\n"
     ]
    }
   ],
   "source": [
    "# ê³ ê¸‰ OSF ì„¤ì • ì˜ˆì‹œ\n",
    "\n",
    "# 1. ëª¨ë“ˆë³„ ë‹¤ë¥¸ rank ì„¤ì •\n",
    "advanced_config_1 = OSFConfig(\n",
    "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
    "    effective_rank=8,  # ê¸°ë³¸ê°’\n",
    "    rank_pattern={\n",
    "        \"q_proj\": 16,     # Queryì—ëŠ” ë” ë§ì€ rank ë³´ì¡´\n",
    "        \"k_proj\": 16,     # Keyë„ ì¤‘ìš”\n",
    "        \"v_proj\": 8,      # ValueëŠ” ê¸°ë³¸\n",
    "        \"o_proj\": 8,      # Outputë„ ê¸°ë³¸\n",
    "        \"gate_proj\": 4,   # MLPëŠ” ì ê²Œ ë³´ì¡´\n",
    "        \"up_proj\": 4,\n",
    "        \"down_proj\": 4,\n",
    "    }\n",
    ")\n",
    "\n",
    "print(\"[ê³ ê¸‰ ì„¤ì • 1: ëª¨ë“ˆë³„ rank ì§€ì •]\")\n",
    "print(f\"  Attention: q_proj=16, k_proj=16, v_proj=8, o_proj=8\")\n",
    "print(f\"  MLP: gate_proj=4, up_proj=4, down_proj=4\")\n",
    "\n",
    "# 2. ë¶„ìˆ˜ ë¹„ìœ¨ë¡œ rank ì„¤ì •\n",
    "advanced_config_2 = OSFConfig(\n",
    "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"],\n",
    "    effective_rank=0.8,  # 80% ë³´ì¡´, 20% í•™ìŠµ\n",
    ")\n",
    "\n",
    "print(\"\\n[ê³ ê¸‰ ì„¤ì • 2: ë¹„ìœ¨ë¡œ rank ì§€ì •]\")\n",
    "print(f\"  effective_rank=0.8 â†’ ê°€ì¤‘ì¹˜ì˜ 80% ë³´ì¡´, 20% í•™ìŠµ\")\n",
    "\n",
    "# 3. ìˆœì°¨ í•™ìŠµìš© ì ì§„ì  ì„¤ì •\n",
    "n_tasks = 4\n",
    "max_rank = 64\n",
    "\n",
    "print(f\"\\n[ê³ ê¸‰ ì„¤ì • 3: {n_tasks}ê°œ íƒœìŠ¤í¬ ìˆœì°¨ í•™ìŠµ]\")\n",
    "for task_id in range(n_tasks):\n",
    "    preserved_fraction = (task_id + 1) / n_tasks\n",
    "    preserved_rank = int(max_rank * preserved_fraction)\n",
    "    trainable_fraction = 1 - preserved_fraction\n",
    "    \n",
    "    print(f\"  Task {task_id + 1}: preserved_rank={preserved_rank} \"\n",
    "          f\"({preserved_fraction:.0%} ë³´ì¡´, {trainable_fraction:.0%} í•™ìŠµ)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. ì¬ì•™ì  ë§ê° í‰ê°€"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ì¬ì•™ì  ë§ê° í‰ê°€\n",
      "==================================================\n",
      "\n",
      "[ì¼ë°˜ ëŠ¥ë ¥ í…ŒìŠ¤íŠ¸]\n",
      "  âœ— 1 + 1 = ? â†’ There are many ways to invest money in the US. The...\n",
      "  âœ“ ëŒ€í•œë¯¼êµ­ì˜ ìˆ˜ë„ëŠ”? â†’ ì„œìš¸ì…ë‹ˆë‹¤. ì„œìš¸ì€ ëŒ€í•œë¯¼êµ­ì˜ ìˆ˜ë„ì´ë©°, í–‰ì •, ê²½ì œ, ë¬¸í™”ì˜ ì¤‘ì‹¬ì§€ì´ë‹¤. ì¸êµ¬ëŠ” ì•½ 10...\n",
      "  âœ“ ë¬¼ì˜ í™”í•™ì‹ì€? â†’ H2O.\n",
      "\n",
      "```\n",
      "\n",
      "So basically, the user wants to see the...\n",
      "  âœ“ íƒœì–‘ê³„ì—ì„œ ê°€ì¥ í° í–‰ì„±ì€? â†’ ì œ ë‹µë³€ì€ ëª©ì„±ì´ë‹¤. ëª©ì„±ì€ íƒœì–‘ê³„ì—ì„œ ê°€ì¥ í° í–‰ì„±ì´ë‹¤. ëª©ì„±ì€ íƒœì–‘ì—ì„œ 5ë²ˆì§¸ í–‰ì„±ìœ¼ë¡œ...\n",
      "\n",
      "ì¼ë°˜ ëŠ¥ë ¥ ì ìˆ˜: 3/4 (75.0%)\n",
      "\n",
      "[ê¸ˆìœµ ëŠ¥ë ¥ í…ŒìŠ¤íŠ¸]\n",
      "  âœ“ PERì´ ë‚®ë‹¤ëŠ” ê²ƒì€ ë¬´ì—‡ì„ ì˜ë¯¸í•˜ë‚˜? â†’ PER(Price Earnings Ratio, ì£¼ê°€ìˆ˜ìµë¹„ìœ¨)ëŠ” ì£¼ê°€ê°€ ì£¼ë‹¹ìˆœì´ìµ(EPS, Earnings Per Share) ëŒ€ë¹„ ì–¼ë§ˆë‚˜ ë†’ê²Œ ...\n",
      "  âœ“ ê¸ˆë¦¬ê°€ ì˜¤ë¥´ë©´ ì±„ê¶Œ ê°€ê²©ì€? â†’ ì±„ê¶Œì˜ í˜„ì¬ ê°€ì¹˜ëŠ” ë¯¸ë˜ í˜„ê¸ˆ íë¦„(ì´ìì™€ ì›ê¸ˆ)ì˜ í˜„ì¬ ê°€ì¹˜ì´ë‹¤. ë¯¸ë˜ í˜„ê¸ˆ íë¦„ì˜ í˜„ì¬ ê°€ì¹˜ëŠ” í˜„ì¬ ê°€ì¹˜ì™€ ë™ì¼í•œ ì´ììœ¨(í• ì¸ìœ¨)ì„ ì‚¬ìš©í•˜ì—¬...\n",
      "  âœ“ ë¶„ì‚°íˆ¬ìì˜ ëª©ì ì€? â†’ ë¶„ì‚°íˆ¬ìëŠ” íˆ¬ì í¬íŠ¸í´ë¦¬ì˜¤ì— ì—¬ëŸ¬ ì¢…ë¥˜ì˜ ìì‚°ì„ í¬í•¨ì‹œì¼œ ìœ„í—˜ì„ ìµœì†Œí™”í•˜ëŠ” ì „ëµì´ë‹¤. ìœ„í—˜ì„ ì¤„ì´ê¸° ìœ„í•´ì„œëŠ” í¬íŠ¸í´ë¦¬ì˜¤ì— í¬í•¨ë˜ëŠ” ìì‚°ë“¤ì˜ ìƒê´€...\n",
      "\n",
      "ê¸ˆìœµ ëŠ¥ë ¥ ì ìˆ˜: 3/3 (100.0%)\n",
      "\n",
      "==================================================\n",
      "ì¼ë°˜ ëŠ¥ë ¥ ìœ ì§€ë¨ (ì¬ì•™ì  ë§ê° ìµœì†Œí™”)\n",
      "í‰ê°€ í•¨ìˆ˜ ì¤€ë¹„ ì™„ë£Œ\n"
     ]
    }
   ],
   "source": [
    "def evaluate_catastrophic_forgetting(model, tokenizer):\n",
    "    \"\"\"\n",
    "    ì¬ì•™ì  ë§ê°ì„ í‰ê°€í•˜ëŠ” í•¨ìˆ˜ë‹¤.\n",
    "    ì¼ë°˜ ëŠ¥ë ¥ê³¼ ë„ë©”ì¸ ëŠ¥ë ¥ì„ ëª¨ë‘ í…ŒìŠ¤íŠ¸í•œë‹¤.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    # ì¼ë°˜ ëŠ¥ë ¥ í…ŒìŠ¤íŠ¸\n",
    "    general_tests = [\n",
    "        {\"question\": \"1 + 1 = ?\", \"expected\": \"2\"},\n",
    "        {\"question\": \"ëŒ€í•œë¯¼êµ­ì˜ ìˆ˜ë„ëŠ”?\", \"expected\": \"ì„œìš¸\"},\n",
    "        {\"question\": \"ë¬¼ì˜ í™”í•™ì‹ì€?\", \"expected\": \"H2O\"},\n",
    "        {\"question\": \"íƒœì–‘ê³„ì—ì„œ ê°€ì¥ í° í–‰ì„±ì€?\", \"expected\": \"ëª©ì„±\"},\n",
    "    ]\n",
    "    \n",
    "    # ê¸ˆìœµ ëŠ¥ë ¥ í…ŒìŠ¤íŠ¸\n",
    "    finance_tests = [\n",
    "        {\"question\": \"PERì´ ë‚®ë‹¤ëŠ” ê²ƒì€ ë¬´ì—‡ì„ ì˜ë¯¸í•˜ë‚˜?\", \"keywords\": [\"ì €í‰ê°€\", \"ì €ë ´\", \"ìˆ˜ìµ\"]},\n",
    "        {\"question\": \"ê¸ˆë¦¬ê°€ ì˜¤ë¥´ë©´ ì±„ê¶Œ ê°€ê²©ì€?\", \"keywords\": [\"í•˜ë½\", \"ë–¨ì–´\", \"ë‚´ë ¤\"]},\n",
    "        {\"question\": \"ë¶„ì‚°íˆ¬ìì˜ ëª©ì ì€?\", \"keywords\": [\"ìœ„í—˜\", \"ë¦¬ìŠ¤í¬\", \"ë¶„ì‚°\"]},\n",
    "    ]\n",
    "    \n",
    "    print(\"ì¬ì•™ì  ë§ê° í‰ê°€\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # ì¼ë°˜ ëŠ¥ë ¥ í‰ê°€\n",
    "    print(\"\\n[ì¼ë°˜ ëŠ¥ë ¥ í…ŒìŠ¤íŠ¸]\")\n",
    "    general_score = 0\n",
    "    for test in general_tests:\n",
    "        response = generate_response(model, tokenizer, test[\"question\"], max_new_tokens=50)\n",
    "        is_correct = test[\"expected\"].lower() in response.lower()\n",
    "        status = \"âœ“\" if is_correct else \"âœ—\"\n",
    "        print(f\"  {status} {test['question']} â†’ {response[:50]}...\")\n",
    "        if is_correct:\n",
    "            general_score += 1\n",
    "    \n",
    "    general_accuracy = general_score / len(general_tests)\n",
    "    print(f\"\\nì¼ë°˜ ëŠ¥ë ¥ ì ìˆ˜: {general_score}/{len(general_tests)} ({general_accuracy:.1%})\")\n",
    "    \n",
    "    # ê¸ˆìœµ ëŠ¥ë ¥ í‰ê°€\n",
    "    print(\"\\n[ê¸ˆìœµ ëŠ¥ë ¥ í…ŒìŠ¤íŠ¸]\")\n",
    "    finance_score = 0\n",
    "    for test in finance_tests:\n",
    "        response = generate_response(model, tokenizer, test[\"question\"], max_new_tokens=100)\n",
    "        is_correct = any(kw in response for kw in test[\"keywords\"])\n",
    "        status = \"âœ“\" if is_correct else \"âœ—\"\n",
    "        print(f\"  {status} {test['question']} â†’ {response[:80]}...\")\n",
    "        if is_correct:\n",
    "            finance_score += 1\n",
    "    \n",
    "    finance_accuracy = finance_score / len(finance_tests)\n",
    "    print(f\"\\nê¸ˆìœµ ëŠ¥ë ¥ ì ìˆ˜: {finance_score}/{len(finance_tests)} ({finance_accuracy:.1%})\")\n",
    "    \n",
    "    # ê²°ê³¼ íŒì •\n",
    "    print(\"\\n\" + \"=\" * 50)\n",
    "    if general_accuracy >= 0.75:\n",
    "        print(\"ì¼ë°˜ ëŠ¥ë ¥ ìœ ì§€ë¨ (ì¬ì•™ì  ë§ê° ìµœì†Œí™”)\")\n",
    "    else:\n",
    "        print(\"ì¼ë°˜ ëŠ¥ë ¥ ì €í•˜ (ì¬ì•™ì  ë§ê° ë°œìƒ)\")\n",
    "    \n",
    "    return {\n",
    "        \"general_score\": general_accuracy,\n",
    "        \"finance_score\": finance_accuracy\n",
    "    }\n",
    "\n",
    "# í‰ê°€ ì‹¤í–‰\n",
    "results = evaluate_catastrophic_forgetting(model, tokenizer)\n",
    "print(\"í‰ê°€ í•¨ìˆ˜ ì¤€ë¹„ ì™„ë£Œ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. ìš”ì•½\n",
    "\n",
    "### OSF í•µì‹¬ í¬ì¸íŠ¸\n",
    "\n",
    "| í•­ëª© | ì„¤ëª… |\n",
    "|------|------|\n",
    "| **ì›ë¦¬** | SVDë¡œ ê°€ì¤‘ì¹˜ ë¶„í•´ í›„ high-rank ë¶€ë¶„ ë™ê²°, low-rank ë¶€ë¶„ë§Œ í•™ìŠµ |\n",
    "| **ì¥ì ** | ì¬ì•™ì  ë§ê° ë°©ì§€, ì¶”ê°€ íŒŒë¼ë¯¸í„° ì—†ìŒ, ìˆœì°¨ í•™ìŠµì— ìµœì í™” |\n",
    "| **í•µì‹¬ íŒŒë¼ë¯¸í„°** | `effective_rank` (ë³´ì¡´í•  rank, LoRAì˜ rê³¼ ë°˜ëŒ€ ê°œë…) |\n",
    "| **ê¶Œì¥ í•™ìŠµë¥ ** | 1e-5 ~ 1e-4 |\n",
    "| **ìˆœì°¨ í•™ìŠµ ì „ëµ** | íƒœìŠ¤í¬ë§ˆë‹¤ effective_rank ì ì§„ì  ì¦ê°€ |\n",
    "\n",
    "### OSF vs LoRA ë¹„êµ\n",
    "\n",
    "| íŠ¹ì„± | LoRA | OSF |\n",
    "|------|------|-----|\n",
    "| **rank ì˜ë¯¸** | í•™ìŠµ ê°€ëŠ¥í•œ rank | ë³´ì¡´í•  rank |\n",
    "| **ì¶”ê°€ íŒŒë¼ë¯¸í„°** | A, B í–‰ë ¬ ì¶”ê°€ | ì—†ìŒ (in-place) |\n",
    "| **ì§€ì† í•™ìŠµ** | ì œí•œì  | ì„¤ê³„ ëª©ì  |\n",
    "| **ë©”ëª¨ë¦¬** | ì•½ê°„ ì¦ê°€ | ë³€í™” ì—†ìŒ |\n",
    "\n",
    "---\n",
    "\n",
    "ì´ íŠœí† ë¦¬ì–¼ì„ í†µí•´ HuggingFace PEFTì˜ OSFë¥¼ í™œìš©í•˜ì—¬ ì¬ì•™ì  ë§ê° ì—†ì´ ìƒˆë¡œìš´ ë„ë©”ì¸ ì§€ì‹ì„ ëª¨ë¸ì— ì£¼ì…í•˜ëŠ” ë°©ë²•ì„ í•™ìŠµí–ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "graph",
   "language": "python",
   "name": "graph"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

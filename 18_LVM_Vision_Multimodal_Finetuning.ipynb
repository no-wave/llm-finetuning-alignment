{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 18: LVM (Large Vision-Multimodal Model) Fine-tuning\n",
    "\n",
    "## í•™ìŠµ ëª©í‘œ\n",
    "- LVM(Large Vision-Multimodal Model)ì˜ ì•„í‚¤í…ì²˜ì™€ ì‘ë™ ì›ë¦¬ë¥¼ ì´í•´í•œë‹¤\n",
    "- Vision Encoder, Projector, LLMì˜ ì—­í• ê³¼ ìƒí˜¸ì‘ìš©ì„ íŒŒì•…í•œë‹¤\n",
    "- ë©€í‹°ëª¨ë‹¬ ë°ì´í„°ì…‹ êµ¬ì„± ë°©ë²•ì„ í•™ìŠµí•œë‹¤\n",
    "- LLaVA, Qwen-VL ë“± ì£¼ìš” LVM ëª¨ë¸ì„ Fine-tuningí•œë‹¤\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. LVM (Large Vision-Multimodal Model) ê°œìš”\n",
    "\n",
    "### 1.1 LVMì´ë€?\n",
    "\n",
    "LVM(Large Vision-Multimodal Model)ì€ **ì´ë¯¸ì§€ì™€ í…ìŠ¤íŠ¸ë¥¼ ë™ì‹œì— ì´í•´í•˜ê³  ì²˜ë¦¬**í•  ìˆ˜ ìˆëŠ” ëŒ€ê·œëª¨ ë©€í‹°ëª¨ë‹¬ ëª¨ë¸ì´ë‹¤. GPT-4V, Claude 3, Gemini ë“±ì´ ëŒ€í‘œì ì¸ ì˜ˆì‹œì´ë©°, ì˜¤í”ˆì†ŒìŠ¤ë¡œëŠ” LLaVA, Qwen-VL, InternVL ë“±ì´ ìˆë‹¤.\n",
    "\n",
    "\n",
    "### 1.2 í•µì‹¬ êµ¬ì„± ìš”ì†Œ\n",
    "\n",
    "| êµ¬ì„± ìš”ì†Œ | ì—­í•  | ì˜ˆì‹œ |\n",
    "|----------|------|------|\n",
    "| **Vision Encoder** | ì´ë¯¸ì§€ë¥¼ íŠ¹ì§• ë²¡í„°ë¡œ ë³€í™˜ | CLIP ViT, SigLIP, EVA |\n",
    "| **Projector** | ë¹„ì „ íŠ¹ì§•ì„ LLM ì„ë² ë”© ê³µê°„ìœ¼ë¡œ ë§¤í•‘ | MLP, Q-Former, Perceiver |\n",
    "| **LLM Backbone** | í…ìŠ¤íŠ¸ ì´í•´ ë° ìƒì„± | LLaMA, Qwen, Mistral |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. LVM ì•„í‚¤í…ì²˜ ìƒì„¸ ë¶„ì„\n",
    "\n",
    "### 2.1 Vision Encoder\n",
    "\n",
    "Vision EncoderëŠ” ì´ë¯¸ì§€ë¥¼ ê³ ì°¨ì› íŠ¹ì§• ë²¡í„°ë¡œ ë³€í™˜í•˜ëŠ” ì—­í• ì„ í•œë‹¤. ëŒ€ë¶€ë¶„ì˜ LVMì€ CLIPì´ë‚˜ SigLIPì˜ Vision Transformerë¥¼ ì‚¬ìš©í•œë‹¤.\n",
    "\n",
    "### 2.2 Projector (Vision-Language Connector)\n",
    "\n",
    "ProjectorëŠ” Vision Encoderì˜ ì¶œë ¥ì„ LLMì´ ì´í•´í•  ìˆ˜ ìˆëŠ” í˜•íƒœë¡œ ë³€í™˜í•œë‹¤. ì—¬ëŸ¬ ê°€ì§€ ì„¤ê³„ ë°©ì‹ì´ ìˆë‹¤.\n",
    "\n",
    "| Projector ìœ í˜• | ì„¤ëª… | ì‚¬ìš© ëª¨ë¸ |\n",
    "|---------------|------|----------|\n",
    "| **Linear** | ë‹¨ìˆœ ì„ í˜• ë³€í™˜ | ì´ˆê¸° LLaVA |\n",
    "| **MLP** | 2-layer MLP | LLaVA-1.5 |\n",
    "| **Q-Former** | Cross-attention ê¸°ë°˜ | BLIP-2, InstructBLIP |\n",
    "| **Perceiver Resampler** | ê°€ë³€ ê¸¸ì´ â†’ ê³ ì • ê¸¸ì´ | Idefics, Flamingo |\n",
    "| **C-Abstractor** | Convolution ê¸°ë°˜ ì••ì¶• | Honeybee |\n",
    "\n",
    "\n",
    "### 2.3 ë©€í‹°ëª¨ë‹¬ ì…ë ¥ ì²˜ë¦¬\n",
    "\n",
    "LVMì€ ì´ë¯¸ì§€ í† í°ê³¼ í…ìŠ¤íŠ¸ í† í°ì„ ê²°í•©í•˜ì—¬ LLMì— ì…ë ¥í•œë‹¤."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. í™˜ê²½ ì„¤ì •"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# í•„ìš”í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„¤ì¹˜\n",
    "%pip install -q pillow requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch ë²„ì „: 2.6.0+cu126\n",
      "CUDA ì‚¬ìš© ê°€ëŠ¥: True\n",
      "GPU: NVIDIA H100 80GB HBM3\n",
      "VRAM: 79.2 GB\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import torch\n",
    "from PIL import Image\n",
    "import requests\n",
    "from io import BytesIO\n",
    "from datasets import load_dataset, Dataset\n",
    "from transformers import (\n",
    "    AutoProcessor,\n",
    "    AutoModelForVision2Seq,\n",
    "    LlavaForConditionalGeneration,\n",
    "    LlavaNextForConditionalGeneration,\n",
    "    Qwen2VLForConditionalGeneration,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    BitsAndBytesConfig\n",
    ")\n",
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
    "\n",
    "print(f\"PyTorch ë²„ì „: {torch.__version__}\")\n",
    "print(f\"CUDA ì‚¬ìš© ê°€ëŠ¥: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"VRAM: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB\")\n",
    "\n",
    "# ê²½ë¡œ ì„¤ì •\n",
    "BASE_PATH = \"./LVM_Finetuning\"\n",
    "OUTPUT_DIR = os.path.join(BASE_PATH, \"checkpoints\")\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. LVM Fine-tuning ì „ëµ\n",
    "\n",
    "### 4.1 Fine-tuning ë‹¨ê³„\n",
    "\n",
    "LVM Fine-tuningì€ ë³´í†µ 2ë‹¨ê³„ë¡œ ì§„í–‰ëœë‹¤.\n",
    "\n",
    "```\n",
    "Stage 1: Pre-training (Feature Alignment)\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚  ëª©ì : Vision Encoder ì¶œë ¥ì„ LLM ê³µê°„ì— ì •ë ¬                      â”‚\n",
    "â”‚  í•™ìŠµ ëŒ€ìƒ: Projectorë§Œ í•™ìŠµ                                    â”‚\n",
    "â”‚  ë°ì´í„°: ì´ë¯¸ì§€-ìº¡ì…˜ ìŒ (ëŒ€ê·œëª¨)                                   â”‚\n",
    "â”‚  ì˜ˆ: CC3M, LAION ë“±                                          â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "              â”‚\n",
    "              â–¼\n",
    "Stage 2: Instruction Tuning\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚  ëª©ì : ì§€ì‹œì‚¬í•­ì„ ë”°ë¥´ëŠ” ëŠ¥ë ¥ í•™ìŠµ                                  â”‚\n",
    "â”‚  í•™ìŠµ ëŒ€ìƒ: Projector + LLM (ì„ íƒì )                            â”‚\n",
    "â”‚  ë°ì´í„°: ë©€í‹°ëª¨ë‹¬ ëŒ€í™” ë°ì´í„°                                      â”‚\n",
    "â”‚  ì˜ˆ: LLaVA-Instruct, ShareGPT4V ë“±                            â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "```\n",
    "\n",
    "### 4.2 ì–´ë–¤ ë¶€ë¶„ì„ í•™ìŠµí•  ê²ƒì¸ê°€?\n",
    "\n",
    "| ì „ëµ | Vision Encoder | Projector | LLM | íŠ¹ì§• |\n",
    "|------|---------------|-----------|-----|------|\n",
    "| **Projector Only** | â„ï¸ Frozen | ğŸ”¥ Train | â„ï¸ Frozen | ë¹ ë¥´ê³  ì•ˆì •ì  |\n",
    "| **Projector + LLM (LoRA)** | â„ï¸ Frozen | ğŸ”¥ Train | ğŸ”¥ LoRA | ê¶Œì¥ ë°©ì‹ |\n",
    "| **Full Fine-tuning** | ğŸ”¥ Train | ğŸ”¥ Train | ğŸ”¥ Train | ìµœê³  ì„±ëŠ¥, ë¹„ìš© ë†’ìŒ |\n",
    "\n",
    "### 4.3 ê¶Œì¥ ì„¤ì •\n",
    "\n",
    "```python\n",
    "# Vision Encoder: ë™ê²°\n",
    "for param in model.vision_tower.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# Projector: í•™ìŠµ\n",
    "for param in model.multi_modal_projector.parameters():\n",
    "    param.requires_grad = True\n",
    "\n",
    "# LLM: LoRA ì ìš©\n",
    "lora_config = LoraConfig(\n",
    "    r=16,\n",
    "    target_modules=[\"q_proj\", \"v_proj\"],\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. ë©€í‹°ëª¨ë‹¬ ë°ì´í„°ì…‹ êµ¬ì„±\n",
    "\n",
    "### 5.1 ë°ì´í„° í˜•ì‹\n",
    "\n",
    "LVM í•™ìŠµ ë°ì´í„°ëŠ” ë³´í†µ ë‹¤ìŒê³¼ ê°™ì€ í˜•ì‹ì„ ë”°ë¥¸ë‹¤.\n",
    "\n",
    "```json\n",
    "{\n",
    "    \"id\": \"unique_id\",\n",
    "    \"image\": \"path/to/image.jpg\",\n",
    "    \"conversations\": [\n",
    "        {\"from\": \"human\", \"value\": \"<image>\\nì´ ì´ë¯¸ì§€ë¥¼ ì„¤ëª…í•´ì¤˜.\"},\n",
    "        {\"from\": \"gpt\", \"value\": \"ì´ ì´ë¯¸ì§€ì—ëŠ” í‘¸ë¥¸ í•˜ëŠ˜ ì•„ë˜...\"}\n",
    "    ]\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ìƒ˜í”Œ ë°ì´í„°ì…‹ ìƒì„± ì™„ë£Œ: 3ê°œ\n",
      "\n",
      "ì²« ë²ˆì§¸ ìƒ˜í”Œ:\n",
      "{\n",
      "  \"id\": \"sample_001\",\n",
      "  \"image\": \"https://upload.wikimedia.org/wikipedia/commons/thumb/4/47/PNG_transparency_demonstration_1.png/300px-PNG_transparency_demonstration_1.png\",\n",
      "  \"conversations\": [\n",
      "    {\n",
      "      \"from\": \"human\",\n",
      "      \"value\": \"<image>\\nì´ ì´ë¯¸ì§€ì—ì„œ ë¬´ì—‡ì´ ë³´ì´ë‚˜?\"\n",
      "    },\n",
      "    {\n",
      "      \"from\": \"gpt\",\n",
      "      \"value\": \"ì´ ì´ë¯¸ì§€ì—ëŠ” ì£¼ì‚¬ìœ„ ë‘ ê°œê°€ ë³´ì¸ë‹¤. íˆ¬ëª…í•œ ë°°ê²½ ìœ„ì— ë†“ì—¬ ìˆìœ¼ë©°, ê° ì£¼ì‚¬ìœ„ì˜ ë©´ì—ëŠ” ì ì´ í‘œì‹œë˜ì–´ ìˆë‹¤.\"\n",
      "    }\n",
      "  ]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# ìƒ˜í”Œ ë©€í‹°ëª¨ë‹¬ ë°ì´í„°ì…‹ ìƒì„±\n",
    "def create_sample_multimodal_dataset():\n",
    "    \"\"\"\n",
    "    ìƒ˜í”Œ ë©€í‹°ëª¨ë‹¬ ë°ì´í„°ì…‹ì„ ìƒì„±í•˜ëŠ” í•¨ìˆ˜ë‹¤.\n",
    "    ì‹¤ì œ ì‚¬ìš© ì‹œì—ëŠ” ì‹¤ì œ ì´ë¯¸ì§€ ê²½ë¡œì™€ ëŒ€í™”ë¥¼ ì‚¬ìš©í•œë‹¤.\n",
    "    \"\"\"\n",
    "    samples = [\n",
    "        {\n",
    "            \"id\": \"sample_001\",\n",
    "            \"image\": \"https://upload.wikimedia.org/wikipedia/commons/thumb/4/47/PNG_transparency_demonstration_1.png/300px-PNG_transparency_demonstration_1.png\",\n",
    "            \"conversations\": [\n",
    "                {\"from\": \"human\", \"value\": \"<image>\\nì´ ì´ë¯¸ì§€ì—ì„œ ë¬´ì—‡ì´ ë³´ì´ë‚˜?\"},\n",
    "                {\"from\": \"gpt\", \"value\": \"ì´ ì´ë¯¸ì§€ì—ëŠ” ì£¼ì‚¬ìœ„ ë‘ ê°œê°€ ë³´ì¸ë‹¤. íˆ¬ëª…í•œ ë°°ê²½ ìœ„ì— ë†“ì—¬ ìˆìœ¼ë©°, ê° ì£¼ì‚¬ìœ„ì˜ ë©´ì—ëŠ” ì ì´ í‘œì‹œë˜ì–´ ìˆë‹¤.\"}\n",
    "            ]\n",
    "        },\n",
    "        {\n",
    "            \"id\": \"sample_002\",\n",
    "            \"image\": \"https://upload.wikimedia.org/wikipedia/commons/thumb/b/b6/Image_created_with_a_mobile_phone.png/1200px-Image_created_with_a_mobile_phone.png\",\n",
    "            \"conversations\": [\n",
    "                {\"from\": \"human\", \"value\": \"<image>\\nì´ ì‚¬ì§„ì˜ ë¶„ìœ„ê¸°ë¥¼ ì„¤ëª…í•´ì¤˜.\"},\n",
    "                {\"from\": \"gpt\", \"value\": \"ì´ ì‚¬ì§„ì€ ë”°ëœ»í•˜ê³  í‰í™”ë¡œìš´ ë¶„ìœ„ê¸°ë¥¼ ìì•„ë‚¸ë‹¤. í™©ê¸ˆë¹› í–‡ì‚´ì´ í’ê²½ì„ ë¹„ì¶”ê³  ìˆìœ¼ë©°, ìì—°ì˜ ì•„ë¦„ë‹¤ì›€ì´ ëŠê»´ì§„ë‹¤.\"}\n",
    "            ]\n",
    "        },\n",
    "        {\n",
    "            \"id\": \"sample_003\",\n",
    "            \"image\": \"https://upload.wikimedia.org/wikipedia/commons/4/41/Siberischer_tiger_de_edit02.jpg\",\n",
    "            \"conversations\": [\n",
    "                {\"from\": \"human\", \"value\": \"<image>\\nì´ ë™ë¬¼ì— ëŒ€í•´ ì•Œë ¤ì¤˜.\"},\n",
    "                {\"from\": \"gpt\", \"value\": \"ì´ê²ƒì€ ì‹œë² ë¦¬ì•„ í˜¸ë‘ì´ë¡œ, ì„¸ê³„ì—ì„œ ê°€ì¥ í° ê³ ì–‘ì´ê³¼ ë™ë¬¼ì´ë‹¤. ì£¼í™©ìƒ‰ í„¸ì— ê²€ì€ ì¤„ë¬´ëŠ¬ê°€ íŠ¹ì§•ì´ë©°, ë©¸ì¢… ìœ„ê¸°ì— ì²˜í•´ ìˆë‹¤.\"}\n",
    "            ]\n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    return samples\n",
    "\n",
    "sample_data = create_sample_multimodal_dataset()\n",
    "print(f\"ìƒ˜í”Œ ë°ì´í„°ì…‹ ìƒì„± ì™„ë£Œ: {len(sample_data)}ê°œ\")\n",
    "print(f\"\\nì²« ë²ˆì§¸ ìƒ˜í”Œ:\")\n",
    "print(json.dumps(sample_data[0], indent=2, ensure_ascii=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ì´ë¯¸ì§€ ë¡œë“œ ì„±ê³µ: (300, 225)\n"
     ]
    }
   ],
   "source": [
    "# ì´ë¯¸ì§€ ë¡œë“œ ìœ í‹¸ë¦¬í‹°\n",
    "def load_image(image_path_or_url: str, timeout: int = 20) -> Image.Image:\n",
    "    \"\"\"\n",
    "    ë¡œì»¬ ê²½ë¡œ ë˜ëŠ” URLì—ì„œ ì´ë¯¸ì§€ë¥¼ ë¡œë“œí•˜ëŠ” í•¨ìˆ˜ë‹¤.\n",
    "    - URLì´ ì´ë¯¸ì§€ê°€ ì•„ë‹Œ ì‘ë‹µ(HTML/JSON/403 ë“±)ì„ ì£¼ë©´ ì›ì¸ íŒŒì•… ê°€ëŠ¥í•œ ì—ëŸ¬ë¥¼ ë˜ì§„ë‹¤.\n",
    "    \"\"\"\n",
    "    if image_path_or_url.startswith(\"http\"):\n",
    "        headers = {\n",
    "            \"User-Agent\": \"Mozilla/5.0\",\n",
    "            \"Accept\": \"image/avif,image/webp,image/apng,image/*,*/*;q=0.8\",\n",
    "        }\n",
    "        r = requests.get(\n",
    "            image_path_or_url,\n",
    "            headers=headers,\n",
    "            timeout=timeout,\n",
    "            allow_redirects=True,\n",
    "        )\n",
    "        r.raise_for_status()\n",
    "\n",
    "        ctype = (r.headers.get(\"Content-Type\") or \"\").lower()\n",
    "        if not ctype.startswith(\"image/\"):\n",
    "            snippet = (r.text[:300] if r.text else \"\").replace(\"\\n\", \" \")\n",
    "            raise ValueError(\n",
    "                f\"URLì´ ì´ë¯¸ì§€ë¥¼ ë°˜í™˜í•˜ì§€ ì•ŠìŒ. Content-Type={ctype}, body[:300]={snippet}\"\n",
    "            )\n",
    "\n",
    "        try:\n",
    "            return Image.open(BytesIO(r.content)).convert(\"RGB\")\n",
    "        except Exception as e:\n",
    "            raise ValueError(f\"ì´ë¯¸ì§€ ë””ì½”ë”© ì‹¤íŒ¨: {e} (Content-Type={ctype})\") from e\n",
    "\n",
    "    # local path\n",
    "    return Image.open(image_path_or_url).convert(\"RGB\")\n",
    "\n",
    "\n",
    "# í…ŒìŠ¤íŠ¸\n",
    "test_image = load_image(sample_data[0][\"image\"])\n",
    "print(f\"ì´ë¯¸ì§€ ë¡œë“œ ì„±ê³µ: {test_image.size}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 ê³µê°œ ë©€í‹°ëª¨ë‹¬ ë°ì´í„°ì…‹"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ê³µê°œ ë©€í‹°ëª¨ë‹¬ ë°ì´í„°ì…‹:\n",
      "\n",
      "[image_caption]\n",
      "  - HuggingFaceM4/COCO\n",
      "  - google-research-datasets/conceptual_captions\n",
      "\n",
      "[vqa]\n",
      "  - HuggingFaceM4/VQAv2\n",
      "  - textvqa/textvqa\n",
      "\n",
      "[conversation]\n",
      "  - liuhaotian/LLaVA-Instruct-150K\n",
      "  - Lin-Chen/ShareGPT4V\n",
      "\n",
      "[document]\n",
      "  - HuggingFaceM4/DocumentVQA\n",
      "  - naver-clova-ix/synthdog-en\n"
     ]
    }
   ],
   "source": [
    "# ê³µê°œ ë©€í‹°ëª¨ë‹¬ ë°ì´í„°ì…‹ ëª©ë¡\n",
    "public_datasets = {\n",
    "    # ì´ë¯¸ì§€-í…ìŠ¤íŠ¸ ìŒ\n",
    "    \"image_caption\": [\n",
    "        \"HuggingFaceM4/COCO\",\n",
    "        \"google-research-datasets/conceptual_captions\",\n",
    "    ],\n",
    "    \n",
    "    # VQA (Visual Question Answering)\n",
    "    \"vqa\": [\n",
    "        \"HuggingFaceM4/VQAv2\",\n",
    "        \"textvqa/textvqa\",\n",
    "    ],\n",
    "    \n",
    "    # ë©€í‹°ëª¨ë‹¬ ëŒ€í™”\n",
    "    \"conversation\": [\n",
    "        \"liuhaotian/LLaVA-Instruct-150K\",\n",
    "        \"Lin-Chen/ShareGPT4V\",\n",
    "    ],\n",
    "    \n",
    "    # ë¬¸ì„œ ì´í•´\n",
    "    \"document\": [\n",
    "        \"HuggingFaceM4/DocumentVQA\",\n",
    "        \"naver-clova-ix/synthdog-en\",\n",
    "    ]\n",
    "}\n",
    "\n",
    "print(\"ê³µê°œ ë©€í‹°ëª¨ë‹¬ ë°ì´í„°ì…‹:\")\n",
    "for category, datasets in public_datasets.items():\n",
    "    print(f\"\\n[{category}]\")\n",
    "    for ds in datasets:\n",
    "        print(f\"  - {ds}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "COCO ë°ì´í„°ì…‹ ë¡œë“œ í•¨ìˆ˜ ì¤€ë¹„ ì™„ë£Œ\n"
     ]
    }
   ],
   "source": [
    "# COCO ìº¡ì…˜ ë°ì´í„°ì…‹ ë¡œë“œ ì˜ˆì‹œ\n",
    "def load_coco_sample():\n",
    "    \"\"\"\n",
    "    COCO ìº¡ì…˜ ë°ì´í„°ì…‹ ìƒ˜í”Œì„ ë¡œë“œí•˜ëŠ” í•¨ìˆ˜ë‹¤.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        dataset = load_dataset(\"HuggingFaceM4/COCO\", split=\"train[:100]\")\n",
    "        print(f\"COCO ë°ì´í„°ì…‹ ë¡œë“œ ì™„ë£Œ: {len(dataset)}ê°œ\")\n",
    "        print(f\"ì»¬ëŸ¼: {dataset.column_names}\")\n",
    "        return dataset\n",
    "    except Exception as e:\n",
    "        print(f\"ë°ì´í„°ì…‹ ë¡œë“œ ì‹¤íŒ¨: {e}\")\n",
    "        return None\n",
    "\n",
    "# coco_dataset = load_coco_sample()\n",
    "print(\"COCO ë°ì´í„°ì…‹ ë¡œë“œ í•¨ìˆ˜ ì¤€ë¹„ ì™„ë£Œ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. LLaVA ëª¨ë¸ Fine-tuning\n",
    "\n",
    "### 6.1 LLaVA (Large Language and Vision Assistant)\n",
    "\n",
    "LLaVAëŠ” ê°€ì¥ ë„ë¦¬ ì‚¬ìš©ë˜ëŠ” ì˜¤í”ˆì†ŒìŠ¤ LVM ì¤‘ í•˜ë‚˜ë¡œ, ê°„ë‹¨í•˜ë©´ì„œë„ íš¨ê³¼ì ì¸ êµ¬ì¡°ë¥¼ ê°€ì§€ê³  ìˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ì‚¬ìš© ê°€ëŠ¥í•œ LLaVA ëª¨ë¸:\n",
      "  - llava-1.5-7b: llava-hf/llava-1.5-7b-hf\n",
      "  - llava-1.5-13b: llava-hf/llava-1.5-13b-hf\n",
      "  - llava-v1.6-mistral-7b: llava-hf/llava-v1.6-mistral-7b-hf\n",
      "  - llava-v1.6-vicuna-7b: llava-hf/llava-v1.6-vicuna-7b-hf\n"
     ]
    }
   ],
   "source": [
    "# LLaVA ëª¨ë¸ ì„¤ì •\n",
    "LLAVA_MODELS = {\n",
    "    \"llava-1.5-7b\": \"llava-hf/llava-1.5-7b-hf\",\n",
    "    \"llava-1.5-13b\": \"llava-hf/llava-1.5-13b-hf\",\n",
    "    \"llava-v1.6-mistral-7b\": \"llava-hf/llava-v1.6-mistral-7b-hf\",\n",
    "    \"llava-v1.6-vicuna-7b\": \"llava-hf/llava-v1.6-vicuna-7b-hf\",\n",
    "}\n",
    "\n",
    "# í…ŒìŠ¤íŠ¸ìš©ìœ¼ë¡œ ì‘ì€ ëª¨ë¸ ì„ íƒ\n",
    "MODEL_ID = \"llava-hf/llava-1.5-7b-hf\"\n",
    "\n",
    "print(\"ì‚¬ìš© ê°€ëŠ¥í•œ LLaVA ëª¨ë¸:\")\n",
    "for name, model_id in LLAVA_MODELS.items():\n",
    "    print(f\"  - {name}: {model_id}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4-bit ì–‘ìí™” ì„¤ì • ì™„ë£Œ\n"
     ]
    }
   ],
   "source": [
    "# 4-bit ì–‘ìí™” ì„¤ì • (ë©”ëª¨ë¦¬ ì ˆì•½)\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    ")\n",
    "\n",
    "print(\"4-bit ì–‘ìí™” ì„¤ì • ì™„ë£Œ\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLaVA ë¡œë“œ í•¨ìˆ˜ ì¤€ë¹„ ì™„ë£Œ\n"
     ]
    }
   ],
   "source": [
    "def load_llava_model(model_id, use_4bit=False):  # use_4bit=Falseë¡œ ë³€ê²½\n",
    "    \"\"\"\n",
    "    LLaVA ëª¨ë¸ê³¼ í”„ë¡œì„¸ì„œë¥¼ ë¡œë“œí•˜ëŠ” í•¨ìˆ˜ë‹¤.\n",
    "    \"\"\"\n",
    "    print(f\"LLaVA ëª¨ë¸ ë¡œë”©: {model_id}\")\n",
    "    \n",
    "    processor = AutoProcessor.from_pretrained(model_id)\n",
    "    \n",
    "    model = LlavaForConditionalGeneration.from_pretrained(\n",
    "        model_id,\n",
    "        torch_dtype=torch.bfloat16,\n",
    "        device_map=\"auto\",\n",
    "        trust_remote_code=True,\n",
    "        # ì–‘ìí™” ì—†ì´ ë¡œë“œ\n",
    "    )\n",
    "    \n",
    "    print(f\"ëª¨ë¸ ë¡œë“œ ì™„ë£Œ: {model.num_parameters():,} íŒŒë¼ë¯¸í„°\")\n",
    "    \n",
    "    return model, processor\n",
    "\n",
    "# ë¡œë“œ (ì‹¤ì œ ì‹¤í–‰ ì‹œ ì£¼ì„ í•´ì œ)\n",
    "# model, processor = load_llava_model(MODEL_ID, use_4bit=True)\n",
    "print(\"LLaVA ë¡œë“œ í•¨ìˆ˜ ì¤€ë¹„ ì™„ë£Œ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2 LLaVAìš© LoRA ì„¤ì •"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLaVA LoRA ì„¤ì •:\n",
      "  r: 16\n",
      "  alpha: 32\n",
      "  target_modules: {'o_proj', 'v_proj', 'k_proj', 'gate_proj', 'q_proj', 'down_proj', 'up_proj'}\n",
      "  modules_to_save: None\n"
     ]
    }
   ],
   "source": [
    "def get_llava_lora_config():\n",
    "    \"\"\"\n",
    "    LLaVA ëª¨ë¸ì— ìµœì í™”ëœ LoRA ì„¤ì •ì„ ë°˜í™˜í•˜ëŠ” í•¨ìˆ˜ë‹¤.\n",
    "    \"\"\"\n",
    "    lora_config = LoraConfig(\n",
    "        r=16,\n",
    "        lora_alpha=32,\n",
    "        target_modules=[\n",
    "            \"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",  # Attention\n",
    "            \"gate_proj\", \"up_proj\", \"down_proj\",     # MLP\n",
    "        ],\n",
    "        lora_dropout=0.05,\n",
    "        bias=\"none\",\n",
    "        task_type=\"CAUSAL_LM\",\n",
    "    )\n",
    "    return lora_config\n",
    "\n",
    "llava_lora_config = get_llava_lora_config()\n",
    "print(\"LLaVA LoRA ì„¤ì •:\")\n",
    "print(f\"  r: {llava_lora_config.r}\")\n",
    "print(f\"  alpha: {llava_lora_config.lora_alpha}\")\n",
    "print(f\"  target_modules: {llava_lora_config.target_modules}\")\n",
    "print(f\"  modules_to_save: {llava_lora_config.modules_to_save}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLaVA í•™ìŠµ ì¤€ë¹„ í•¨ìˆ˜ ì¤€ë¹„ ì™„ë£Œ\n"
     ]
    }
   ],
   "source": [
    "def prepare_llava_for_training(model):\n",
    "    \"\"\"\n",
    "    LLaVA ëª¨ë¸ì„ Fine-tuningìš©ìœ¼ë¡œ ì¤€ë¹„í•˜ëŠ” í•¨ìˆ˜ë‹¤.\n",
    "    \"\"\"\n",
    "    # 1. Vision Encoder ë™ê²°\n",
    "    for param in model.vision_tower.parameters():\n",
    "        param.requires_grad = False\n",
    "    print(\"âœ“ Vision Encoder ë™ê²° ì™„ë£Œ\")\n",
    "    \n",
    "    # 2. Projector í•™ìŠµ ê°€ëŠ¥ ì„¤ì •\n",
    "    for param in model.multi_modal_projector.parameters():\n",
    "        param.requires_grad = True\n",
    "    print(\"âœ“ Projector í•™ìŠµ ê°€ëŠ¥ ì„¤ì • ì™„ë£Œ\")\n",
    "    \n",
    "    # 3. LoRA ì ìš©\n",
    "    lora_config = LoraConfig(\n",
    "        r=16,\n",
    "        lora_alpha=32,\n",
    "        target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"],\n",
    "        lora_dropout=0.05,\n",
    "        bias=\"none\",\n",
    "        task_type=\"CAUSAL_LM\",\n",
    "        modules_to_save=[\"multi_modal_projector\"],  # ì–‘ìí™” ì—†ì„ ë•Œë§Œ ì‚¬ìš© ê°€ëŠ¥\n",
    "    )\n",
    "    model = get_peft_model(model, lora_config)\n",
    "    print(\"âœ“ LoRA ì ìš© ì™„ë£Œ\")\n",
    "    \n",
    "    model.print_trainable_parameters()\n",
    "    \n",
    "    return model\n",
    "\n",
    "print(\"LLaVA í•™ìŠµ ì¤€ë¹„ í•¨ìˆ˜ ì¤€ë¹„ ì™„ë£Œ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.3 ë°ì´í„° ì „ì²˜ë¦¬"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ë°ì´í„° ì „ì²˜ë¦¬ í•¨ìˆ˜ ì¤€ë¹„ ì™„ë£Œ\n"
     ]
    }
   ],
   "source": [
    "def preprocess_llava_data(examples, processor, max_length=2048):\n",
    "    \"\"\"\n",
    "    LLaVA í•™ìŠµ ë°ì´í„°ë¥¼ ì „ì²˜ë¦¬í•˜ëŠ” í•¨ìˆ˜ë‹¤.\n",
    "    \"\"\"\n",
    "    images = []\n",
    "    texts = []\n",
    "    \n",
    "    for example in examples:\n",
    "        # ì´ë¯¸ì§€ ë¡œë“œ\n",
    "        image = load_image(example[\"image\"])\n",
    "        images.append(image)\n",
    "        \n",
    "        # ëŒ€í™” í˜•ì‹ êµ¬ì„±\n",
    "        conversation = example[\"conversations\"]\n",
    "        \n",
    "        # LLaVA í˜•ì‹ìœ¼ë¡œ ë³€í™˜\n",
    "        text = \"\"\n",
    "        for turn in conversation:\n",
    "            if turn[\"from\"] == \"human\":\n",
    "                text += f\"USER: {turn['value']}\\n\"\n",
    "            else:\n",
    "                text += f\"ASSISTANT: {turn['value']}\\n\"\n",
    "        \n",
    "        texts.append(text)\n",
    "    \n",
    "    # í”„ë¡œì„¸ì„œë¡œ ë°°ì¹˜ ì²˜ë¦¬\n",
    "    batch = processor(\n",
    "        text=texts,\n",
    "        images=images,\n",
    "        padding=True,\n",
    "        truncation=True,\n",
    "        max_length=max_length,\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "    \n",
    "    # labels ì„¤ì •\n",
    "    batch[\"labels\"] = batch[\"input_ids\"].clone()\n",
    "    \n",
    "    return batch\n",
    "\n",
    "print(\"ë°ì´í„° ì „ì²˜ë¦¬ í•¨ìˆ˜ ì¤€ë¹„ ì™„ë£Œ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.4 LLaVA Fine-tuning ì‹¤í–‰"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "LLaVA Fine-tuning ì‹œì‘\n",
      "============================================================\n",
      "\n",
      "[1/4] ëª¨ë¸ ë¡œë”©...\n",
      "LLaVA ëª¨ë¸ ë¡œë”©: llava-hf/llava-1.5-7b-hf\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f3b1e7c3abbe4af6a5c0a1e925cccead",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ëª¨ë¸ ë¡œë“œ ì™„ë£Œ: 7,063,427,072 íŒŒë¼ë¯¸í„°\n",
      "\n",
      "[2/4] í•™ìŠµ ì¤€ë¹„...\n",
      "âœ“ Vision Encoder ë™ê²° ì™„ë£Œ\n",
      "âœ“ Projector í•™ìŠµ ê°€ëŠ¥ ì„¤ì • ì™„ë£Œ\n",
      "âœ“ LoRA ì ìš© ì™„ë£Œ\n",
      "trainable params: 40,116,224 || all params: 7,103,543,296 || trainable%: 0.5647\n",
      "\n",
      "[3/4] ë°ì´í„° ì „ì²˜ë¦¬...\n"
     ]
    },
    {
     "ename": "HTTPError",
     "evalue": "429 Client Error: Too many requests. Please contact noc@wikimedia.org for further information (0068e25) for url: https://upload.wikimedia.org/wikipedia/commons/thumb/b/b6/Image_created_with_a_mobile_phone.png/1200px-Image_created_with_a_mobile_phone.png",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mHTTPError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[37]\u001b[39m\u001b[32m, line 66\u001b[39m\n\u001b[32m     63\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m model, processor\n\u001b[32m     65\u001b[39m \u001b[38;5;66;03m# Train\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m66\u001b[39m model, processor = \u001b[43mfinetune_llava\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     67\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel_id\u001b[49m\u001b[43m=\u001b[49m\u001b[43mMODEL_ID\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     68\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtrain_data\u001b[49m\u001b[43m=\u001b[49m\u001b[43msample_data\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     69\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[43mos\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m.\u001b[49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43mOUTPUT_DIR\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mLLAVA_Finetuned\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     70\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m     72\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mLLaVA Fine-tuning í•¨ìˆ˜ ì¤€ë¹„ ì™„ë£Œ\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[37]\u001b[39m\u001b[32m, line 26\u001b[39m, in \u001b[36mfinetune_llava\u001b[39m\u001b[34m(model_id, train_data, output_dir, num_epochs, learning_rate, batch_size)\u001b[39m\n\u001b[32m     24\u001b[39m \u001b[38;5;66;03m# 3. ë°ì´í„° ì „ì²˜ë¦¬\u001b[39;00m\n\u001b[32m     25\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m[3/4] ë°ì´í„° ì „ì²˜ë¦¬...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m26\u001b[39m train_dataset = \u001b[43mpreprocess_llava_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprocessor\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     28\u001b[39m \u001b[38;5;66;03m# 4. í•™ìŠµ ì„¤ì •\u001b[39;00m\n\u001b[32m     29\u001b[39m training_args = TrainingArguments(\n\u001b[32m     30\u001b[39m     output_dir=output_dir,\n\u001b[32m     31\u001b[39m     num_train_epochs=num_epochs,\n\u001b[32m   (...)\u001b[39m\u001b[32m     41\u001b[39m     report_to=\u001b[33m\"\u001b[39m\u001b[33mnone\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     42\u001b[39m )\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[36]\u001b[39m\u001b[32m, line 10\u001b[39m, in \u001b[36mpreprocess_llava_data\u001b[39m\u001b[34m(examples, processor, max_length)\u001b[39m\n\u001b[32m      6\u001b[39m texts = []\n\u001b[32m      8\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m example \u001b[38;5;129;01min\u001b[39;00m examples:\n\u001b[32m      9\u001b[39m     \u001b[38;5;66;03m# ì´ë¯¸ì§€ ë¡œë“œ\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m10\u001b[39m     image = \u001b[43mload_image\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexample\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mimage\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     11\u001b[39m     images.append(image)\n\u001b[32m     13\u001b[39m     \u001b[38;5;66;03m# ëŒ€í™” í˜•ì‹ êµ¬ì„±\u001b[39;00m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[16]\u001b[39m\u001b[32m, line 18\u001b[39m, in \u001b[36mload_image\u001b[39m\u001b[34m(image_path_or_url, timeout)\u001b[39m\n\u001b[32m      8\u001b[39m headers = {\n\u001b[32m      9\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mUser-Agent\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33mMozilla/5.0\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     10\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mAccept\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33mimage/avif,image/webp,image/apng,image/*,*/*;q=0.8\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     11\u001b[39m }\n\u001b[32m     12\u001b[39m r = requests.get(\n\u001b[32m     13\u001b[39m     image_path_or_url,\n\u001b[32m     14\u001b[39m     headers=headers,\n\u001b[32m     15\u001b[39m     timeout=timeout,\n\u001b[32m     16\u001b[39m     allow_redirects=\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[32m     17\u001b[39m )\n\u001b[32m---> \u001b[39m\u001b[32m18\u001b[39m \u001b[43mr\u001b[49m\u001b[43m.\u001b[49m\u001b[43mraise_for_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     20\u001b[39m ctype = (r.headers.get(\u001b[33m\"\u001b[39m\u001b[33mContent-Type\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m).lower()\n\u001b[32m     21\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m ctype.startswith(\u001b[33m\"\u001b[39m\u001b[33mimage/\u001b[39m\u001b[33m\"\u001b[39m):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/llm/lib/python3.11/site-packages/requests/models.py:1026\u001b[39m, in \u001b[36mResponse.raise_for_status\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1021\u001b[39m     http_error_msg = (\n\u001b[32m   1022\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m.status_code\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m Server Error: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mreason\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m for url: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m.url\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m   1023\u001b[39m     )\n\u001b[32m   1025\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m http_error_msg:\n\u001b[32m-> \u001b[39m\u001b[32m1026\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m HTTPError(http_error_msg, response=\u001b[38;5;28mself\u001b[39m)\n",
      "\u001b[31mHTTPError\u001b[39m: 429 Client Error: Too many requests. Please contact noc@wikimedia.org for further information (0068e25) for url: https://upload.wikimedia.org/wikipedia/commons/thumb/b/b6/Image_created_with_a_mobile_phone.png/1200px-Image_created_with_a_mobile_phone.png"
     ]
    }
   ],
   "source": [
    "def finetune_llava(\n",
    "    model_id,\n",
    "    train_data,\n",
    "    output_dir,\n",
    "    num_epochs=3,\n",
    "    learning_rate=2e-5,\n",
    "    batch_size=2,\n",
    "):\n",
    "    \"\"\"\n",
    "    LLaVA ëª¨ë¸ì„ Fine-tuningí•˜ëŠ” í•¨ìˆ˜ë‹¤.\n",
    "    \"\"\"\n",
    "    print(\"=\" * 60)\n",
    "    print(\"LLaVA Fine-tuning ì‹œì‘\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # 1. ëª¨ë¸ ë¡œë“œ\n",
    "    print(\"\\n[1/4] ëª¨ë¸ ë¡œë”©...\")\n",
    "    model, processor = load_llava_model(model_id, use_4bit=True)\n",
    "    \n",
    "    # 2. í•™ìŠµ ì¤€ë¹„\n",
    "    print(\"\\n[2/4] í•™ìŠµ ì¤€ë¹„...\")\n",
    "    model = prepare_llava_for_training(model)\n",
    "    \n",
    "    # 3. ë°ì´í„° ì „ì²˜ë¦¬\n",
    "    print(\"\\n[3/4] ë°ì´í„° ì „ì²˜ë¦¬...\")\n",
    "    train_dataset = preprocess_llava_data(train_data, processor)\n",
    "    \n",
    "    # 4. í•™ìŠµ ì„¤ì •\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=output_dir,\n",
    "        num_train_epochs=num_epochs,\n",
    "        per_device_train_batch_size=batch_size,\n",
    "        gradient_accumulation_steps=4,\n",
    "        learning_rate=learning_rate,\n",
    "        warmup_ratio=0.1,\n",
    "        lr_scheduler_type=\"cosine\",\n",
    "        bf16=True,\n",
    "        logging_steps=10,\n",
    "        save_strategy=\"epoch\",\n",
    "        remove_unused_columns=False,\n",
    "        report_to=\"none\",\n",
    "    )\n",
    "    \n",
    "    # 5. Trainer ìƒì„± ë° í•™ìŠµ\n",
    "    print(\"\\n[4/4] í•™ìŠµ ì‹œì‘...\")\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=train_dataset,\n",
    "    )\n",
    "    \n",
    "    trainer.train()\n",
    "    \n",
    "    # ì €ì¥\n",
    "    trainer.save_model(output_dir)\n",
    "    processor.save_pretrained(output_dir)\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"LLaVA Fine-tuning ì™„ë£Œ!\")\n",
    "    print(f\"ëª¨ë¸ ì €ì¥ ìœ„ì¹˜: {output_dir}\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    return model, processor\n",
    "\n",
    "# Train\n",
    "model, processor = finetune_llava(\n",
    "    model_id=MODEL_ID,\n",
    "    train_data=sample_data,\n",
    "    output_dir=os.path.join(OUTPUT_DIR, \"LLAVA_Finetuned\"),\n",
    ")\n",
    "\n",
    "print(\"LLaVA Fine-tuning í•¨ìˆ˜ ì¤€ë¹„ ì™„ë£Œ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Qwen2-VL Fine-tuning\n",
    "\n",
    "### 7.1 Qwen2-VL ì†Œê°œ\n",
    "\n",
    "Qwen2-VLì€ Alibabaì—ì„œ ê°œë°œí•œ ìµœì‹  LVMìœ¼ë¡œ, ë™ì  í•´ìƒë„ì™€ ë™ì˜ìƒ ì´í•´ë¥¼ ì§€ì›í•œë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ì‚¬ìš© ê°€ëŠ¥í•œ Qwen2-VL ëª¨ë¸:\n",
      "  - qwen2-vl-2b: Qwen/Qwen2-VL-2B-Instruct\n",
      "  - qwen2-vl-7b: Qwen/Qwen2-VL-7B-Instruct\n",
      "  - qwen2-vl-72b: Qwen/Qwen2-VL-72B-Instruct\n"
     ]
    }
   ],
   "source": [
    "# Qwen2-VL ëª¨ë¸ ì„¤ì •\n",
    "QWEN2_VL_MODELS = {\n",
    "    \"qwen2-vl-2b\": \"Qwen/Qwen2-VL-2B-Instruct\",\n",
    "    \"qwen2-vl-7b\": \"Qwen/Qwen2-VL-7B-Instruct\",\n",
    "    \"qwen2-vl-72b\": \"Qwen/Qwen2-VL-72B-Instruct\",\n",
    "}\n",
    "\n",
    "print(\"ì‚¬ìš© ê°€ëŠ¥í•œ Qwen2-VL ëª¨ë¸:\")\n",
    "for name, model_id in QWEN2_VL_MODELS.items():\n",
    "    print(f\"  - {name}: {model_id}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Qwen2-VL ë¡œë“œ í•¨ìˆ˜ ì¤€ë¹„ ì™„ë£Œ\n"
     ]
    }
   ],
   "source": [
    "def load_qwen2_vl_model(model_id, use_4bit=True):\n",
    "    \"\"\"\n",
    "    Qwen2-VL ëª¨ë¸ê³¼ í”„ë¡œì„¸ì„œë¥¼ ë¡œë“œí•˜ëŠ” í•¨ìˆ˜ë‹¤.\n",
    "    \"\"\"\n",
    "    print(f\"Qwen2-VL ëª¨ë¸ ë¡œë”©: {model_id}\")\n",
    "    \n",
    "    # í”„ë¡œì„¸ì„œ ë¡œë“œ\n",
    "    processor = AutoProcessor.from_pretrained(model_id)\n",
    "    \n",
    "    # ëª¨ë¸ ë¡œë“œ\n",
    "    model_kwargs = {\n",
    "        \"torch_dtype\": torch.bfloat16,\n",
    "        \"device_map\": \"auto\",\n",
    "        \"trust_remote_code\": True,\n",
    "    }\n",
    "    \n",
    "    if use_4bit:\n",
    "        model_kwargs[\"quantization_config\"] = bnb_config\n",
    "    \n",
    "    model = Qwen2VLForConditionalGeneration.from_pretrained(\n",
    "        model_id,\n",
    "        **model_kwargs\n",
    "    )\n",
    "    \n",
    "    print(f\"ëª¨ë¸ ë¡œë“œ ì™„ë£Œ: {model.num_parameters():,} íŒŒë¼ë¯¸í„°\")\n",
    "    \n",
    "    return model, processor\n",
    "\n",
    "print(\"Qwen2-VL ë¡œë“œ í•¨ìˆ˜ ì¤€ë¹„ ì™„ë£Œ\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Qwen2-VL LoRA ì„¤ì • í•¨ìˆ˜ ì¤€ë¹„ ì™„ë£Œ\n"
     ]
    }
   ],
   "source": [
    "def get_qwen2_vl_lora_config():\n",
    "    \"\"\"\n",
    "    Qwen2-VL ëª¨ë¸ì— ìµœì í™”ëœ LoRA ì„¤ì •ì„ ë°˜í™˜í•˜ëŠ” í•¨ìˆ˜ë‹¤.\n",
    "    \"\"\"\n",
    "    lora_config = LoraConfig(\n",
    "        r=16,\n",
    "        lora_alpha=32,\n",
    "        target_modules=[\n",
    "            \"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "            \"gate_proj\", \"up_proj\", \"down_proj\",\n",
    "        ],\n",
    "        lora_dropout=0.05,\n",
    "        bias=\"none\",\n",
    "        task_type=\"CAUSAL_LM\",\n",
    "    )\n",
    "    return lora_config\n",
    "\n",
    "print(\"Qwen2-VL LoRA ì„¤ì • í•¨ìˆ˜ ì¤€ë¹„ ì™„ë£Œ\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Qwen2-VL ë°ì´í„° ì „ì²˜ë¦¬ í•¨ìˆ˜ ì¤€ë¹„ ì™„ë£Œ\n"
     ]
    }
   ],
   "source": [
    "def preprocess_qwen2_vl_data(examples, processor, max_length=2048):\n",
    "    \"\"\"\n",
    "    Qwen2-VL í•™ìŠµ ë°ì´í„°ë¥¼ ì „ì²˜ë¦¬í•˜ëŠ” í•¨ìˆ˜ë‹¤.\n",
    "    \"\"\"\n",
    "    processed_data = []\n",
    "    \n",
    "    for example in examples:\n",
    "        # ì´ë¯¸ì§€ ë¡œë“œ\n",
    "        image = load_image(example[\"image\"])\n",
    "        \n",
    "        # Qwen2-VL í˜•ì‹ìœ¼ë¡œ ë©”ì‹œì§€ êµ¬ì„±\n",
    "        messages = []\n",
    "        for turn in example[\"conversations\"]:\n",
    "            if turn[\"from\"] == \"human\":\n",
    "                # ì´ë¯¸ì§€ê°€ í¬í•¨ëœ ì²« ë²ˆì§¸ ì‚¬ìš©ì ë©”ì‹œì§€\n",
    "                if \"<image>\" in turn[\"value\"]:\n",
    "                    content = [\n",
    "                        {\"type\": \"image\", \"image\": image},\n",
    "                        {\"type\": \"text\", \"text\": turn[\"value\"].replace(\"<image>\\n\", \"\")}\n",
    "                    ]\n",
    "                else:\n",
    "                    content = [{\"type\": \"text\", \"text\": turn[\"value\"]}]\n",
    "                messages.append({\"role\": \"user\", \"content\": content})\n",
    "            else:\n",
    "                messages.append({\"role\": \"assistant\", \"content\": turn[\"value\"]})\n",
    "        \n",
    "        # í”„ë¡œì„¸ì„œë¡œ ì²˜ë¦¬\n",
    "        text = processor.apply_chat_template(\n",
    "            messages,\n",
    "            tokenize=False,\n",
    "            add_generation_prompt=False\n",
    "        )\n",
    "        \n",
    "        processed_data.append({\n",
    "            \"text\": text,\n",
    "            \"images\": [image]\n",
    "        })\n",
    "    \n",
    "    return processed_data\n",
    "\n",
    "print(\"Qwen2-VL ë°ì´í„° ì „ì²˜ë¦¬ í•¨ìˆ˜ ì¤€ë¹„ ì™„ë£Œ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. ëª¨ë¸ ì¶”ë¡  í…ŒìŠ¤íŠ¸"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLaVA ì¶”ë¡  í•¨ìˆ˜ ì¤€ë¹„ ì™„ë£Œ\n"
     ]
    }
   ],
   "source": [
    "def test_llava_inference(model, processor, image_path, question):\n",
    "    \"\"\"\n",
    "    LLaVA ëª¨ë¸ë¡œ ì¶”ë¡ ì„ ìˆ˜í–‰í•˜ëŠ” í•¨ìˆ˜ë‹¤.\n",
    "    \"\"\"\n",
    "    # ì´ë¯¸ì§€ ë¡œë“œ\n",
    "    image = load_image(image_path)\n",
    "    \n",
    "    # í”„ë¡¬í”„íŠ¸ êµ¬ì„±\n",
    "    prompt = f\"USER: <image>\\n{question}\\nASSISTANT:\"\n",
    "    \n",
    "    # ì…ë ¥ ì²˜ë¦¬\n",
    "    inputs = processor(\n",
    "        text=prompt,\n",
    "        images=image,\n",
    "        return_tensors=\"pt\"\n",
    "    ).to(model.device)\n",
    "    \n",
    "    # ìƒì„±\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=256,\n",
    "            temperature=0.7,\n",
    "            do_sample=True,\n",
    "        )\n",
    "    \n",
    "    # ë””ì½”ë”©\n",
    "    response = processor.decode(outputs[0], skip_special_tokens=True)\n",
    "    \n",
    "    # ASSISTANT: ì´í›„ ë¶€ë¶„ë§Œ ì¶”ì¶œ\n",
    "    answer = response.split(\"ASSISTANT:\")[-1].strip()\n",
    "    \n",
    "    return answer\n",
    "\n",
    "print(\"LLaVA ì¶”ë¡  í•¨ìˆ˜ ì¤€ë¹„ ì™„ë£Œ\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Qwen2-VL ì¶”ë¡  í•¨ìˆ˜ ì¤€ë¹„ ì™„ë£Œ\n"
     ]
    }
   ],
   "source": [
    "def test_qwen2_vl_inference(model, processor, image_path, question):\n",
    "    \"\"\"\n",
    "    Qwen2-VL ëª¨ë¸ë¡œ ì¶”ë¡ ì„ ìˆ˜í–‰í•˜ëŠ” í•¨ìˆ˜ë‹¤.\n",
    "    \"\"\"\n",
    "    # ì´ë¯¸ì§€ ë¡œë“œ\n",
    "    image = load_image(image_path)\n",
    "    \n",
    "    # ë©”ì‹œì§€ êµ¬ì„±\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [\n",
    "                {\"type\": \"image\", \"image\": image},\n",
    "                {\"type\": \"text\", \"text\": question}\n",
    "            ]\n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    # í”„ë¡¬í”„íŠ¸ ìƒì„±\n",
    "    text = processor.apply_chat_template(\n",
    "        messages,\n",
    "        tokenize=False,\n",
    "        add_generation_prompt=True\n",
    "    )\n",
    "    \n",
    "    # ì…ë ¥ ì²˜ë¦¬\n",
    "    inputs = processor(\n",
    "        text=[text],\n",
    "        images=[image],\n",
    "        return_tensors=\"pt\",\n",
    "        padding=True\n",
    "    ).to(model.device)\n",
    "    \n",
    "    # ìƒì„±\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=256,\n",
    "            temperature=0.7,\n",
    "            do_sample=True,\n",
    "        )\n",
    "    \n",
    "    # ë””ì½”ë”©\n",
    "    response = processor.batch_decode(\n",
    "        outputs[:, inputs.input_ids.shape[1]:],\n",
    "        skip_special_tokens=True\n",
    "    )[0]\n",
    "    \n",
    "    return response\n",
    "\n",
    "print(\"Qwen2-VL ì¶”ë¡  í•¨ìˆ˜ ì¤€ë¹„ ì™„ë£Œ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. LVM Fine-tuning ë² ìŠ¤íŠ¸ í”„ë™í‹°ìŠ¤\n",
    "\n",
    "### **LVM íŒŒì¸íŠœë‹ í•µì‹¬ ê°€ì´ë“œ**\n",
    "\n",
    "**1. í•™ìŠµ ì „ëµ (Training Strategy)**\n",
    "ëª¨ë¸ì˜ ê° êµ¬ì„± ìš”ì†Œë³„ë¡œ í•™ìŠµ ì—¬ë¶€ë¥¼ ë‹¤ë¥´ê²Œ ì„¤ì •í•˜ì—¬ íš¨ìœ¨ì„±ì„ ê·¹ëŒ€í™”í•©ë‹ˆë‹¤.\n",
    "\n",
    "* **Vision Encoder:** ëŒ€ë¶€ë¶„ **ë™ê²°(Freeze)**í•˜ì—¬ í•™ìŠµí•˜ì§€ ì•ŠìŒ.\n",
    "* **Projector:** ì‹œê° ì •ë³´ì™€ ì–¸ì–´ ëª¨ë¸ì„ ì—°ê²°í•˜ëŠ” í•µì‹¬ì´ë¯€ë¡œ **í•„ìˆ˜ í•™ìŠµ**.\n",
    "* **LLM:** ì „ì²´ íŒŒë¼ë¯¸í„° í•™ìŠµë³´ë‹¤ëŠ” **LoRA(Low-Rank Adaptation)** ì ìš©ì„ ê¶Œì¥.\n",
    "\n",
    "**2. ë©”ëª¨ë¦¬ ìµœì í™” (Memory Optimization)**\n",
    "ê±°ëŒ€ ëª¨ë¸ì„ íš¨ìœ¨ì ìœ¼ë¡œ í•™ìŠµí•˜ê¸° ìœ„í•œ í•„ìˆ˜ ì„¤ì •ì…ë‹ˆë‹¤.\n",
    "\n",
    "* **ì–‘ìí™”:** 4-bit ì–‘ìí™”(QLoRA)ë¥¼ ì‚¬ìš©í•˜ì—¬ VRAM ì‚¬ìš©ëŸ‰ ì ˆê°.\n",
    "* **ì²´í¬í¬ì¸íŒ…:** Gradient Checkpointingì„ í™œì„±í™”í•˜ì—¬ ë©”ëª¨ë¦¬ íš¨ìœ¨ í™•ë³´.\n",
    "* **ë°°ì¹˜ ì„¤ì •:** ì‘ì€ ë°°ì¹˜ ì‚¬ì´ì¦ˆ(Batch size)ë¥¼ ì‚¬ìš©í•˜ë˜, **Gradient Accumulation(ê·¸ë˜ë””ì–¸íŠ¸ ëˆ„ì )**ì„ í†µí•´ í° ë°°ì¹˜ íš¨ê³¼ë¥¼ ëƒ„.\n",
    "\n",
    "**3. í•™ìŠµë¥  ì„¤ì • (Learning Rate)**\n",
    "í•™ìŠµ ëŒ€ìƒì— ë”°ë¼ í•™ìŠµë¥ (LR)ì„ ë‹¤ë¥´ê²Œ ê°€ì ¸ê°€ëŠ” ê²ƒì´ ì¤‘ìš”í•©ë‹ˆë‹¤.\n",
    "\n",
    "| í•™ìŠµ ëŒ€ìƒ | ê¶Œì¥ í•™ìŠµë¥  (Learning Rate) | ë¹„ê³  |\n",
    "| --- | --- | --- |\n",
    "| **Projector** | `1e-4` ~ `2e-4` | ë¹„êµì  ë†’ê²Œ ì„¤ì • |\n",
    "| **LLM (LoRA)** | `1e-5` ~ `2e-5` | ë¯¸ì„¸ ì¡°ì •ì´ë¯€ë¡œ ë‚®ê²Œ ì„¤ì • |\n",
    "| **í†µí•© í•™ìŠµ** | `2e-5` | Projectorì™€ LLM ë™ì‹œ í•™ìŠµ ì‹œ |\n",
    "\n",
    "**4. ë°ì´í„° í’ˆì§ˆ (Data Quality)**\n",
    "ëª¨ë¸ ì„±ëŠ¥ì— ê°€ì¥ í° ì˜í–¥ì„ ë¯¸ì¹˜ëŠ” ìš”ì†Œì…ë‹ˆë‹¤.\n",
    "\n",
    "* **ê³ í’ˆì§ˆ ë°ì´í„°:** ë…¸ì´ì¦ˆê°€ ì—†ëŠ” ëª…í™•í•œ ì´ë¯¸ì§€-í…ìŠ¤íŠ¸ ìŒ ì‚¬ìš©.\n",
    "* **í•´ìƒë„ ë‹¤ì–‘í™”:** ë‹¤ì–‘í•œ í¬ê¸°ì™€ ë¹„ìœ¨ì˜ ì´ë¯¸ì§€ë¥¼ í¬í•¨í•˜ì—¬ ì¼ë°˜í™” ì„±ëŠ¥ í–¥ìƒ.\n",
    "* **ìƒì„¸í•œ ì„¤ëª…:** ë‹¨ìˆœí•œ íƒœê¹…ë³´ë‹¤ëŠ” ìƒì„¸í•˜ê³  ì •í™•í•œ ìº¡ì…˜/ëŒ€í™” ë°ì´í„° ê¶Œì¥.\n",
    "\n",
    "**5. í‰ê°€ ì§€í‘œ (Evaluation)**\n",
    "ë‹¤ì–‘í•œ ê´€ì ì—ì„œ ëª¨ë¸ì˜ ì„±ëŠ¥ì„ ê²€ì¦í•©ë‹ˆë‹¤.\n",
    "\n",
    "* **ì •ëŸ‰ì  í‰ê°€ (ìº¡ì…”ë‹):** BLEU, CIDEr, METEOR ë“± í…ìŠ¤íŠ¸ ìœ ì‚¬ë„ ì§€í‘œ.\n",
    "* **ê³¼ì œë³„ í‰ê°€ (VQA):** ì§ˆë¬¸ì— ëŒ€í•œ ì •ë‹µë¥ (Accuracy).\n",
    "* **ì •ì„±ì  í‰ê°€:** GPT-4Vë¥¼ ì´ìš©í•œ ìë™ í‰ê°€ ë˜ëŠ” ì‚¬ëŒì— ì˜í•œ ì§ì ‘ í‰ê°€."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. ë„ë©”ì¸ íŠ¹í™” LVM Fine-tuning ì˜ˆì‹œ\n",
    "\n",
    "### 10.1 ì˜ë£Œ ì´ë¯¸ì§€ ë¶„ì„"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ì˜ë£Œ ì´ë¯¸ì§€ ë¶„ì„ ë°ì´í„° í˜•ì‹]\n",
      "{\n",
      "  \"id\": \"medical_001\",\n",
      "  \"image\": \"path/to/xray_image.jpg\",\n",
      "  \"conversations\": [\n",
      "    {\n",
      "      \"from\": \"human\",\n",
      "      \"value\": \"<image>\\nì´ í‰ë¶€ X-ray ì´ë¯¸ì§€ì—ì„œ ì´ìƒ ì†Œê²¬ì´ ìˆëŠ”ê°€?\"\n",
      "    },\n",
      "    {\n",
      "      \"from\": \"gpt\",\n",
      "      \"value\": \"ì´ í‰ë¶€ X-ray ì´ë¯¸ì§€ë¥¼ ë¶„ì„í•œ ê²°ê³¼, ì¢Œì¸¡ í í•˜ì—½ì— ê²½í™”(consolidation) ì†Œê²¬ì´ ê´€ì°°ëœë‹¤. ì´ëŠ” íë ´ì„ ì‹œì‚¬í•  ìˆ˜ ìˆìœ¼ë‚˜, ì •í™•í•œ ì§„ë‹¨ì„ ìœ„í•´ì„œëŠ” ì„ìƒ ì¦ìƒê³¼ í•¨ê»˜ ì¶”ê°€ì ì¸ ê²€ì‚¬ê°€ í•„ìš”í•˜ë‹¤. ë°˜ë“œì‹œ ì „ë¬¸ ì˜ë£Œì§„ê³¼ ìƒë‹´í•  ê²ƒì„ ê¶Œê³ í•œë‹¤.\"\n",
      "    }\n",
      "  ]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# ì˜ë£Œ ì´ë¯¸ì§€ ë¶„ì„ìš© ë°ì´í„° í˜•ì‹ ì˜ˆì‹œ\n",
    "medical_data_example = {\n",
    "    \"id\": \"medical_001\",\n",
    "    \"image\": \"path/to/xray_image.jpg\",\n",
    "    \"conversations\": [\n",
    "        {\n",
    "            \"from\": \"human\",\n",
    "            \"value\": \"<image>\\nì´ í‰ë¶€ X-ray ì´ë¯¸ì§€ì—ì„œ ì´ìƒ ì†Œê²¬ì´ ìˆëŠ”ê°€?\"\n",
    "        },\n",
    "        {\n",
    "            \"from\": \"gpt\",\n",
    "            \"value\": \"ì´ í‰ë¶€ X-ray ì´ë¯¸ì§€ë¥¼ ë¶„ì„í•œ ê²°ê³¼, ì¢Œì¸¡ í í•˜ì—½ì— ê²½í™”(consolidation) ì†Œê²¬ì´ ê´€ì°°ëœë‹¤. ì´ëŠ” íë ´ì„ ì‹œì‚¬í•  ìˆ˜ ìˆìœ¼ë‚˜, ì •í™•í•œ ì§„ë‹¨ì„ ìœ„í•´ì„œëŠ” ì„ìƒ ì¦ìƒê³¼ í•¨ê»˜ ì¶”ê°€ì ì¸ ê²€ì‚¬ê°€ í•„ìš”í•˜ë‹¤. ë°˜ë“œì‹œ ì „ë¬¸ ì˜ë£Œì§„ê³¼ ìƒë‹´í•  ê²ƒì„ ê¶Œê³ í•œë‹¤.\"\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "\n",
    "print(\"[ì˜ë£Œ ì´ë¯¸ì§€ ë¶„ì„ ë°ì´í„° í˜•ì‹]\")\n",
    "print(json.dumps(medical_data_example, indent=2, ensure_ascii=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10.2 ë¬¸ì„œ ì´í•´ (OCR + ì´í•´)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ë¬¸ì„œ ì´í•´ ë°ì´í„° í˜•ì‹]\n",
      "{\n",
      "  \"id\": \"document_001\",\n",
      "  \"image\": \"path/to/invoice.jpg\",\n",
      "  \"conversations\": [\n",
      "    {\n",
      "      \"from\": \"human\",\n",
      "      \"value\": \"<image>\\nì´ ì²­êµ¬ì„œì—ì„œ ì´ ê¸ˆì•¡ì€ ì–¼ë§ˆì¸ê°€?\"\n",
      "    },\n",
      "    {\n",
      "      \"from\": \"gpt\",\n",
      "      \"value\": \"ì´ ì²­êµ¬ì„œì˜ ì´ ê¸ˆì•¡ì€ 1,250,000ì›ì´ë‹¤. í’ˆëª©ë³„ë¡œ ë³´ë©´ ì œí’ˆ Aê°€ 500,000ì›, ì œí’ˆ Bê°€ 750,000ì›ì´ë©°, ë¶€ê°€ì„¸ í¬í•¨ ê¸ˆì•¡ì´ë‹¤.\"\n",
      "    }\n",
      "  ]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# ë¬¸ì„œ ì´í•´ìš© ë°ì´í„° í˜•ì‹ ì˜ˆì‹œ\n",
    "document_data_example = {\n",
    "    \"id\": \"document_001\",\n",
    "    \"image\": \"path/to/invoice.jpg\",\n",
    "    \"conversations\": [\n",
    "        {\n",
    "            \"from\": \"human\",\n",
    "            \"value\": \"<image>\\nì´ ì²­êµ¬ì„œì—ì„œ ì´ ê¸ˆì•¡ì€ ì–¼ë§ˆì¸ê°€?\"\n",
    "        },\n",
    "        {\n",
    "            \"from\": \"gpt\",\n",
    "            \"value\": \"ì´ ì²­êµ¬ì„œì˜ ì´ ê¸ˆì•¡ì€ 1,250,000ì›ì´ë‹¤. í’ˆëª©ë³„ë¡œ ë³´ë©´ ì œí’ˆ Aê°€ 500,000ì›, ì œí’ˆ Bê°€ 750,000ì›ì´ë©°, ë¶€ê°€ì„¸ í¬í•¨ ê¸ˆì•¡ì´ë‹¤.\"\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "\n",
    "print(\"[ë¬¸ì„œ ì´í•´ ë°ì´í„° í˜•ì‹]\")\n",
    "print(json.dumps(document_data_example, indent=2, ensure_ascii=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10.3 ì œí’ˆ ì´ë¯¸ì§€ ë¶„ì„"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ì œí’ˆ ì´ë¯¸ì§€ ë¶„ì„ ë°ì´í„° í˜•ì‹]\n",
      "{\n",
      "  \"id\": \"product_001\",\n",
      "  \"image\": \"path/to/product.jpg\",\n",
      "  \"conversations\": [\n",
      "    {\n",
      "      \"from\": \"human\",\n",
      "      \"value\": \"<image>\\nì´ ì œí’ˆì˜ íŠ¹ì§•ì„ ë¶„ì„í•´ì¤˜.\"\n",
      "    },\n",
      "    {\n",
      "      \"from\": \"gpt\",\n",
      "      \"value\": \"ì´ ì œí’ˆì€ ë¬´ì„  ë¸”ë£¨íˆ¬ìŠ¤ ì´ì–´í°ìœ¼ë¡œ, ë‹¤ìŒê³¼ ê°™ì€ íŠ¹ì§•ì´ ìˆë‹¤:\\n\\n1. ë””ìì¸: í™”ì´íŠ¸ ì»¬ëŸ¬ì˜ ì¸ì´ì–´ íƒ€ì…\\n2. ì¶©ì „ ì¼€ì´ìŠ¤: ì»´íŒ©íŠ¸í•œ ì›í˜• ì¼€ì´ìŠ¤ í¬í•¨\\n3. ì˜ˆìƒ ì°©ìš©ê°: ì¸ì²´ê³µí•™ì  ì„¤ê³„\\n4. ë¸Œëœë“œ: ì´ë¯¸ì§€ì—ì„œ í™•ì¸ë˜ëŠ” ë¡œê³  ê¸°ë°˜ìœ¼ë¡œ í”„ë¦¬ë¯¸ì—„ ë¸Œëœë“œë¡œ ì¶”ì •\\n\\nì •í™•í•œ ìŠ¤í™ì€ ì œí’ˆ ìƒì„¸ í˜ì´ì§€ë¥¼ í™•ì¸í•˜ëŠ” ê²ƒì´ ì¢‹ë‹¤.\"\n",
      "    }\n",
      "  ]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# ì œí’ˆ ì´ë¯¸ì§€ ë¶„ì„ìš© ë°ì´í„° í˜•ì‹ ì˜ˆì‹œ\n",
    "product_data_example = {\n",
    "    \"id\": \"product_001\",\n",
    "    \"image\": \"path/to/product.jpg\",\n",
    "    \"conversations\": [\n",
    "        {\n",
    "            \"from\": \"human\",\n",
    "            \"value\": \"<image>\\nì´ ì œí’ˆì˜ íŠ¹ì§•ì„ ë¶„ì„í•´ì¤˜.\"\n",
    "        },\n",
    "        {\n",
    "            \"from\": \"gpt\",\n",
    "            \"value\": \"ì´ ì œí’ˆì€ ë¬´ì„  ë¸”ë£¨íˆ¬ìŠ¤ ì´ì–´í°ìœ¼ë¡œ, ë‹¤ìŒê³¼ ê°™ì€ íŠ¹ì§•ì´ ìˆë‹¤:\\n\\n1. ë””ìì¸: í™”ì´íŠ¸ ì»¬ëŸ¬ì˜ ì¸ì´ì–´ íƒ€ì…\\n2. ì¶©ì „ ì¼€ì´ìŠ¤: ì»´íŒ©íŠ¸í•œ ì›í˜• ì¼€ì´ìŠ¤ í¬í•¨\\n3. ì˜ˆìƒ ì°©ìš©ê°: ì¸ì²´ê³µí•™ì  ì„¤ê³„\\n4. ë¸Œëœë“œ: ì´ë¯¸ì§€ì—ì„œ í™•ì¸ë˜ëŠ” ë¡œê³  ê¸°ë°˜ìœ¼ë¡œ í”„ë¦¬ë¯¸ì—„ ë¸Œëœë“œë¡œ ì¶”ì •\\n\\nì •í™•í•œ ìŠ¤í™ì€ ì œí’ˆ ìƒì„¸ í˜ì´ì§€ë¥¼ í™•ì¸í•˜ëŠ” ê²ƒì´ ì¢‹ë‹¤.\"\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "\n",
    "print(\"[ì œí’ˆ ì´ë¯¸ì§€ ë¶„ì„ ë°ì´í„° í˜•ì‹]\")\n",
    "print(json.dumps(product_data_example, indent=2, ensure_ascii=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. ë™ì˜ìƒ ì´í•´ ëª¨ë¸ (Video LLM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ë™ì˜ìƒ ì´í•´ LLM ëª¨ë¸:\n",
      "============================================================\n",
      "\n",
      "[Qwen2-VL]\n",
      "  Model: Qwen/Qwen2-VL-7B-Instruct\n",
      "  íŠ¹ì§•: ì´ë¯¸ì§€ + ë™ì˜ìƒ í†µí•© ì§€ì›\n",
      "  Max Frames: ë™ì  ìƒ˜í”Œë§\n",
      "\n",
      "[LLaVA-NeXT-Video]\n",
      "  Model: llava-hf/LLaVA-NeXT-Video-7B-hf\n",
      "  íŠ¹ì§•: ë¹„ë””ì˜¤ íŠ¹í™”\n",
      "  Max Frames: 32\n",
      "\n",
      "[Video-LLaVA]\n",
      "  Model: LanguageBind/Video-LLaVA-7B-hf\n",
      "  íŠ¹ì§•: ì´ë¯¸ì§€ + ë¹„ë””ì˜¤ í†µí•© ì¸ì½”ë”\n",
      "  Max Frames: 8\n",
      "\n",
      "[InternVL2-Video]\n",
      "  Model: OpenGVLab/InternVL2-8B\n",
      "  íŠ¹ì§•: ê³ ì„±ëŠ¥ ë¹„ë””ì˜¤ ì´í•´\n",
      "  Max Frames: 64\n"
     ]
    }
   ],
   "source": [
    "# ë™ì˜ìƒ LLM ëª¨ë¸ ëª©ë¡\n",
    "video_llm_models = {\n",
    "    \"Qwen2-VL\": {\n",
    "        \"model_id\": \"Qwen/Qwen2-VL-7B-Instruct\",\n",
    "        \"íŠ¹ì§•\": \"ì´ë¯¸ì§€ + ë™ì˜ìƒ í†µí•© ì§€ì›\",\n",
    "        \"max_frames\": \"ë™ì  ìƒ˜í”Œë§\"\n",
    "    },\n",
    "    \"LLaVA-NeXT-Video\": {\n",
    "        \"model_id\": \"llava-hf/LLaVA-NeXT-Video-7B-hf\",\n",
    "        \"íŠ¹ì§•\": \"ë¹„ë””ì˜¤ íŠ¹í™”\",\n",
    "        \"max_frames\": 32\n",
    "    },\n",
    "    \"Video-LLaVA\": {\n",
    "        \"model_id\": \"LanguageBind/Video-LLaVA-7B-hf\",\n",
    "        \"íŠ¹ì§•\": \"ì´ë¯¸ì§€ + ë¹„ë””ì˜¤ í†µí•© ì¸ì½”ë”\",\n",
    "        \"max_frames\": 8\n",
    "    },\n",
    "    \"InternVL2-Video\": {\n",
    "        \"model_id\": \"OpenGVLab/InternVL2-8B\",\n",
    "        \"íŠ¹ì§•\": \"ê³ ì„±ëŠ¥ ë¹„ë””ì˜¤ ì´í•´\",\n",
    "        \"max_frames\": 64\n",
    "    }\n",
    "}\n",
    "\n",
    "print(\"ë™ì˜ìƒ ì´í•´ LLM ëª¨ë¸:\")\n",
    "print(\"=\" * 60)\n",
    "for name, info in video_llm_models.items():\n",
    "    print(f\"\\n[{name}]\")\n",
    "    print(f\"  Model: {info['model_id']}\")\n",
    "    print(f\"  íŠ¹ì§•: {info['íŠ¹ì§•']}\")\n",
    "    print(f\"  Max Frames: {info['max_frames']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ë™ì˜ìƒ í”„ë ˆì„ ì¶”ì¶œ í•¨ìˆ˜ ì¤€ë¹„ ì™„ë£Œ\n"
     ]
    }
   ],
   "source": [
    "def process_video_frames(video_path, num_frames=8):\n",
    "    \"\"\"\n",
    "    ë™ì˜ìƒì—ì„œ í”„ë ˆì„ì„ ì¶”ì¶œí•˜ëŠ” í•¨ìˆ˜ë‹¤.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        import cv2\n",
    "        \n",
    "        cap = cv2.VideoCapture(video_path)\n",
    "        total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "        \n",
    "        # ê· ë“±í•˜ê²Œ í”„ë ˆì„ ìƒ˜í”Œë§\n",
    "        frame_indices = [int(i * total_frames / num_frames) for i in range(num_frames)]\n",
    "        \n",
    "        frames = []\n",
    "        for idx in frame_indices:\n",
    "            cap.set(cv2.CAP_PROP_POS_FRAMES, idx)\n",
    "            ret, frame = cap.read()\n",
    "            if ret:\n",
    "                # BGR to RGB\n",
    "                frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "                frames.append(Image.fromarray(frame))\n",
    "        \n",
    "        cap.release()\n",
    "        return frames\n",
    "        \n",
    "    except ImportError:\n",
    "        print(\"cv2ê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ë‹¤. pip install opencv-pythonì„ ì‹¤í–‰í•œë‹¤.\")\n",
    "        return None\n",
    "\n",
    "print(\"ë™ì˜ìƒ í”„ë ˆì„ ì¶”ì¶œ í•¨ìˆ˜ ì¤€ë¹„ ì™„ë£Œ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. ìš”ì•½\n",
    "\n",
    "### LVM Fine-tuning í•µì‹¬ í¬ì¸íŠ¸\n",
    "\n",
    "| í•­ëª© | ì„¤ëª… |\n",
    "|------|------|\n",
    "| **êµ¬ì¡°** | Vision Encoder + Projector + LLM |\n",
    "| **ê¶Œì¥ ì „ëµ** | Vision Encoder ë™ê²°, Projector í•™ìŠµ, LLM LoRA |\n",
    "| **ë°ì´í„° í˜•ì‹** | ì´ë¯¸ì§€ + ëŒ€í™” ìŒ (JSON) |\n",
    "| **í•™ìŠµë¥ ** | Projector: 1e-4, LLM: 1e-5 |\n",
    "| **ë©”ëª¨ë¦¬ ìµœì í™”** | 4-bit ì–‘ìí™”, Gradient Checkpointing |\n",
    "\n",
    "### ì£¼ìš” ì˜¤í”ˆì†ŒìŠ¤ ëª¨ë¸\n",
    "\n",
    "| ëª¨ë¸ | íŠ¹ì§• | ì¶”ì²œ ìš©ë„ |\n",
    "|------|------|----------|\n",
    "| **LLaVA** | ê°„ë‹¨, ë„ë¦¬ ì‚¬ìš© | ì¼ë°˜ ëª©ì  |\n",
    "| **Qwen2-VL** | ë™ì  í•´ìƒë„, ë™ì˜ìƒ | ê³ í’ˆì§ˆ ìš”êµ¬ |\n",
    "| **InternVL** | ëŒ€ê·œëª¨ ë¹„ì „ ì¸ì½”ë” | ì„¸ë°€í•œ ì´í•´ |\n",
    "| **Phi-3-Vision** | ê²½ëŸ‰ | ì—£ì§€ ë°°í¬ |\n",
    "\n",
    "---\n",
    "\n",
    "ì´ íŠœí† ë¦¬ì–¼ì„ í†µí•´ LVM(Large Vision-Multimodal Model)ì˜ êµ¬ì¡°ë¥¼ ì´í•´í•˜ê³ , LLaVA, Qwen2-VL ë“± ì£¼ìš” ëª¨ë¸ì„ Fine-tuningí•˜ëŠ” ë°©ë²•ì„ í•™ìŠµí–ˆë‹¤."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "graph",
   "language": "python",
   "name": "graph"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

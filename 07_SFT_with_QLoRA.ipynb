{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HSCYz8KtZheQ"
   },
   "source": [
    "# Chapter 07: QLoRA를 활용한 SFT\n",
    "\n",
    "## 1. 학습 목표\n",
    "\n",
    "* QLoRA의 원리와 장점(메모리 절약, 성능 유지)을 이해한다.\n",
    "* 4-bit 양자화 모델에 LoRA를 적용하여 금융 도메인 데이터를 학습한다.\n",
    "* `paged_adamw_8bit` 옵티마이저를 활용해 메모리 부족(OOM)을 방지한다.\n",
    "\n",
    "## 2. QLoRA 설정 및 모델 로드\n",
    "\n",
    "QLoRA의 핵심은 **4-bit NormalFloat (NF4)** 양자화와 **이중 양자화(Double Quantization)**이다. 이를 통해 20B급 모델을 약 12~14GB VRAM에 로드할 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "# optional imports (환경에 따라 없을 수 있음)\n",
    "try:\n",
    "    from transformers import BitsAndBytesConfig\n",
    "except Exception:\n",
    "    BitsAndBytesConfig = None\n",
    "\n",
    "try:\n",
    "    from transformers import Mxfp4Config\n",
    "except Exception:\n",
    "    Mxfp4Config = None\n",
    "\n",
    "try:\n",
    "    from peft import prepare_model_for_kbit_training\n",
    "except Exception:\n",
    "    prepare_model_for_kbit_training = None\n",
    "\n",
    "\n",
    "def _is_gpt_oss(model_id: str) -> bool:\n",
    "    mid = (model_id or \"\").lower()\n",
    "    return \"openai/gpt-oss\" in mid or mid.startswith(\"openai/gpt-oss\")\n",
    "\n",
    "\n",
    "def _make_bnb_nf4_4bit_config():\n",
    "    if BitsAndBytesConfig is None:\n",
    "        raise ImportError(\n",
    "            \"BitsAndBytesConfig를 import할 수 없음. \"\n",
    "            \"pip install -U bitsandbytes transformers accelerate 를 확인하라.\"\n",
    "        )\n",
    "    return BitsAndBytesConfig(\n",
    "        load_in_4bit=True,\n",
    "        bnb_4bit_quant_type=\"nf4\",\n",
    "        bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "        bnb_4bit_use_double_quant=True,\n",
    "    )\n",
    "\n",
    "def load_model_and_tokenizer(\n",
    "    model_id: str,\n",
    "    *,\n",
    "    device_map: str = \"auto\",\n",
    "    trust_remote_code: bool = True,\n",
    "    gpt_oss_dequantize_to_bf16: bool = False,  # gpt-oss-20b만 해당\n",
    "    enable_gradient_checkpointing: bool = True,\n",
    "):\n",
    "    \"\"\"\n",
    "    - gpt-oss-20b: MXFP4 네이티브 로드(기본) 또는 BF16 디양자화 로드\n",
    "    - 그 외: bitsandbytes NF4 4bit 로드 + (가능하면) k-bit 학습 준비\n",
    "    \"\"\"\n",
    "    is_oss = _is_gpt_oss(model_id)\n",
    "\n",
    "    if is_oss:\n",
    "        if Mxfp4Config is None:\n",
    "            raise ImportError(\n",
    "                \"Mxfp4Config를 import할 수 없음. \"\n",
    "                \"pip install -U transformers 로 업데이트하라.\"\n",
    "            )\n",
    "\n",
    "        quant_cfg = Mxfp4Config(dequantize=bool(gpt_oss_dequantize_to_bf16))\n",
    "        torch_dtype = torch.bfloat16 if gpt_oss_dequantize_to_bf16 else \"auto\"\n",
    "\n",
    "        model = AutoModelForCausalLM.from_pretrained(\n",
    "            model_id,\n",
    "            device_map=device_map,\n",
    "            trust_remote_code=trust_remote_code,\n",
    "            quantization_config=quant_cfg,\n",
    "            torch_dtype=torch_dtype,\n",
    "            # MXFP4 환경에서 sdpa가 안 맞는 경우가 있어 안전하게 eager 권장\n",
    "            attn_implementation=\"eager\",\n",
    "        )\n",
    "\n",
    "        mode = \"gpt-oss / MXFP4\"\n",
    "        if gpt_oss_dequantize_to_bf16:\n",
    "            mode += \" (dequantize->bf16)\"\n",
    "\n",
    "    else:\n",
    "        bnb_cfg = _make_bnb_nf4_4bit_config()\n",
    "\n",
    "        model = AutoModelForCausalLM.from_pretrained(\n",
    "            model_id,\n",
    "            device_map=device_map,\n",
    "            trust_remote_code=trust_remote_code,\n",
    "            quantization_config=bnb_cfg,\n",
    "            torch_dtype=torch.bfloat16,\n",
    "            attn_implementation=\"sdpa\",\n",
    "        )\n",
    "\n",
    "        # QLoRA/4bit 학습 준비(가능할 때만)\n",
    "        if prepare_model_for_kbit_training is not None:\n",
    "            model = prepare_model_for_kbit_training(model)\n",
    "        mode = \"bnb / NF4 4bit (QLoRA-ready)\"\n",
    "\n",
    "    # 공통: 학습 준비 옵션(권장)\n",
    "    if enable_gradient_checkpointing and hasattr(model, \"gradient_checkpointing_enable\"):\n",
    "        model.gradient_checkpointing_enable()\n",
    "    if hasattr(model.config, \"use_cache\"):\n",
    "        model.config.use_cache = False  # 학습 시 권장\n",
    "\n",
    "    # tokenizer\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_id, trust_remote_code=trust_remote_code)\n",
    "    if tokenizer.pad_token is None:\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "    tokenizer.padding_side = \"right\"\n",
    "\n",
    "    return model, tokenizer, {\"mode\": mode}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n",
      "MXFP4 quantization requires Triton and kernels installed: CUDA requires Triton >= 3.4.0, XPU requires Triton >= 3.5.0, we will default to dequantizing the model to bf16\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "826cceea341f4533a192edb5c321b65e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gpt-oss / MXFP4\n",
      "model dtype: torch.bfloat16\n",
      "모델 로드 완료: 41.83 GB\n"
     ]
    }
   ],
   "source": [
    "# gpt-oss-20b 로드 시\n",
    "model_id = \"openai/gpt-oss-20b\"\n",
    "#model_id = \"Qwen/Qwen3-14B\"\n",
    "\n",
    "# 1. gpt-oss-20b: MXFP4 네이티브 4bit\n",
    "model, tokenizer, info = load_model_and_tokenizer(\n",
    "    model_id, \n",
    "    gpt_oss_dequantize_to_bf16=False\n",
    ")\n",
    "\n",
    "# 2. gpt-oss-20b: BF16로 디양자화(학습 파이프라인 맞출 때)\n",
    "# model, tokenizer, info = load_model_and_tokenizer(model_id, gpt_oss_dequantize_to_bf16=True)\n",
    "\n",
    "print(info[\"mode\"])\n",
    "print(\"model dtype:\", getattr(model, \"dtype\", None))\n",
    "print(f\"모델 로드 완료: {model.get_memory_footprint() / 1e9:.2f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "js86YAcKZheU"
   },
   "source": [
    "## 3. LoRA 어댑터 설정\n",
    "\n",
    "모델의 표현력을 최대한 유지하면서 학습하기 위해 모든 선형 레이어(`Linear`)에 LoRA를 적용한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "oqQ8bbiIZheU"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 7,962,624 || all params: 20,922,719,808 || trainable%: 0.0381\n"
     ]
    }
   ],
   "source": [
    "# LoRA 설정\n",
    "lora_config = LoraConfig(\n",
    "    r=16,                       # Rank\n",
    "    lora_alpha=32,              # Alpha\n",
    "    target_modules=[            # 모든 Linear 레이어 타겟팅\n",
    "        \"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "        \"gate_proj\", \"up_proj\", \"down_proj\"\n",
    "    ],\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\"\n",
    ")\n",
    "\n",
    "# 모델에 LoRA 적용\n",
    "model = get_peft_model(model, lora_config)\n",
    "model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "U-96erj8ZheU"
   },
   "source": [
    "## 4. 도메인 데이터셋 준비 (금융)\n",
    "\n",
    "이번 챕터에서는 일반적인 대화가 아닌, 금융 지식을 학습시키기 위해 `gbharti/finance-alpaca` 데이터셋을 사용한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "SCVzr7AtZheU"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "금융 데이터셋 준비 완료: 1000개\n",
      "### Instruction:\n",
      "당신은 금융 전문가 AI 어시스턴트입니다. 다음 질문에 전문적으로 답변하세요.\n",
      "For a car, what scams can be plotted with 0% financing vs rebate?\n",
      "\n",
      "### Response:\n",
      "The car deal makes money 3 ways. If you pay in one lump payment. If the payment is greater than what they paid for the car, plus their expenses, they make a p...\n"
     ]
    }
   ],
   "source": [
    "# 금융 데이터셋 로드\n",
    "dataset = load_dataset(\"gbharti/finance-alpaca\", split=\"train[:1000]\")\n",
    "\n",
    "def format_finance_instruction(example):\n",
    "    \"\"\"\n",
    "    금융 데이터셋을 프롬프트 형식으로 변환하는 함수다.\n",
    "    \"\"\"\n",
    "    instruction = example['instruction']\n",
    "    input_text = example.get('input', '')\n",
    "    output = example['output']\n",
    "\n",
    "    # 금융 전문가 페르소나 주입 (선택 사항)\n",
    "    prefix = \"당신은 금융 전문가 AI 어시스턴트입니다. 다음 질문에 전문적으로 답변하세요.\"\n",
    "\n",
    "    if input_text:\n",
    "        text = f\"\"\"### Instruction:\n",
    "{prefix}\n",
    "{instruction}\n",
    "\n",
    "### Input:\n",
    "{input_text}\n",
    "\n",
    "### Response:\n",
    "{output}\"\"\"\n",
    "    else:\n",
    "        text = f\"\"\"### Instruction:\n",
    "{prefix}\n",
    "{instruction}\n",
    "\n",
    "### Response:\n",
    "{output}\"\"\"\n",
    "\n",
    "    return {\"text\": text}\n",
    "\n",
    "# 데이터 포맷팅\n",
    "formatted_dataset = dataset.map(format_finance_instruction)\n",
    "\n",
    "print(f\"금융 데이터셋 준비 완료: {len(formatted_dataset)}개\")\n",
    "print(formatted_dataset[0]['text'][:300] + \"...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dXn-rzYXZheU"
   },
   "source": [
    "## 5. 학습 설정 및 실행\n",
    "\n",
    "메모리 효율을 위해 `paged_adamw_8bit` 옵티마이저를 사용한다. 이는 GPU 메모리가 부족할 때 CPU RAM을 활용하여 OOM(Out of Memory) 오류를 방지하는 QLoRA 학습의 필수 요소다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "ToxiB3MXZheV"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "금융 모델 QLoRA 학습 시작...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mkubwai\u001b[0m to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.23.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/josh/llm-finetuning/wandb/run-20251225_105542-a7xmsow6</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/kubwai/huggingface/runs/a7xmsow6' target=\"_blank\">glorious-firebrand-4</a></strong> to <a href='https://wandb.ai/kubwai/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/kubwai/huggingface' target=\"_blank\">https://wandb.ai/kubwai/huggingface</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/kubwai/huggingface/runs/a7xmsow6' target=\"_blank\">https://wandb.ai/kubwai/huggingface/runs/a7xmsow6</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='100' max='100' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [100/100 09:51, Epoch 1/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>3.599500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>2.510100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>2.433200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>2.354200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>2.326000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>2.340800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>2.323400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>2.261000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90</td>\n",
       "      <td>2.266000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>2.305800</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "학습 완료!\n"
     ]
    }
   ],
   "source": [
    "# SFT 학습 설정\n",
    "training_args = SFTConfig(\n",
    "    output_dir=\"./GPT-OSS-20B-Finance-QLoRA\",\n",
    "\n",
    "    # 학습 파라미터\n",
    "    max_steps=100,                      # 실습용 (실제 학습 시 num_train_epochs 사용)\n",
    "    per_device_train_batch_size=4,\n",
    "    gradient_accumulation_steps=4,\n",
    "    learning_rate=2e-4,\n",
    "\n",
    "    # 메모리 최적화 옵션\n",
    "    optim=\"paged_adamw_8bit\",           # 페이징 옵티마이저 (중요)\n",
    "    fp16=False,\n",
    "    bf16=True,                          # BF16 사용 권장\n",
    "\n",
    "    # 로깅 및 저장\n",
    "    logging_steps=10,\n",
    "    save_strategy=\"steps\",\n",
    "    save_steps=50,\n",
    "\n",
    "    # 데이터 설정\n",
    "    #max_seq_length=512,\n",
    "    dataset_text_field=\"text\",\n",
    "    packing=False\n",
    ")\n",
    "\n",
    "# Trainer 생성\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    train_dataset=formatted_dataset,\n",
    "    args=training_args,\n",
    "    processing_class=tokenizer,\n",
    ")\n",
    "\n",
    "print(\"금융 모델 QLoRA 학습 시작...\")\n",
    "trainer.train()\n",
    "print(\"학습 완료!\")\n",
    "\n",
    "# 모델 저장\n",
    "trainer.save_model(\"./GPT-OSS-20B-Financ-Final\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HvH_TDsHZheV"
   },
   "source": [
    "## 6. 요약\n",
    "\n",
    "QLoRA는 4-bit 양자화를 통해 메모리 사용량을 획기적으로 줄이면서도 LoRA 학습을 가능하게 하는 기술이다.\n",
    "이번 챕터에서는 금융 도메인 데이터를 활용하여 특수 목적의 모델을 효율적으로 학습하는 방법을 실습했다.\n",
    "\n",
    "**다음 챕터**: Chapter 08 - 도메인 적응 SFT에서는 금융 외에도 의료, 법률 등 다양한 도메인 데이터를 처리하는 전략과 시스템 프롬프트 활용법을 다룬다."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "graph",
   "language": "python",
   "name": "graph"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

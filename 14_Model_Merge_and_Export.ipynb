{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "T69Lha29bjgX"
   },
   "source": [
    "# Chapter 14: 모델 병합 및 내보내기\n",
    "\n",
    "## 1. 학습 목표\n",
    "\n",
    "* LoRA 어댑터를 베이스 모델에 병합하여 단일 모델로 만든다.\n",
    "* 병합된 모델을 저장하고 GGUF 형식으로 변환한다.\n",
    "* HuggingFace Hub에 모델을 업로드하고 모델 카드를 작성한다.\n",
    "\n",
    "## 2. LoRA 병합 (Merge)\n",
    "\n",
    "### 2.1 병합의 필요성\n",
    "\n",
    "학습 시에는 메모리를 아끼기 위해 베이스 모델을 동결하고 LoRA 어댑터만 사용했지만, 실제 배포 시에는 어댑터를 따로 로드하는 연산 비용을 없애기 위해 두 모델을 합치는 것이 유리하다.\n",
    "\n",
    "```text\n",
    "학습 시: Base Model (동결) + LoRA Adapter (학습)\n",
    "배포 시: Merged Model (단일 모델) → 추론 속도 향상, 관리 용이\n",
    "\n",
    "```\n",
    "\n",
    "### 2.2 병합 코드 구현\n",
    "\n",
    "`peft` 라이브러리의 `merge_and_unload()` 메서드를 사용하면 간단히 병합할 수 있다. 이때 주의할 점은 병합을 위해 **베이스 모델을 FP16 또는 BF16으로 다시 로드**해야 한다는 것이다 (4-bit 상태로는 병합 불가)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "KG_3kk6xbjgY"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. 베이스 모델 로드 (FP16): openai/gpt-oss-20b\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n",
      "MXFP4 quantization requires Triton and kernels installed: CUDA requires Triton >= 3.4.0, XPU requires Triton >= 3.5.0, we will default to dequantizing the model to bf16\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8d599f49dda54273b3698d0fd8fde7f3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2. LoRA 어댑터 결합: ./GPT-OSS-20B-DPO-Final\n",
      "3. 가중치 병합 (Merge & Unload)...\n",
      "4. 병합된 모델 저장: ./GPT-OSS-20B-merged\n",
      "병합 및 저장 완료!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GptOssForCausalLM(\n",
       "  (model): GptOssModel(\n",
       "    (embed_tokens): Embedding(201088, 2880, padding_idx=199999)\n",
       "    (layers): ModuleList(\n",
       "      (0-23): 24 x GptOssDecoderLayer(\n",
       "        (self_attn): GptOssAttention(\n",
       "          (q_proj): Linear(in_features=2880, out_features=4096, bias=True)\n",
       "          (k_proj): Linear(in_features=2880, out_features=512, bias=True)\n",
       "          (v_proj): Linear(in_features=2880, out_features=512, bias=True)\n",
       "          (o_proj): Linear(in_features=4096, out_features=2880, bias=True)\n",
       "        )\n",
       "        (mlp): GptOssMLP(\n",
       "          (router): GptOssTopKRouter()\n",
       "          (experts): GptOssExperts()\n",
       "        )\n",
       "        (input_layernorm): GptOssRMSNorm((2880,), eps=1e-05)\n",
       "        (post_attention_layernorm): GptOssRMSNorm((2880,), eps=1e-05)\n",
       "      )\n",
       "    )\n",
       "    (norm): GptOssRMSNorm((2880,), eps=1e-05)\n",
       "    (rotary_emb): GptOssRotaryEmbedding()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=2880, out_features=201088, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from peft import PeftModel\n",
    "import torch\n",
    "import os\n",
    "\n",
    "def merge_lora_to_base(base_model_id, adapter_path, output_path):\n",
    "    \"\"\"\n",
    "    LoRA 어댑터를 베이스 모델에 영구적으로 병합하여 저장하는 함수다.\n",
    "    \"\"\"\n",
    "    print(f\"1. 베이스 모델 로드 (FP16): {base_model_id}\")\n",
    "\n",
    "    # 병합을 위해 양자화 없이 FP16으로 로드한다.\n",
    "    # (시스템 메모리가 부족하면 'cpu'로 로드 후 병합할 수도 있음)\n",
    "    base_model = AutoModelForCausalLM.from_pretrained(\n",
    "        base_model_id,\n",
    "        torch_dtype=torch.float16,\n",
    "        device_map=\"auto\",\n",
    "        trust_remote_code=True\n",
    "    )\n",
    "\n",
    "    print(f\"2. LoRA 어댑터 결합: {adapter_path}\")\n",
    "    model = PeftModel.from_pretrained(base_model, adapter_path)\n",
    "\n",
    "    print(\"3. 가중치 병합 (Merge & Unload)...\")\n",
    "    merged_model = model.merge_and_unload()\n",
    "\n",
    "    print(f\"4. 병합된 모델 저장: {output_path}\")\n",
    "    merged_model.save_pretrained(output_path)\n",
    "\n",
    "    # 토크나이저도 함께 저장해야 배포 시 편리하다.\n",
    "    tokenizer = AutoTokenizer.from_pretrained(base_model_id, trust_remote_code=True)\n",
    "    tokenizer.save_pretrained(output_path)\n",
    "\n",
    "    print(\"병합 및 저장 완료!\")\n",
    "    return merged_model\n",
    "\n",
    "# 실행 예시 (경로 수정 필요)\n",
    "merge_lora_to_base(\"openai/gpt-oss-20b\", \"./GPT-OSS-20B-DPO-Final\", \"./GPT-OSS-20B-Merged\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kb2fkA9zbjgZ"
   },
   "source": [
    "## 3. GGUF 변환 (로컬 구동용)\n",
    "\n",
    "병합된 모델을 개인 PC나 모바일 기기에서 돌리려면 `GGUF` 포맷으로 변환해야 한다. 이는 `llama.cpp` 프레임워크에서 사용하는 표준 포맷이다.\n",
    "\n",
    "### 3.1 변환 프로세스\n",
    "\n",
    "```bash\n",
    "# 1. llama.cpp 도구 설치 (클론 및 빌드)\n",
    "git clone https://github.com/ggerganov/llama.cpp\n",
    "cd llama.cpp\n",
    "make\n",
    "\n",
    "# 2. 필수 라이브러리 설치\n",
    "pip install -r requirements.txt\n",
    "\n",
    "# 3. 모델 변환 (HuggingFace 포맷 -> GGUF fp16)\n",
    "# 사용법: python convert-hf-to-gguf.py [모델 경로] --outfile [출력 파일명]\n",
    "python convert_hf_to_gguf.py ../gpt-oss-20b-merged --outfile gpt-oss-20b.gguf\n",
    "\n",
    "# 4. 양자화 (선택: fp16 -> q4_k_m)\n",
    "# 용량을 줄이기 위해 다시 4비트로 양자화한다.\n",
    "./llama-quantize gpt-oss-20b.gguf gpt-oss-20b-q4_k_m.gguf q4_k_m\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kb2fkA9zbjgZ"
   },
   "source": [
    "## 4. HuggingFace Hub 업로드\n",
    "\n",
    "학습한 모델을 커뮤니티에 공유하거나 서버에 배포하기 위해 HuggingFace Hub에 업로드한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "DKSx_FiabjgZ"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "리포지토리 준비 완료: nowave/gpt-oss-20b-dpo-finetuned\n",
      "업로드 시작 (시간이 소요될 수 있음)...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3d96b5ad727c4400b358ea1e48d33584",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing Files (0 / 0): |          |  0.00B /  0.00B            "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0c6aa6261aee43d29984a9926668cb90",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "New Data Upload: |          |  0.00B /  0.00B            "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "업로드 완료: https://huggingface.co/nowave/gpt-oss-20b-dpo-finetuned\n"
     ]
    }
   ],
   "source": [
    "from huggingface_hub import HfApi, create_repo\n",
    "\n",
    "def upload_to_hub(local_model_path, repo_id, token=None):\n",
    "    \"\"\"\n",
    "    로컬에 저장된 모델 폴더를 HuggingFace Hub에 업로드한다.\n",
    "    \"\"\"\n",
    "    api = HfApi(token=token)\n",
    "\n",
    "    # 리포지토리 생성 (이미 있으면 건너뜀)\n",
    "    try:\n",
    "        create_repo(repo_id, private=True, exist_ok=True)\n",
    "        print(f\"리포지토리 준비 완료: {repo_id}\")\n",
    "    except Exception as e:\n",
    "        print(f\"리포지토리 확인 중 오류: {e}\")\n",
    "\n",
    "    # 폴더 전체 업로드\n",
    "    print(\"업로드 시작 (시간이 소요될 수 있음)...\")\n",
    "    api.upload_folder(\n",
    "        folder_path=local_model_path,\n",
    "        repo_id=repo_id,\n",
    "        repo_type=\"model\"\n",
    "    )\n",
    "    print(f\"업로드 완료: https://huggingface.co/{repo_id}\")\n",
    "\n",
    "# 실행 예시\n",
    "upload_to_hub(\"./GPT-OSS-20B-Merged\", \"nowave/gpt-oss-20b-dpo-finetuned\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FWCJom-PbjgZ"
   },
   "source": [
    "## 5. 모델 카드 작성 (Model Card)\n",
    "\n",
    "모델의 신뢰도를 높이기 위해 `README.md` (모델 카드)를 작성해야 한다. 다음은 표준 템플릿이다.\n",
    "\n",
    "```markdown\n",
    "---\n",
    "license: apache-2.0\n",
    "language:\n",
    "- ko\n",
    "- en\n",
    "tags:\n",
    "- fine-tuned\n",
    "- gpt-oss-20b\n",
    "- lora\n",
    "base_model: Qwen/Qwen3-14B\n",
    "---\n",
    "\n",
    "# GPT-OSS-20B Finetuned Model\n",
    "\n",
    "## 모델 설명\n",
    "이 모델은 `Qwen3-14B`를 기반으로 [금융/의료/일반] 도메인 데이터셋을 사용하여 Fine-tuning한 모델입니다.\n",
    "\n",
    "## 학습 설정\n",
    "- **방법**: QLoRA (4-bit)\n",
    "- **Adapter Rank**: 16\n",
    "- **Alpha**: 32\n",
    "- **데이터셋**: [데이터셋 이름]\n",
    "- **학습률**: 2e-4\n",
    "\n",
    "## 사용법\n",
    "```\n",
    "```python\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "model_id = \"username/Qwen3-14B-finance-finetuned\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id, device_map=\"auto\")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vRVhYp85bjga"
   },
   "source": [
    "## 6. 요약\n",
    "\n",
    "이 챕터에서는 학습 결과물을 배포 가능한 형태로 만드는 과정을 수행했다.\n",
    "\n",
    "1.  **병합**: `merge_and_unload`를 통해 단일 모델로 변환하여 추론 효율을 높였다.\n",
    "2.  **변환**: `llama.cpp`를 위한 GGUF 포맷 변환 방법을 확인했다.\n",
    "3.  **배포**: HuggingFace Hub에 업로드하고 모델 카드를 작성하여 공유 준비를 마쳤다.\n",
    "\n",
    "다음 챕터는 **Chapter 15: 모니터링 및 프로덕션**으로, 실제 서비스 운영 시 필요한 MLOps 요소들을 다룬다."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "graph",
   "language": "python",
   "name": "graph"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3mfkYTcHbSfX"
   },
   "source": [
    "# Chapter 13: 모델 평가 및 테스트\n",
    "\n",
    "## 1. 학습 목표\n",
    "\n",
    "* **정량적 지표**: Perplexity, BLEU, ROUGE 등의 자동 평가 지표를 이해하고 계산한다.\n",
    "* **정성적 평가**: GPT-4 등 고성능 모델을 심판(Judge)으로 활용하는 **LLM-as-Judge** 시스템을 구축한다.\n",
    "* **기능 평가**: 모델이 명령어를 잘 따르는지(Instruction Following) 형식을 검증한다.\n",
    "* **안전장치**: 재앙적 망각(Catastrophic Forgetting) 테스트를 통해 모델의 품질을 보장한다.\n",
    "\n",
    "## 2. 자동 평가 지표 (Quantitative Metrics)\n",
    "\n",
    "### 2.1 주요 지표 개요\n",
    "\n",
    "사람이 일일이 채점하는 것은 비용이 많이 들기 때문에, 자동화된 수치 지표를 먼저 확인한다.\n",
    "\n",
    "| 지표 | 용도 | 설명 | 해석 |\n",
    "| --- | --- | --- | --- |\n",
    "| **Perplexity (PPL)** | 언어 모델링 | 모델이 다음에 올 단어를 얼마나 확신을 가지고 예측하는가? | **낮을수록 좋음** (1에 가까울수록 완벽) |\n",
    "| **BLEU** | 번역, 생성 | 정답 문장과 생성 문장의 N-gram(단어 조합)이 얼마나 겹치는가? | 높을수록 좋음 (보통 0~100점) |\n",
    "| **ROUGE** | 요약 | 생성된 요약문이 정답 요약문의 핵심 단어를 얼마나 포함(Recall)하는가? | 높을수록 좋음 |\n",
    "| **Accuracy** | 객관식/분류 | 정해진 답을 정확히 맞췄는가? | 높을수록 좋음 |\n",
    "\n",
    "### 2.2 Perplexity 계산 실습\n",
    "\n",
    "Perplexity는 모델의 '놀람' 정도를 의미한다. 모델이 텍스트를 자연스럽게 예측할수록 수치가 낮아진다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -q evaluate rouge_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "jNbhDCBCbSfY"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/josh/anaconda3/envs/llm/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "`torch_dtype` is deprecated! Use `dtype` instead!\n",
      "Loading checkpoint shards: 100%|██████████| 8/8 [00:06<00:00,  1.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "문장: 인공지능은 컴퓨터 공학의 중요한 분야이다.\n",
      "Perplexity: 13.17\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import math\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "def calculate_perplexity(model, tokenizer, text):\n",
    "    \"\"\"\n",
    "    주어진 텍스트에 대한 모델의 Perplexity를 계산하는 함수다.\n",
    "    \"\"\"\n",
    "    # 텍스트를 토큰화 (입력을 텐서로 변환)\n",
    "    encodings = tokenizer(text, return_tensors=\"pt\")\n",
    "    input_ids = encodings.input_ids.to(model.device)\n",
    "\n",
    "    # 모델의 손실(Loss) 계산\n",
    "    # labels를 input_ids와 동일하게 주면 언어 모델링 손실(Next Token Prediction Loss)이 계산된다.\n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids, labels=input_ids)\n",
    "        loss = outputs.loss\n",
    "\n",
    "    # Perplexity = exp(Loss)\n",
    "    perplexity = math.exp(loss.item())\n",
    "    return perplexity\n",
    "\n",
    "# 모델 로드 (이전 챕터에서 학습된 모델 또는 베이스 모델 사용)\n",
    "model_id = \"Qwen/Qwen3-14B\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id, device_map=\"auto\", torch_dtype=torch.bfloat16\n",
    ")\n",
    "\n",
    "# 테스트\n",
    "text = \"인공지능은 컴퓨터 공학의 중요한 분야이다.\"\n",
    "ppl = calculate_perplexity(model, tokenizer, text)\n",
    "print(f\"문장: {text}\")\n",
    "print(f\"Perplexity: {ppl:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ewofub8EbSfZ"
   },
   "source": [
    "### 2.3 ROUGE & BLEU 계산\n",
    "\n",
    "생성된 텍스트와 정답(Reference) 간의 유사도를 측정한다. `evaluate` 라이브러리를 사용하면 간편하게 계산할 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "a4WsuOGSbSfZ"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading builder script: 6.14kB [00:00, 4.31MB/s]\n",
      "Downloading builder script: 5.94kB [00:00, 6.54MB/s]\n",
      "Downloading extra modules: 4.07kB [00:00, 3.46MB/s]                   \n",
      "Downloading extra modules: 3.34kB [00:00, 3.39MB/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "평가 결과: {'rouge1': np.float64(0.0), 'rouge2': np.float64(0.0), 'rougeL': np.float64(0.0), 'bleu': 0.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# 라이브러리 설치 필요: pip install evaluate rouge_score\n",
    "import evaluate\n",
    "\n",
    "def evaluate_metrics(predictions, references):\n",
    "    \"\"\"\n",
    "    ROUGE와 BLEU 점수를 동시에 계산하는 함수다.\n",
    "    \"\"\"\n",
    "    # 지표 로드\n",
    "    rouge = evaluate.load('rouge')\n",
    "    bleu = evaluate.load('bleu')\n",
    "\n",
    "    # ROUGE 계산\n",
    "    rouge_results = rouge.compute(predictions=predictions, references=references)\n",
    "\n",
    "    # BLEU 계산\n",
    "    bleu_results = bleu.compute(predictions=predictions, references=references)\n",
    "\n",
    "    return {\n",
    "        \"rouge1\": rouge_results['rouge1'],\n",
    "        \"rouge2\": rouge_results['rouge2'],\n",
    "        \"rougeL\": rouge_results['rougeL'],\n",
    "        \"bleu\": bleu_results['bleu']\n",
    "    }\n",
    "\n",
    "# 예시 데이터\n",
    "preds = [\"딥러닝은 머신러닝의 한 분야이다.\"]\n",
    "refs = [\"딥러닝은 인공지능의 하위 분야인 머신러닝의 일종이다.\"]\n",
    "\n",
    "metrics = evaluate_metrics(preds, refs)\n",
    "print(\"평가 결과:\", metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JG-t-siZbSfZ"
   },
   "source": [
    "## 3. LLM-as-Judge (정성적 평가)\n",
    "\n",
    "텍스트 생성 모델의 품질은 수치로만 판단하기 어렵다. 최근에는 GPT-4와 같은 고성능 모델에게 \"이 답변이 얼마나 좋은지 평가해줘\"라고 요청하는 **LLM-as-Judge** 방식이 표준으로 자리 잡고 있다.\n",
    "\n",
    "### 3.1 평가 프롬프트 설계\n",
    "\n",
    "심판 모델에게 명확한 채점 기준(Rubric)을 제공해야 공정한 평가가 가능하다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "D9ZAKLWbbSfZ"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "당신은 공정한 AI 평가자입니다. 다음 질문에 대한 AI의 응답을 평가해주세요.\n",
      "\n",
      "[질문]: 파이썬의 GIL에 대해 설명해줘.\n",
      "[응답]: GIL은 Global Interpreter Lock의 약자로...\n",
      "\n",
      "다음 4가지 기준으로 1점부터 10점까지 점수를 매기고, 그 이유를 설명해주세요.\n",
      "\n",
      "1. 정확성(Accuracy): 정보가 사실과 일치하는가?\n",
      "2. 완전성(Completeness): 질문의 모든 요구사항을 충족했는가?\n",
      "3. 명확성(Clarity): 문장이 이해하기 쉽고 논리적인가?\n",
      "4. 유용성(Usefulness): 사용자에게 실질적인 도움이 되는가?\n",
      "\n",
      "반드시 아래 JSON 형식으로만 출력하세요:\n",
      "{\n",
      "    \"accuracy\": <점수>,\n",
      "    \"completeness\": <점수>,\n",
      "    \"clarity\": <점수>,\n",
      "    \"usefulness\": <점수>,\n",
      "    \"overall_score\": <평균 점수>,\n",
      "    \"feedback\": \"<구체적인 피드백>\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "def llm_judge_prompt(question, response):\n",
    "    \"\"\"\n",
    "    심판 모델(GPT-4 등)에게 입력할 평가용 프롬프트를 생성한다.\n",
    "    \"\"\"\n",
    "    prompt = f\"\"\"당신은 공정한 AI 평가자입니다. 다음 질문에 대한 AI의 응답을 평가해주세요.\n",
    "\n",
    "[질문]: {question}\n",
    "[응답]: {response}\n",
    "\n",
    "다음 4가지 기준으로 1점부터 10점까지 점수를 매기고, 그 이유를 설명해주세요.\n",
    "\n",
    "1. 정확성(Accuracy): 정보가 사실과 일치하는가?\n",
    "2. 완전성(Completeness): 질문의 모든 요구사항을 충족했는가?\n",
    "3. 명확성(Clarity): 문장이 이해하기 쉽고 논리적인가?\n",
    "4. 유용성(Usefulness): 사용자에게 실질적인 도움이 되는가?\n",
    "\n",
    "반드시 아래 JSON 형식으로만 출력하세요:\n",
    "{{\n",
    "    \"accuracy\": <점수>,\n",
    "    \"completeness\": <점수>,\n",
    "    \"clarity\": <점수>,\n",
    "    \"usefulness\": <점수>,\n",
    "    \"overall_score\": <평균 점수>,\n",
    "    \"feedback\": \"<구체적인 피드백>\"\n",
    "}}\"\"\"\n",
    "    return prompt\n",
    "\n",
    "# 사용 예시 (실제로는 OpenAI API 등을 호출하여 이 프롬프트를 보낸다)\n",
    "q = \"파이썬의 GIL에 대해 설명해줘.\"\n",
    "a = \"GIL은 Global Interpreter Lock의 약자로...\"\n",
    "print(llm_judge_prompt(q, a))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mG3t14ADbSfa"
   },
   "source": [
    "## 4. 태스크별 기능 평가 (Instruction Following)\n",
    "\n",
    "모델이 지시사항(특히 포맷)을 잘 지키는지 검증하는 코드다. 예를 들어 \"JSON으로 출력하라\"는 지시를 어기면, 내용은 맞아도 시스템 연동에 실패하기 때문이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "WF1uFwGbbSfa"
   },
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def evaluate_format_compliance(model, tokenizer, test_cases):\n",
    "    \"\"\"\n",
    "    모델이 요청된 포맷(예: JSON)을 준수하는지 테스트하는 함수다.\n",
    "    \"\"\"\n",
    "    results = []\n",
    "\n",
    "    for case in test_cases:\n",
    "        prompt = case[\"prompt\"]\n",
    "        expected_format = case.get(\"expected_format\", \"text\")\n",
    "\n",
    "        # 모델 추론\n",
    "        inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "        outputs = model.generate(**inputs, max_new_tokens=200)\n",
    "        response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "        # 프롬프트 제외한 응답만 추출 (간단한 처리)\n",
    "        response_content = response[len(prompt):].strip()\n",
    "\n",
    "        # 검증 로직\n",
    "        is_valid = False\n",
    "        if expected_format == \"json\":\n",
    "            try:\n",
    "                json.loads(response_content)\n",
    "                is_valid = True\n",
    "            except:\n",
    "                is_valid = False\n",
    "\n",
    "        results.append({\n",
    "            \"prompt\": prompt,\n",
    "            \"response\": response_content,\n",
    "            \"valid_format\": is_valid\n",
    "        })\n",
    "\n",
    "    return results\n",
    "\n",
    "# 테스트 케이스\n",
    "tests = [\n",
    "    {\n",
    "        \"prompt\": \"사과의 영양성분을 JSON 형식으로 알려줘.\\n\",\n",
    "        \"expected_format\": \"json\"\n",
    "    }\n",
    "]\n",
    "\n",
    "# (실제 실행 시 모델 로드 필요)\n",
    "# results = evaluate_format_compliance(model, tokenizer, tests)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rPD4Vv-sbSfb"
   },
   "source": [
    "## 5. 재앙적 망각(Catastrophic Forgetting) 테스트\n",
    "\n",
    "도메인 지식을 배우느라 기본적인 상식이나 언어 능력을 잃어버렸는지 확인하는 과정이다. 간단한 상식 퀴즈를 통해 이를 감지할 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "G3k_Ur75bSfb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[재앙적 망각 테스트]\n",
      "질문: 1 + 1은 무엇인가? -> 정답 (응답: 1 + 1은 무엇인가? 1 + 1은 2입니다. 이는 기본적인 덧셈의 규칙)\n",
      "질문: 대한민국의 수도는 어디인가? -> 정답 (응답: 대한민국의 수도는 어디인가? 대한민국의 수도는 서울이다. 서울은 수도권에 위치하며, 대한민국)\n",
      "질문: 사과는 무슨 색인가? -> 오답 (응답: 사과는 무슨 색인가? 사과의 색깔은 무엇인가요?\n",
      "\n",
      "사과는 일반적으로 **빨강)\n",
      "\n",
      "기본 지식 보존율: 66.7%\n",
      "경고: 재앙적 망각이 의심된다. 학습률을 낮추거나 데이터 믹싱을 고려해야 한다.\n"
     ]
    }
   ],
   "source": [
    "def test_general_knowledge(model, tokenizer):\n",
    "    \"\"\"\n",
    "    기본 상식 질문을 통해 재앙적 망각 여부를 진단한다.\n",
    "    \"\"\"\n",
    "    basic_questions = [\n",
    "        {\"q\": \"1 + 1은 무엇인가?\", \"a\": [\"2\", \"이\"]},\n",
    "        {\"q\": \"대한민국의 수도는 어디인가?\", \"a\": [\"서울\"]},\n",
    "        {\"q\": \"사과는 무슨 색인가?\", \"a\": [\"빨간\", \"초록\", \"red\", \"green\"]}\n",
    "    ]\n",
    "\n",
    "    correct_count = 0\n",
    "\n",
    "    print(\"\\n[재앙적 망각 테스트]\")\n",
    "    for item in basic_questions:\n",
    "        inputs = tokenizer(item['q'], return_tensors=\"pt\").to(model.device)\n",
    "        outputs = model.generate(**inputs, max_new_tokens=20)\n",
    "        response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "        # 정답 키워드가 포함되었는지 확인\n",
    "        is_correct = any(ans in response for ans in item['a'])\n",
    "        if is_correct:\n",
    "            correct_count += 1\n",
    "            print(f\"질문: {item['q']} -> 정답 (응답: {response.strip()})\")\n",
    "        else:\n",
    "            print(f\"질문: {item['q']} -> 오답 (응답: {response.strip()})\")\n",
    "\n",
    "    score = correct_count / len(basic_questions) * 100\n",
    "    print(f\"\\n기본 지식 보존율: {score:.1f}%\")\n",
    "\n",
    "    if score < 80:\n",
    "        print(\"경고: 재앙적 망각이 의심된다. 학습률을 낮추거나 데이터 믹싱을 고려해야 한다.\")\n",
    "\n",
    "test_general_knowledge(model, tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QpImwvy2bSfb"
   },
   "source": [
    "## 6. 요약\n",
    "\n",
    "이 챕터에서는 학습된 모델을 다각도로 평가하는 방법을 실습했다.\n",
    "\n",
    "1. **자동 지표**: PPL, BLEU 등으로 학습 상태를 빠르게 진단한다.\n",
    "2. **LLM-as-Judge**: GPT-4 등을 활용해 사람처럼 정성적인 평가를 수행한다.\n",
    "3. **기능 검증**: JSON 출력 등 지시 이행 능력을 테스트한다.\n",
    "4. **망각 방지**: 기본 상식 테스트를 통해 모델의 일반화 능력이 유지되는지 확인한다.\n",
    "\n",
    "평가까지 마쳤다면, 이제 이 모델을 실제 서비스에 사용할 수 있도록 패키징하는 단계가 남았다.\n",
    "\n",
    "다음 챕터는 **Chapter 14: 모델 병합 및 내보내기**로, LoRA 어댑터를 베이스 모델과 합치고 GGUF 등으로 변환하여 배포하는 방법을 다룬다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "graph",
   "language": "python",
   "name": "graph"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

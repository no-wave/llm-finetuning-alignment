{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "H8jzTUo7Xifv"
   },
   "source": [
    "# Chapter 06: LoRA를 활용한 SFT\n",
    "\n",
    "## 1. 학습 목표\n",
    "\n",
    "* LoRA(Low-Rank Adaptation)의 핵심 파라미터를 이해하고 설정한다.\n",
    "* `SFTTrainer`를 사용하여 데이터셋과 모델을 연결하고 학습 루프를 실행한다.\n",
    "* 학습된 어댑터(Adapter)를 저장하고, 로드하여 추론 테스트를 수행한다.\n",
    "* 학습 손실(Loss) 그래프를 통해 학습이 정상적으로 진행되는지 확인한다.\n",
    "\n",
    "## 2. LoRA 이론 복습\n",
    "\n",
    "### 2.1 LoRA의 핵심 원리\n",
    "\n",
    "거대 언어 모델의 모든 파라미터를 학습하는 대신, 가중치 행렬의 변화량()을 저순위 행렬의 곱()으로 근사하여 학습하는 방식이다.\n",
    "\n",
    "* **메모리 효율**: 전체 파라미터의 1% 미만만 학습하므로 VRAM 사용량이 획기적으로 줄어든다.\n",
    "* **모듈성**: 학습된 어댑터 용량이 매우 작아(수백 MB), 하나의 베이스 모델에 여러 어댑터를 갈아끼우며 사용할 수 있다.\n",
    "\n",
    "### 2.2 주요 하이퍼파라미터\n",
    "\n",
    "| 파라미터 | 설명 | 권장값 (20B 모델 기준) |\n",
    "| --- | --- | --- |\n",
    "| **r (Rank)** | 저순위 행렬의 차원. 높을수록 표현력 증가하지만 메모리 사용량도 늘어난다. | 16 ~ 64 |\n",
    "| **lora_alpha** | 학습 업데이트의 스케일링 팩터. 보통 Rank의 2배로 설정한다. | 32 ~ 128 |\n",
    "| **target_modules** | LoRA를 적용할 레이어. Attention과 FFN 모듈 전체에 적용하는 추세다. | `[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"]` |\n",
    "| **lora_dropout** | 과적합 방지를 위한 드롭아웃 비율. | 0.05 ~ 0.1 |\n",
    "\n",
    "## 3. 실습 환경 준비 및 라이브러리 임포트\n",
    "\n",
    "필요한 라이브러리를 불러오고, 학습에 사용할 디바이스 상태를 확인한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "TjPGz7xQXifx"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "사용 디바이스: cuda\n",
      "GPU: NVIDIA H100 80GB HBM3\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    BitsAndBytesConfig\n",
    ")\n",
    "from peft import (\n",
    "    LoraConfig,\n",
    "    get_peft_model,\n",
    "    prepare_model_for_kbit_training,\n",
    "    TaskType\n",
    ")\n",
    "from trl import SFTTrainer, SFTConfig\n",
    "from datasets import load_dataset\n",
    "import os\n",
    "\n",
    "# CUDA 사용 가능 여부 확인\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"사용 디바이스: {device}\")\n",
    "if device == \"cuda\":\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1rGL-hSUXifx"
   },
   "source": [
    "## 4. 데이터셋 준비 및 전처리\n",
    "\n",
    "Chapter 03에서 다룬 것과 같이 데이터셋을 로드하고, 모델이 이해할 수 있는 프롬프트 형식으로 변환한다. 여기서는 `tatsu-lab/alpaca` 데이터셋을 사용한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "AWVP6MRVXify"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "데이터셋 준비 완료: 1000개\n",
      "\n",
      "[샘플 데이터]\n",
      "### Instruction:\n",
      "Give three tips for staying healthy.\n",
      "\n",
      "### Response:\n",
      "1.Eat a balanced diet and make sure to include plenty of fruits and vegetables. \n",
      "2. Exercise regularly to keep your body active and strong. \n",
      "3. Get enough sleep and maintain a consistent sleep schedule.\n"
     ]
    }
   ],
   "source": [
    "# 1. 데이터셋 로드\n",
    "# 실습을 위해 1,000개 샘플만 사용한다. (실제 학습 시에는 전체 데이터 사용)\n",
    "dataset = load_dataset(\"tatsu-lab/alpaca\", split=\"train[:1000]\")\n",
    "\n",
    "# 2. 프롬프트 포맷팅 함수 정의\n",
    "def format_instruction(example):\n",
    "    \"\"\"\n",
    "    Alpaca 형식의 데이터를 LLM 학습을 위한 텍스트 프롬프트로 변환한다.\n",
    "    \"\"\"\n",
    "    instruction = example['instruction']\n",
    "    input_text = example.get('input', '')\n",
    "    output = example['output']\n",
    "\n",
    "    # 입력(Input) 유무에 따른 포맷 분기\n",
    "    if input_text:\n",
    "        text = f\"\"\"### Instruction:\n",
    "{instruction}\n",
    "\n",
    "### Input:\n",
    "{input_text}\n",
    "\n",
    "### Response:\n",
    "{output}\"\"\"\n",
    "    else:\n",
    "        text = f\"\"\"### Instruction:\n",
    "{instruction}\n",
    "\n",
    "### Response:\n",
    "{output}\"\"\"\n",
    "\n",
    "    return {\"text\": text}\n",
    "\n",
    "# 3. 데이터셋 매핑\n",
    "formatted_dataset = dataset.map(format_instruction)\n",
    "\n",
    "print(f\"데이터셋 준비 완료: {len(formatted_dataset)}개\")\n",
    "print(\"\\n[샘플 데이터]\")\n",
    "print(formatted_dataset[0]['text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NkF0gOC5Xify"
   },
   "source": [
    "## 5. 모델 및 토크나이저 로드 (QLoRA)\n",
    "\n",
    "Chapter 05에서 실습한 대로 `gpt-oss-20b`과 별행으로 `Qwen/Qwen3-14B` 모델을 4-bit로 로드한다.\n",
    "단 `gpt-oss-20b`는 4bit로 이미 양자화되어 학습한 모델임으로 로드에 주의하자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "M6S5DqCuXify"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "74aa2636d81f47a39da9b21e2c93ea56",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "모델 및 토크나이저 로드 완료\n"
     ]
    }
   ],
   "source": [
    "# 모델 ID 설정 \n",
    "model_id = \"Qwen/Qwen3-14B\"\n",
    "\n",
    "# # 1. 4-bit 양자화 설정\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    ")\n",
    "\n",
    "# 2. 모델 로드\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True,\n",
    "    attn_implementation=\"sdpa\"  # 최신 PyTorch 사용 시 가속\n",
    ")\n",
    "\n",
    "# 3. 토크나이저 로드\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id, trust_remote_code=True)\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"right\" # trl의 SFTTrainer는 right padding을 지원한다.\n",
    "\n",
    "# 4. 학습 전처리 (Gradient Checkpointing 활성화 등)\n",
    "model = prepare_model_for_kbit_training(model)\n",
    "\n",
    "print(\"모델 및 토크나이저 로드 완료\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 일반 16bit 모델과 4bit 모델 로드를 위한 사용자 정의함수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "# optional imports (환경에 따라 없을 수 있음)\n",
    "try:\n",
    "    from transformers import BitsAndBytesConfig\n",
    "except Exception:\n",
    "    BitsAndBytesConfig = None\n",
    "\n",
    "try:\n",
    "    from transformers import Mxfp4Config\n",
    "except Exception:\n",
    "    Mxfp4Config = None\n",
    "\n",
    "try:\n",
    "    from peft import prepare_model_for_kbit_training\n",
    "except Exception:\n",
    "    prepare_model_for_kbit_training = None\n",
    "\n",
    "\n",
    "def _is_gpt_oss(model_id: str) -> bool:\n",
    "    mid = (model_id or \"\").lower()\n",
    "    return \"openai/gpt-oss\" in mid or mid.startswith(\"openai/gpt-oss\")\n",
    "\n",
    "\n",
    "def _make_bnb_nf4_4bit_config():\n",
    "    if BitsAndBytesConfig is None:\n",
    "        raise ImportError(\n",
    "            \"BitsAndBytesConfig를 import할 수 없음. \"\n",
    "            \"pip install -U bitsandbytes transformers accelerate 를 확인하라.\"\n",
    "        )\n",
    "    return BitsAndBytesConfig(\n",
    "        load_in_4bit=True,\n",
    "        bnb_4bit_quant_type=\"nf4\",\n",
    "        bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "        bnb_4bit_use_double_quant=True,\n",
    "    )\n",
    "\n",
    "def load_model_and_tokenizer(\n",
    "    model_id: str,\n",
    "    *,\n",
    "    device_map: str = \"auto\",\n",
    "    trust_remote_code: bool = True,\n",
    "    gpt_oss_dequantize_to_bf16: bool = False,  # gpt-oss-20b만 해당\n",
    "    enable_gradient_checkpointing: bool = True,\n",
    "):\n",
    "    \"\"\"\n",
    "    - gpt-oss-20b: MXFP4 네이티브 로드(기본) 또는 BF16 디양자화 로드\n",
    "    - 그 외: bitsandbytes NF4 4bit 로드 + (가능하면) k-bit 학습 준비\n",
    "    \"\"\"\n",
    "    is_oss = _is_gpt_oss(model_id)\n",
    "\n",
    "    if is_oss:\n",
    "        if Mxfp4Config is None:\n",
    "            raise ImportError(\n",
    "                \"Mxfp4Config를 import할 수 없음. \"\n",
    "                \"pip install -U transformers 로 업데이트하라.\"\n",
    "            )\n",
    "\n",
    "        quant_cfg = Mxfp4Config(dequantize=bool(gpt_oss_dequantize_to_bf16))\n",
    "        torch_dtype = torch.bfloat16 if gpt_oss_dequantize_to_bf16 else \"auto\"\n",
    "\n",
    "        model = AutoModelForCausalLM.from_pretrained(\n",
    "            model_id,\n",
    "            device_map=device_map,\n",
    "            trust_remote_code=trust_remote_code,\n",
    "            quantization_config=quant_cfg,\n",
    "            torch_dtype=torch_dtype,\n",
    "            # MXFP4 환경에서 sdpa가 안 맞는 경우가 있어 안전하게 eager 권장\n",
    "            attn_implementation=\"eager\",\n",
    "        )\n",
    "\n",
    "        mode = \"gpt-oss / MXFP4\"\n",
    "        if gpt_oss_dequantize_to_bf16:\n",
    "            mode += \" (dequantize->bf16)\"\n",
    "\n",
    "    else:\n",
    "        bnb_cfg = _make_bnb_nf4_4bit_config()\n",
    "\n",
    "        model = AutoModelForCausalLM.from_pretrained(\n",
    "            model_id,\n",
    "            device_map=device_map,\n",
    "            trust_remote_code=trust_remote_code,\n",
    "            quantization_config=bnb_cfg,\n",
    "            torch_dtype=torch.bfloat16,\n",
    "            attn_implementation=\"sdpa\",\n",
    "        )\n",
    "\n",
    "        # QLoRA/4bit 학습 준비(가능할 때만)\n",
    "        if prepare_model_for_kbit_training is not None:\n",
    "            model = prepare_model_for_kbit_training(model)\n",
    "        mode = \"bnb / NF4 4bit (QLoRA-ready)\"\n",
    "\n",
    "    # 공통: 학습 준비 옵션(권장)\n",
    "    if enable_gradient_checkpointing and hasattr(model, \"gradient_checkpointing_enable\"):\n",
    "        model.gradient_checkpointing_enable()\n",
    "    if hasattr(model.config, \"use_cache\"):\n",
    "        model.config.use_cache = False  # 학습 시 권장\n",
    "\n",
    "    # tokenizer\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_id, trust_remote_code=trust_remote_code)\n",
    "    if tokenizer.pad_token is None:\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "    tokenizer.padding_side = \"right\"\n",
    "\n",
    "    return model, tokenizer, {\"mode\": mode}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4232ce6706ee4484bf3aecad74592a3f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gpt-oss / MXFP4\n",
      "model dtype: torch.bfloat16\n"
     ]
    }
   ],
   "source": [
    "# gpt-oss-20b 로드 시\n",
    "model_id = \"openai/gpt-oss-20b\"\n",
    "# model_id = \"Qwen/Qwen3-14B\"\n",
    "\n",
    "# 1. gpt-oss-20b: MXFP4 네이티브 4bit\n",
    "model, tokenizer, info = load_model_and_tokenizer(\n",
    "    model_id, \n",
    "    gpt_oss_dequantize_to_bf16=False\n",
    ")\n",
    "\n",
    "# 2. gpt-oss-20b: BF16로 디양자화(학습 파이프라인 맞출 때)\n",
    "# model, tokenizer, info = load_model_and_tokenizer(model_id, gpt_oss_dequantize_to_bf16=True)\n",
    "\n",
    "print(info[\"mode\"])\n",
    "print(\"model dtype:\", getattr(model, \"dtype\", None))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Kxv0mwLKXify"
   },
   "source": [
    "## 6. LoRA 어댑터 설정 및 적용\n",
    "\n",
    "베이스 모델에 LoRA 어댑터를 부착한다. 모델의 모든 선형 레이어(`Linear`)를 타겟으로 지정하면 성능이 극대화된다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "kagYcTTVXify"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 7,962,624 || all params: 20,922,719,808 || trainable%: 0.0381\n"
     ]
    }
   ],
   "source": [
    "# LoRA 설정\n",
    "lora_config = LoraConfig(\n",
    "    r=16,                       # Rank: 16 (일반적인 권장값)\n",
    "    lora_alpha=32,              # Alpha: 32 (Rank의 2배)\n",
    "    target_modules=[            # 모든 Linear 레이어에 적용\n",
    "        \"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "        \"gate_proj\", \"up_proj\", \"down_proj\"\n",
    "    ],\n",
    "    lora_dropout=0.05,          # 드롭아웃\n",
    "    bias=\"none\",                # 바이어스 미학습\n",
    "    task_type=TaskType.CAUSAL_LM\n",
    ")\n",
    "\n",
    "# 모델에 LoRA 적용\n",
    "model = get_peft_model(model, lora_config)\n",
    "\n",
    "# 학습 가능한 파라미터 수 확인\n",
    "model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1ODvqaLmXifz"
   },
   "source": [
    "## 7. SFTTrainer 설정 및 학습 실행\n",
    "\n",
    "`trl` 라이브러리의 `SFTTrainer`를 사용하여 학습을 설정하고 실행한다. 메모리 부족(OOM)을 방지하기 위해 배치 크기와 그라디언트 누적(Accumulation)을 적절히 조절해야 한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "TckhmZzfXifz"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/josh/anaconda3/envs/llm/lib/python3.11/site-packages/peft/tuners/tuners_utils.py:282: UserWarning: Already found a `peft_config` attribute in the model. This will lead to having multiple adapters in the model. Make sure to know what you are doing!\n",
      "  warnings.warn(\n",
      "The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'bos_token_id': 199998}.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "학습 시작...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='189' max='189' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [189/189 15:25, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>2.441700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>1.564200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>1.378300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>1.359900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>1.415800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>1.336600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>1.350200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>1.289400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90</td>\n",
       "      <td>1.302600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>1.330000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>110</td>\n",
       "      <td>1.303900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>120</td>\n",
       "      <td>1.266100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>130</td>\n",
       "      <td>1.298600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>140</td>\n",
       "      <td>1.243700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>1.276600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>160</td>\n",
       "      <td>1.234200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>170</td>\n",
       "      <td>1.243000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>180</td>\n",
       "      <td>1.244200</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "학습 완료!\n"
     ]
    }
   ],
   "source": [
    "# 학습 결과 저장 경로\n",
    "output_dir = \"./GPT-OSS-20B-SFT-LoRA\"\n",
    "\n",
    "# 학습 설정 (SFTConfig)\n",
    "training_args = SFTConfig(\n",
    "    output_dir=output_dir,\n",
    "\n",
    "    # 학습 하이퍼파라미터\n",
    "    num_train_epochs=3,                 # 에포크 수 (실습용 1, 실제 3 권장)\n",
    "    per_device_train_batch_size=4,      # GPU당 배치 크기 (메모리에 따라 조절)\n",
    "    gradient_accumulation_steps=4,      # 그라디언트 누적 (실효 배치 크기 = 4 * 4 = 16)\n",
    "    learning_rate=2e-4,                 # QLoRA 권장 학습률\n",
    "\n",
    "    # 최적화 및 스케줄러\n",
    "    optim=\"paged_adamw_8bit\",           # 메모리 절약형 옵티마이저\n",
    "    lr_scheduler_type=\"cosine\",         # 코사인 스케줄러\n",
    "    warmup_ratio=0.03,                  # 워밍업 비율\n",
    "\n",
    "    # 정밀도 및 로깅\n",
    "    fp16=False,\n",
    "    bf16=True,                          # Ampere(30/40번대) GPU 이상 권장\n",
    "    logging_steps=10,                   # 10 스텝마다 로그 출력\n",
    "\n",
    "    # 저장 전략\n",
    "    save_strategy=\"steps\",\n",
    "    save_steps=50,                      # 50 스텝마다 체크포인트 저장\n",
    "\n",
    "    # 데이터 처리\n",
    "    #max_seq_length=512,                 # 시퀀스 길이 제한 (메모리 절약)\n",
    "    dataset_text_field=\"text\",          # 데이터셋의 텍스트 필드명\n",
    "    packing=False,                      # 데이터 패킹 비활성화 (개별 학습)\n",
    "\n",
    "    report_to=\"none\"                    # W&B 사용 시 \"wandb\"로 변경\n",
    ")\n",
    "\n",
    "# Trainer 생성\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    train_dataset=formatted_dataset,\n",
    "    args=training_args,\n",
    "    processing_class=tokenizer,\n",
    "    peft_config=lora_config             # LoRA 설정 전달\n",
    ")\n",
    "\n",
    "print(\"학습 시작...\")\n",
    "trainer.train()\n",
    "print(\"학습 완료!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "E_hzkHzKXifz"
   },
   "source": [
    "## 8. 모델 저장 및 테스트\n",
    "\n",
    "학습이 완료되면 어댑터(LoRA 가중치)를 저장하고, 간단한 추론 테스트를 통해 성능을 검증한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "IRts9BBbXifz"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.\n",
      "Caching is incompatible with gradient checkpointing in GptOssDecoderLayer. Setting `past_key_values=None`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "모델 저장 완료: ./GPT-OSS-20B-SFT-LoRA\n",
      "\n",
      "[추론 테스트 결과]\n",
      "--------------------------------------------------\n",
      "질문: 인공지능의 미래에 대해 설명해줘.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/josh/anaconda3/envs/llm/lib/python3.11/site-packages/torch/utils/checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "답변: 인(10. **Instruction() {\n",
      "                // Create a 'value': config = 2002]` has the component is `Array} \\) { \n",
      "        # In PHP code to manage their responses\n",
      "    def _instance`. The 'B be computed earlier text string manipulation of the 'string.IsDefined Functions\n",
      "\n",
      "- **Label>3, 'click(function () => \"What can solve this code snippet does not found on a vector<int64-bit number n^3/4) + dα+1, 'https://cdn.icons.fifoBuffer(buf: The output file. So they might be more information for an object\n",
      "--------------------------------------------------\n",
      "질문: Python으로 피보나치 수열을 구하는 코드를 작성해줘.\n",
      "답변: def def _context` is a 'label> output: A. So the code snippet above text, or 'reactive behavior of the number 2+1e5/3- `False\n",
      "\n",
      "The 'A$ = \\(\\math.pi / 4 => 1\n",
      "    print(f\" button = 100, \"isNaN} \\) + z + 0;\n",
      "            int(data[1, they might be used for file and the `get_data.data.data.data.decode('Password(password, perhaps a given description, I can use a function, and the system's inventory, the code is set.\n",
      "     *\n"
     ]
    }
   ],
   "source": [
    "# 1. 최종 모델(어댑터) 저장\n",
    "trainer.save_model(output_dir)\n",
    "tokenizer.save_pretrained(output_dir)\n",
    "print(f\"모델 저장 완료: {output_dir}\")\n",
    "\n",
    "# 2. 추론 테스트 함수\n",
    "def generate_response(prompt, model, tokenizer):\n",
    "    \"\"\"\n",
    "    학습된 모델로 응답을 생성하는 함수다.\n",
    "    \"\"\"\n",
    "    formatted_prompt = f\"### Instruction:\\n{prompt}\\n\\n### Response:\\n\"\n",
    "\n",
    "    inputs = tokenizer(formatted_prompt, return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=128,\n",
    "            temperature=0.7,\n",
    "            top_p=0.9,\n",
    "            repetition_penalty=1.1,\n",
    "            do_sample=True,\n",
    "            pad_token_id=tokenizer.pad_token_id,\n",
    "            eos_token_id=tokenizer.eos_token_id\n",
    "        )\n",
    "\n",
    "    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    # 프롬프트 부분 제거하고 응답만 추출\n",
    "    return response.split(\"### Response:\\n\")[-1].strip()\n",
    "\n",
    "# 3. 테스트 실행\n",
    "test_prompts = [\n",
    "    \"인공지능의 미래에 대해 설명해줘.\",\n",
    "    \"Python으로 피보나치 수열을 구하는 코드를 작성해줘.\"\n",
    "]\n",
    "\n",
    "print(\"\\n[추론 테스트 결과]\")\n",
    "for prompt in test_prompts:\n",
    "    print(\"-\" * 50)\n",
    "    print(f\"질문: {prompt}\")\n",
    "    response = generate_response(prompt, model, tokenizer)\n",
    "    print(f\"답변: {response}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jdD4-vJfXifz"
   },
   "source": [
    "## 9. 요약\n",
    "\n",
    "이 챕터에서는 **LoRA를 활용한 SFT**의 전체 파이프라인을 실습했다.\n",
    "\n",
    "1. **데이터 준비**: Alpaca 포맷 데이터를 모델 학습용 프롬프트로 변환했다.\n",
    "2. **QLoRA 로딩**: 4-bit 양자화된 베이스 모델을 로드하여 메모리 효율을 확보했다.\n",
    "3. **LoRA 설정**: 모든 선형 레이어에 어댑터를 부착하여 학습 성능을 극대화했다.\n",
    "4. **학습 실행**: `SFTTrainer`와 `paged_adamw_8bit` 옵티마이저를 통해 안정적으로 학습을 수행했다.\n",
    "\n",
    "이로써 기본적인 파인튜닝 과정은 마쳤다. 다음 챕터에서는 QLoRA에 특화된 심화 내용과 팁을 더 깊이 다룬다.\n",
    "\n",
    "다음 챕터는 **Chapter 07: QLoRA를 활용한 SFT**로, 이미 6장에서 QLoRA를 사용했지만, 7장에서는 QLoRA의 내부 동작 원리와 메모리 최적화에 대해 더 심도 있게 다루거나 6장과 통합하여 진행할 수 있다. (사용자의 요청 순서에 따라 7장을 진행한다.)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "graph",
   "language": "python",
   "name": "graph"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4oen4NCYbCX0"
   },
   "source": [
    "# Chapter 12: 고급 LoRA 기법\n",
    "\n",
    "## 1. 학습 목표\n",
    "\n",
    "* **DoRA (Weight-Decomposed LoRA)**와 **AdaLoRA** 등 고급 PEFT 기법의 원리를 이해한다.\n",
    "* `use_dora=True` 등 간단한 설정으로 학습 성능을 높이는 방법을 실습한다.\n",
    "* 데이터 규모에 따른 최적의 **Rank** 선택 가이드를 익힌다.\n",
    "* **선택적 레이어 학습**을 통해 재앙적 망각(Catastrophic Forgetting)을 방지한다.\n",
    "\n",
    "## 2. DoRA (Weight-Decomposed LoRA)\n",
    "\n",
    "### 2.1 개념\n",
    "\n",
    "DoRA는 2024년에 제안된 기법으로, 가중치를 **크기(Magnitude)**와 **방향(Direction)**으로 분해하여 학습한다. LoRA는 방향만 업데이트하는 경향이 있는데, DoRA는 크기 벡터를 별도로 학습하여 Full Fine-tuning에 더 가까운 학습 패턴을 보인다.\n",
    "\n",
    "```text\n",
    "W = m × (V / ||V||)\n",
    "- m: 크기 벡터 (Magnitude) → 직접 학습 (파라미터 수 매우 적음)\n",
    "- V: 방향 행렬 (Direction) → LoRA로 학습 (저순위 행렬 분해)\n",
    "\n",
    "```\n",
    "\n",
    "### 2.2 장점\n",
    "\n",
    "* 같은 Rank에서 일반 LoRA보다 더 높은 성능을 보인다.\n",
    "* 특히 낮은 Rank (r=4, 8)에서도 학습 안정성이 뛰어나다.\n",
    "* 별도의 추론 오버헤드가 없다 (병합 시 일반 LoRA와 동일).\n",
    "\n",
    "### 2.3 설정 실습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "khjU__3KbCX2"
   },
   "outputs": [],
   "source": [
    "from peft import LoraConfig, get_peft_model\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "# 모델 로드 (예시)\n",
    "#model = AutoModelForCausalLM.from_pretrained(\"Qwen/Qwen3-14B\", ...)\n",
    "\n",
    "# DoRA 설정\n",
    "# use_dora=True 옵션만 추가하면 된다.\n",
    "dora_config = LoraConfig(\n",
    "    r=16,\n",
    "    lora_alpha=32,\n",
    "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    "    use_dora=True  # 핵심 설정: DoRA 활성화\n",
    ")\n",
    "\n",
    "print(\"DoRA 설정 완료: use_dora=True\")\n",
    "#model = get_peft_model(model, dora_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "--PYdcrrbCX2"
   },
   "source": [
    "## 3. AdaLoRA (Adaptive LoRA)\n",
    "\n",
    "### 3.1 개념\n",
    "\n",
    "모든 레이어에 동일한 Rank를 할당하는 것은 비효율적일 수 있다. AdaLoRA는 학습 과정에서 **중요한 레이어에는 높은 Rank**를, 덜 중요한 레이어에는 낮은 Rank를 동적으로 할당한다. 이를 통해 파라미터 예산을 효율적으로 사용한다.\n",
    "\n",
    "### 3.2 설정 실습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "6KuNEXOzbCX3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AdaLoRA 설정 완료: 동적 Rank 할당\n"
     ]
    }
   ],
   "source": [
    "from peft import AdaLoraConfig\n",
    "\n",
    "# AdaLoRA 설정\n",
    "adalora_config = AdaLoraConfig(\n",
    "    init_r=16,              # 초기 Rank (약간 높게 시작)\n",
    "    target_r=8,             # 목표 평균 Rank (최종적으로 맞출 Rank)\n",
    "    tinit=200,              # 워밍업 스텝 (초기에는 Rank 조절 안 함)\n",
    "    tfinal=1000,            # Rank 조절 종료 스텝\n",
    "    deltaT=10,              # Rank 업데이트 간격\n",
    "    orth_reg_weight=0.1,    # 정직교성 규제 가중치\n",
    "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"],\n",
    "    task_type=\"CAUSAL_LM\",\n",
    "    total_step=10000\n",
    ")\n",
    "\n",
    "print(\"AdaLoRA 설정 완료: 동적 Rank 할당\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Y0AfjogbbCX3"
   },
   "source": [
    "## 4. Rank 선택 가이드\n",
    "\n",
    "Rank(`r`)는 학습 가능한 파라미터 수와 직결된다. 무조건 높다고 좋은 것은 아니며, 데이터 규모와 태스크 복잡도에 따라 적절한 값을 선택해야 한다.\n",
    "\n",
    "### 4.1 데이터 규모별 권장 Rank\n",
    "\n",
    "| 데이터 규모 (예제 수) | 권장 Rank | 설명 |\n",
    "| --- | --- | --- |\n",
    "| **500 ~ 1,000** | **r=8** | 적은 데이터로 과적합 방지, 표면적인 톤앤매너 학습에 적합 |\n",
    "| **1,000 ~ 5,000** | **r=16** | 일반적인 SFT에 가장 권장되는 값, 안정적인 성능 |\n",
    "| **5,000 ~ 20,000** | **r=32** | 복잡한 지시사항이나 새로운 지식 주입이 필요할 때 |\n",
    "| **20,000+** | **r=64** | 데이터가 충분히 많고, 논리적 추론 능력을 강화해야 할 때 |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "_WfL9jXTbCX3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "데이터 규모별 Rank 추천:\n",
      "  데이터 800개 → 권장 Rank: 8\n",
      "  데이터 3000개 → 권장 Rank: 16\n",
      "  데이터 15000개 → 권장 Rank: 32\n",
      "  데이터 50000개 → 권장 Rank: 64\n"
     ]
    }
   ],
   "source": [
    "def recommend_rank(dataset_size):\n",
    "    \"\"\"데이터 크기에 따른 Rank 추천 함수\"\"\"\n",
    "    if dataset_size < 1000:\n",
    "        return 8\n",
    "    elif dataset_size < 5000:\n",
    "        return 16\n",
    "    elif dataset_size < 20000:\n",
    "        return 32\n",
    "    else:\n",
    "        return 64\n",
    "\n",
    "# 예시 확인\n",
    "dataset_sizes = [800, 3000, 15000, 50000]\n",
    "print(\"데이터 규모별 Rank 추천:\")\n",
    "for size in dataset_sizes:\n",
    "    print(f\"  데이터 {size}개 → 권장 Rank: {recommend_rank(size)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XnH-7FlBbCX3"
   },
   "source": [
    "## 5. 선택적 레이어 학습 (Layer Selection)\n",
    "\n",
    "모델의 **하위 레이어(앞쪽)**는 일반적인 언어적 특징(문법 등)을 담당하고, **상위 레이어(뒤쪽)**는 구체적인 정보나 태스크 특화 기능을 담당하는 경향이 있다.\n",
    "재앙적 망각(Catastrophic Forgetting)을 막고 싶다면, 앞쪽 레이어는 보존하고 뒤쪽 레이어만 학습하는 전략이 유효하다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "7mKkBWNNbCX3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "선택적 학습 설정: Layer 20 ~ 39만 학습\n",
      "효과: 초기 레이어의 일반 지식 보존, 후기 레이어의 태스크 적응\n"
     ]
    }
   ],
   "source": [
    "# 후반부 레이어만 학습하도록 설정하는 예시\n",
    "# gpt-oss-20b (Qwen-14B)는 약 40~48개의 레이어를 가짐\n",
    "total_layers = 40\n",
    "train_start_layer = 20  # 절반 이후부터 학습\n",
    "\n",
    "selective_config = LoraConfig(\n",
    "    r=16,\n",
    "    lora_alpha=32,\n",
    "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"],\n",
    "\n",
    "    # layers_to_transform: 특정 인덱스의 레이어만 LoRA 적용\n",
    "    layers_to_transform=list(range(train_start_layer, total_layers)),\n",
    "\n",
    "    task_type=\"CAUSAL_LM\"\n",
    ")\n",
    "\n",
    "print(f\"선택적 학습 설정: Layer {train_start_layer} ~ {total_layers-1}만 학습\")\n",
    "print(\"효과: 초기 레이어의 일반 지식 보존, 후기 레이어의 태스크 적응\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "69Jia6tUbCX3"
   },
   "source": [
    "## 6. 요약\n",
    "\n",
    "| 기법 | 특징 | 추천 상황 |\n",
    "| --- | --- | --- |\n",
    "| **Standard LoRA** | 가장 기본적이고 검증됨 | 일반적인 모든 상황 (Start here) |\n",
    "| **DoRA** | 크기와 방향 분리 학습 | **성능 최적화**가 필요할 때 (강력 추천) |\n",
    "| **AdaLoRA** | 레이어별 동적 Rank | 파라미터 효율을 극한으로 추구할 때 |\n",
    "| **선택적 학습** | 특정 레이어만 타겟팅 | 기존 지식 유지(망각 방지)가 중요할 때 |\n",
    "\n",
    "실무에서는 **DoRA (`use_dora=True`)**를 기본으로 사용하고, 데이터 양에 맞춰 Rank를 16~32 사이로 조절하는 것이 가장 무난하고 강력한 조합이다.\n",
    "\n",
    "다음 챕터는 **Chapter 13: 모델 평가 및 테스트**로, 학습된 모델의 성능을 객관적인 지표(Perplexity, BLEU 등)와 LLM-as-Judge 방식으로 평가하는 방법을 다룬다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "graph",
   "language": "python",
   "name": "graph"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

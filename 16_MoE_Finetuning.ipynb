{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Y_GZeuEpcR-m"
   },
   "source": [
    "# Chapter 16: MoE (Mixture-of-Experts) Fine-tuning\n",
    "\n",
    "## 1. 학습 목표\n",
    "\n",
    "* MoE 아키텍처의 작동 원리(Router와 Expert)를 이해한다.\n",
    "* **Router 붕괴(Router Collapse)** 문제를 이해하고 이를 방지하는 전략을 익힌다.\n",
    "* Router를 동결(Freeze)하고 Expert만 학습하는 LoRA 설정을 구현한다.\n",
    "* Mixtral 8x7B 등 대형 MoE 모델을 위한 최적의 하이퍼파라미터를 설정한다.\n",
    "\n",
    "## 2. MoE (Mixture-of-Experts) 아키텍처란?\n",
    "\n",
    "### 2.1 개념\n",
    "\n",
    "MoE는 모든 입력에 대해 모델의 전체 파라미터를 사용하는 Dense 모델과 달리, **조건부 연산(Conditional Computation)**을 수행한다. 입력 토큰에 따라 **일부 전문가(Expert)만 활성화**하여, 파라미터 수는 매우 많지만 실제 연산량(FLOPs)은 적게 유지하는 효율적인 구조다.\n",
    "\n",
    "```text\n",
    "구조: [Input] -> [Router (Gating)] -> [Top-K Experts] -> [Weighted Sum] -> [Output]\n",
    "\n",
    "예시 (Mixtral 8x7B):\n",
    "- 총 파라미터: 47B (470억 개)\n",
    "- 활성 파라미터: 13B (토큰당 2개의 Expert만 사용)\n",
    "-> 47B 모델의 지식을 가지고 있지만, 추론 속도는 13B 모델만큼 빠르다.\n",
    "\n",
    "```\n",
    "\n",
    "### 2.2 주요 MoE 모델\n",
    "\n",
    "* **Mixtral 8x7B**: 오픈소스 MoE의 표준. 8개의 Expert 중 2개를 사용(Top-2).\n",
    "* **DeepSeek-MoE**: 64개 이상의 잘게 쪼개진(Fine-grained) Expert 사용.\n",
    "* **Qwen2-MoE**: 고성능 경량 MoE 모델.\n",
    "\n",
    "## 3. MoE Fine-tuning의 핵심 난관: Router 붕괴\n",
    "\n",
    "MoE 모델을 일반적인 방법(높은 학습률, 모든 파라미터 학습)으로 튜닝하면 **Router 붕괴(Router Collapse)** 현상이 발생하기 쉽다.\n",
    "이는 Router가 특정 Expert(예: Expert 1)만 계속 선택하게 되어, 나머지 Expert들이 놀게 되고 모델 성능이 급격히 저하되는 현상이다.\n",
    "\n",
    "### MoE 튜닝 2대 원칙\n",
    "\n",
    "1. **Router 동결 (Freeze)**: Router가 이미 학습한 분배 능력을 유지하도록 가중치를 업데이트하지 않는다.\n",
    "2. **낮은 학습률 (Low LR)**: 일반 모델(2e-4)보다 훨씬 낮은 **1e-5 ~ 5e-6** 수준의 학습률을 사용해야 한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. 도메인 데이터셋 준비\n",
    "\n",
    "각 분야별로 대표적인 오픈소스 데이터셋들이 존재한다. 이번 실습에서는 금융 분야의 `gbharti/finance-alpaca`를 사용한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "금융 데이터셋 로딩 중...\n",
      "데이터 개수: 1000\n",
      "샘플 데이터:\n",
      "{'instruction': 'For a car, what scams can be plotted with 0% financing vs rebate?', 'input': '', 'output': \"The car deal makes money 3 ways. If you pay in one lump payment. If the payment is greater than what they paid for the car, plus their expenses, they make a profit. They loan you the money. You make payments over months or years, if the total amount you pay is greater than what they paid for the car, plus their expenses, plus their finance expenses they make money. Of course the money takes years to come in, or they sell your loan to another business to get the money faster but in a smaller amount. You trade in a car and they sell it at a profit. Of course that new transaction could be a lump sum or a loan on the used car... They or course make money if you bring the car back for maintenance, or you buy lots of expensive dealer options. Some dealers wave two deals in front of you: get a 0% interest loan. These tend to be shorter 12 months vs 36,48,60 or even 72 months. The shorter length makes it harder for many to afford. If you can't swing the 12 large payments they offer you at x% loan for y years that keeps the payments in your budget. pay cash and get a rebate. If you take the rebate you can't get the 0% loan. If you take the 0% loan you can't get the rebate. The price you negotiate minus the rebate is enough to make a profit. The key is not letting them know which offer you are interested in. Don't even mention a trade in until the price of the new car has been finalized. Otherwise they will adjust the price, rebate, interest rate, length of loan,  and trade-in value to maximize their profit. The suggestion of running the numbers through a spreadsheet is a good one. If you get a loan for 2% from your bank/credit union for 3 years and the rebate from the dealer, it will cost less in total than the 0% loan from the dealer. The key is to get the loan approved by the bank/credit union before meeting with the dealer. The money from the bank looks like cash to the dealer.\", 'text': ''}\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# 도메인별 대표 데이터셋 (참고용)\n",
    "domain_datasets = {\n",
    "    \"금융\": \"gbharti/finance-alpaca\",\n",
    "    \"의료\": \"medalpaca/medical_meadow_medical_flashcards\",\n",
    "    \"코딩\": \"TokenBender/code_instructions_122k_alpaca_style\"\n",
    "}\n",
    "\n",
    "# 1. 금융 데이터셋 로드\n",
    "print(\"금융 데이터셋 로딩 중...\")\n",
    "dataset = load_dataset(\"gbharti/finance-alpaca\", split=\"train[:1000]\")\n",
    "\n",
    "print(f\"데이터 개수: {len(dataset)}\")\n",
    "print(f\"샘플 데이터:\\n{dataset[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. 시스템 프롬프트 설계 (페르소나 주입)\n",
    "\n",
    "도메인 적응의 핵심은 모델에게 **\"당신은 전문가입니다\"**라고 최면을 거는 것이다. 이를 위해 데이터 포맷팅 단계에서 시스템 프롬프트를 추가한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[포맷팅된 데이터 샘플]\n",
      "### System:\n",
      "당신은 금융 전문가 AI 어시스턴트이다. 질문에 대해 전문적이고 분석적인 답변을 제공하라.\n",
      "\n",
      "### Instruction:\n",
      "For a car, what scams can be plotted with 0% financing vs rebate?\n",
      "\n",
      "### Response:\n",
      "The car deal makes money 3 ways. If you pay in one lump payment. If the payment is greater than what they paid for the car, plus their expenses, they make a profit. They loan you the money. You make payments over months or years, if the total amount you pay is greater than what they paid for the car, plus their expenses, plus their finance expenses they make money. Of course the money takes years to come in, or they sell your loan to another business to get the money faster but in a smaller amount. You trade in a car and they sell it at a profit. Of course that new transaction could be a lump sum or a loan on the used car... They or course make money if you bring the car back for maintenance, or you buy lots of expensive dealer options. Some dealers wave two deals in front of you: get a 0% interest loan. These tend to be shorter 12 months vs 36,48,60 or even 72 months. The shorter length makes it harder for many to afford. If you can't swing the 12 large payments they offer you at x% loan for y years that keeps the payments in your budget. pay cash and get a rebate. If you take the rebate you can't get the 0% loan. If you take the 0% loan you can't get the rebate. The price you negotiate minus the rebate is enough to make a profit. The key is not letting them know which offer you are interested in. Don't even mention a trade in until the price of the new car has been finalized. Otherwise they will adjust the price, rebate, interest rate, length of loan,  and trade-in value to maximize their profit. The suggestion of running the numbers through a spreadsheet is a good one. If you get a loan for 2% from your bank/credit union for 3 years and the rebate from the dealer, it will cost less in total than the 0% loan from the dealer. The key is to get the loan approved by the bank/credit union before meeting with the dealer. The money from the bank looks like cash to the dealer.\n"
     ]
    }
   ],
   "source": [
    "def format_domain_instruction(example, domain=\"금융\"):\n",
    "    \"\"\"\n",
    "    도메인별 시스템 프롬프트를 적용하여 데이터를 포맷팅하는 함수다.\n",
    "    \"\"\"\n",
    "    # 도메인별 시스템 프롬프트 정의\n",
    "    system_prompts = {\n",
    "        \"금융\": \"당신은 금융 전문가 AI 어시스턴트이다. 질문에 대해 전문적이고 분석적인 답변을 제공하라.\",\n",
    "        \"의료\": \"당신은 의료 정보 AI 어시스턴트이다. 의학적 사실에 기반하여 답변하되, 반드시 전문가와의 상담을 권유하라.\",\n",
    "        \"법률\": \"당신은 법률 전문가 AI 어시스턴트이다. 관련 법령과 판례에 기반하여 답변하되, 법적 효력이 없음을 명시하라.\"\n",
    "    }\n",
    "\n",
    "    instruction = example['instruction']\n",
    "    input_text = example.get('input', '')\n",
    "    output = example['output']\n",
    "\n",
    "    # 해당 도메인의 시스템 프롬프트 선택\n",
    "    sys_prompt = system_prompts.get(domain, \"당신은 유용한 AI 어시스턴트이다.\")\n",
    "\n",
    "    # 프롬프트 구성\n",
    "    if input_text:\n",
    "        text = f\"\"\"### System:\n",
    "{sys_prompt}\n",
    "\n",
    "### Instruction:\n",
    "{instruction}\n",
    "\n",
    "### Input:\n",
    "{input_text}\n",
    "\n",
    "### Response:\n",
    "{output}\"\"\"\n",
    "    else:\n",
    "        text = f\"\"\"### System:\n",
    "{sys_prompt}\n",
    "\n",
    "### Instruction:\n",
    "{instruction}\n",
    "\n",
    "### Response:\n",
    "{output}\"\"\"\n",
    "\n",
    "    return {\"text\": text}\n",
    "\n",
    "# 금융 도메인으로 포맷팅 적용\n",
    "formatted_dataset = dataset.map(lambda x: format_domain_instruction(x, \"금융\"))\n",
    "\n",
    "print(\"\\n[포맷팅된 데이터 샘플]\")\n",
    "print(formatted_dataset[0]['text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Y_GZeuEpcR-m"
   },
   "source": [
    "## 6. MoE 모델 로드 및 구조 분석\n",
    "\n",
    "실습을 위해 `Qwen/Qwen1.5-MoE-A2.7B`와 같은 작은 MoE 모델이나 `mistralai/Mixtral-8x7B-v0.1`을 로드한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "E_xXZp2UcR-n"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoE 모델 로딩: mistralai/Mixtral-8x7B-v0.1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7056563c92d4488d96e849c895da702c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 19 files:   0%|          | 0/19 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "79560e4416984e8a80d01b59214f289b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00018-of-00019.safetensors:   0%|          | 0.00/4.98G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4f651be85de743a599570180a8e63b7d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00013-of-00019.safetensors:   0%|          | 0.00/4.98G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "65043b4a16794518ac66f610114c867e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00017-of-00019.safetensors:   0%|          | 0.00/4.90G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "55227019939f49408b60b24410143c23",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00016-of-00019.safetensors:   0%|          | 0.00/4.98G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "98b4af1569e5435498ee3c5424c32be4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00019-of-00019.safetensors:   0%|          | 0.00/4.22G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f10677980c76431cb04d16e5703b1e4a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/19 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8c8781768e744fb38fde387f840fcd63",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/116 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6db8d553eec643e6ab9229711d196345",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/967 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ef1b8d7c1fe142e38ec36e434b36864c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.model:   0%|          | 0.00/493k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bea1ab41e39d4425bdf4912db17f81cf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b06636702f0a423ab8ca3b6841144e54",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/72.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "모델 모듈 이름 확인:\n",
      "Router 발견: model.layers.0.block_sparse_moe.gate\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "\n",
    "# 1. 4-bit 양자화 설정 (MoE는 모델이 크므로 필수)\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    ")\n",
    "\n",
    "# 2. MoE 모델 로드 (예: Mixtral-8x7B)\n",
    "model_name = \"mistralai/Mixtral-8x7B-v0.1\"\n",
    "\n",
    "print(f\"MoE 모델 로딩: {model_name}\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# 3. 모델 구조 확인 (Router 모듈 이름 찾기)\n",
    "print(\"모델 모듈 이름 확인:\")\n",
    "for name, module in model.named_modules():\n",
    "    if \"gate\" in name or \"router\" in name:\n",
    "        print(f\"Router 발견: {name}\")\n",
    "        break\n",
    "# Mixtral의 경우 보통 'block_sparse_moe.gate' 등의 이름을 가짐"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Uf1IIp4ocR-o"
   },
   "source": [
    "## 7. Router Freezing을 위한 LoRA 설정\n",
    "\n",
    "LoRA 설정 시 `modules_to_save`나 타겟 모듈 설정을 통해 Router 학습을 방지하거나 명시적으로 제외해야 한다. 가장 안전한 방법은 Router를 LoRA 타겟에서 제외하고, 원본 파라미터도 동결하는 것이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "z9BOdL2IcR-o"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 6,815,744 || all params: 46,709,608,448 || trainable%: 0.0146\n"
     ]
    }
   ],
   "source": [
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
    "\n",
    "# 학습 전처리\n",
    "model = prepare_model_for_kbit_training(model)\n",
    "\n",
    "def get_moe_lora_config(model_type=\"mixtral\"):\n",
    "    \"\"\"\n",
    "    MoE 모델용 LoRA 설정. Router는 학습하지 않도록 설정한다.\n",
    "    \"\"\"\n",
    "    # 모델별 타겟 모듈\n",
    "    if model_type == \"mixtral\":\n",
    "        target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"] # Attention만 학습 (안전)\n",
    "        # 전문가 네트워크(w1, w2, w3)까지 학습하려면 추가 가능하지만 VRAM 소모 큼\n",
    "\n",
    "    config = LoraConfig(\n",
    "        r=8,                           # MoE는 Rank를 낮게 잡아도 충분함\n",
    "        lora_alpha=16,\n",
    "        target_modules=target_modules,\n",
    "        modules_to_save=None,          # Router(gate)를 여기에 넣지 않음 -> 동결 유지\n",
    "        lora_dropout=0.05,\n",
    "        bias=\"none\",\n",
    "        task_type=\"CAUSAL_LM\"\n",
    "    )\n",
    "    return config\n",
    "\n",
    "lora_config = get_moe_lora_config(\"mixtral\")\n",
    "model = get_peft_model(model, lora_config)\n",
    "\n",
    "# 안전장치: Router 파라미터가 확실히 동결되었는지 수동 확인 및 조치\n",
    "for name, param in model.named_parameters():\n",
    "    if \"gate\" in name or \"router\" in name:\n",
    "        param.requires_grad = False  # 강제 동결\n",
    "\n",
    "model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Expert 모니터 + 자동 대응 콜백\n",
    "\n",
    "학습이 잘 되고 있는지 확인하려면 Loss뿐만 아니라 **Expert가 골고루 사용되고 있는지** 확인해야 한다. 특정 Expert만 사용된다면(Load Balancing 깨짐) 학습을 중단하고 LR을 낮춰야 한다.\n",
    "\n",
    "TRL SFTTrainer 학습 시 “Expert 분포 모니터링(W&B 로그)”을 그대로 붙혀서 매 every_n_steps마다 진단 프롬프트를 흘려 top1_share / entropy_norm / KL 등을 기록하는 Class를 삽입한다.\n",
    "\n",
    "\n",
    "TRL SFTTrainer 학습 중 Expert 쏠림(load imbalance)을 감지하면\n",
    "- LR을 자동으로 낮추고(예: ×0.5)\n",
    "- 쏠림이 일정 횟수 연속으로 지속되면 학습을 자동 중단(early stop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from dataclasses import dataclass\n",
    "from collections import defaultdict\n",
    "from typing import List, Dict, Any, Optional\n",
    "\n",
    "import torch\n",
    "from transformers import TrainerCallback\n",
    "\n",
    "try:\n",
    "    import torch.distributed as dist\n",
    "except Exception:\n",
    "    dist = None\n",
    "\n",
    "try:\n",
    "    import wandb\n",
    "except Exception:\n",
    "    wandb = None\n",
    "\n",
    "\n",
    "def _is_rank0() -> bool:\n",
    "    if dist is None or not dist.is_available() or not dist.is_initialized():\n",
    "        return True\n",
    "    return dist.get_rank() == 0\n",
    "\n",
    "\n",
    "def _find_router_modules(model, keywords=(\"moe.gate\", \".gate\", \"router\", \"gate\")):\n",
    "    hits = []\n",
    "    for name, module in model.named_modules():\n",
    "        lname = name.lower()\n",
    "        if any(k in lname for k in keywords) and isinstance(module, torch.nn.Linear):\n",
    "            hits.append((name, module))\n",
    "    return hits\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def monitor_expert_usage(\n",
    "    model,\n",
    "    tokenizer,\n",
    "    input_texts: List[str],\n",
    "    *,\n",
    "    top_k: int = 2,\n",
    "    max_length: int = 256,\n",
    "    keywords=(\"moe.gate\", \".gate\", \"router\", \"gate\"),\n",
    ") -> Dict[str, Dict[str, Any]]:\n",
    "    model.eval()\n",
    "    device = next(model.parameters()).device\n",
    "\n",
    "    router_modules = _find_router_modules(model, keywords=keywords)\n",
    "    if not router_modules:\n",
    "        raise RuntimeError(\"라우터/게이트 모듈을 찾지 못했다. keywords를 조정하라.\")\n",
    "\n",
    "    stats = {\n",
    "        \"counts\": defaultdict(lambda: None),\n",
    "        \"entropy_sum\": defaultdict(float),\n",
    "        \"calls\": defaultdict(int),\n",
    "    }\n",
    "\n",
    "    hooks = []\n",
    "\n",
    "    def make_hook(name):\n",
    "        def hook_fn(module, inputs, output):\n",
    "            logits = output\n",
    "            if not torch.is_tensor(logits) or logits.ndim < 2:\n",
    "                return\n",
    "\n",
    "            n = logits.shape[-1]\n",
    "            flat = logits.reshape(-1, n).float()\n",
    "            probs = torch.softmax(flat, dim=-1)\n",
    "\n",
    "            ent = -(probs * probs.clamp_min(1e-12).log()).sum(dim=-1)\n",
    "            stats[\"entropy_sum\"][name] += ent.mean().item()\n",
    "            stats[\"calls\"][name] += 1\n",
    "\n",
    "            k = min(top_k, n)\n",
    "            top_idx = probs.topk(k=k, dim=-1).indices  # [tokens, k]\n",
    "\n",
    "            counts = stats[\"counts\"][name]\n",
    "            if counts is None or counts.numel() != n:\n",
    "                counts = torch.zeros(n, dtype=torch.long)\n",
    "                stats[\"counts\"][name] = counts\n",
    "\n",
    "            for i in range(k):\n",
    "                idx = top_idx[:, i].detach().cpu()\n",
    "                counts.index_add_(0, idx, torch.ones_like(idx, dtype=torch.long))\n",
    "        return hook_fn\n",
    "\n",
    "    for name, mod in router_modules:\n",
    "        hooks.append(mod.register_forward_hook(make_hook(name)))\n",
    "\n",
    "    batch = tokenizer(\n",
    "        input_texts,\n",
    "        return_tensors=\"pt\",\n",
    "        padding=True,\n",
    "        truncation=True,\n",
    "        max_length=max_length,\n",
    "    ).to(device)\n",
    "\n",
    "    _ = model(**batch)\n",
    "\n",
    "    for h in hooks:\n",
    "        h.remove()\n",
    "\n",
    "    results = {}\n",
    "    for name, _ in router_modules:\n",
    "        counts = stats[\"counts\"][name]\n",
    "        if counts is None or counts.sum().item() == 0:\n",
    "            continue\n",
    "\n",
    "        total = counts.sum().item()\n",
    "        p = (counts.float() / total)\n",
    "        n = p.numel()\n",
    "\n",
    "        top1_share = p.max().item()\n",
    "        entropy = -(p * p.clamp_min(1e-12).log()).sum().item()\n",
    "        entropy_norm = entropy / math.log(n)\n",
    "\n",
    "        uniform = torch.full_like(p, 1.0 / n)\n",
    "        kl_to_uniform = (p * (p.clamp_min(1e-12).log() - uniform.log())).sum().item()\n",
    "\n",
    "        avg_token_entropy = stats[\"entropy_sum\"][name] / max(stats[\"calls\"][name], 1)\n",
    "\n",
    "        results[name] = {\n",
    "            \"num_experts\": int(n),\n",
    "            \"top1_share\": float(top1_share),\n",
    "            \"entropy_norm\": float(entropy_norm),\n",
    "            \"kl_to_uniform\": float(kl_to_uniform),\n",
    "            \"avg_token_entropy\": float(avg_token_entropy),\n",
    "            \"counts\": counts.tolist(),\n",
    "        }\n",
    "\n",
    "    return results\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class ExpertAutoControlConfig:\n",
    "    sample_texts: List[str]\n",
    "    every_n_steps: int = 50\n",
    "\n",
    "    # 모니터링 파라미터\n",
    "    top_k: int = 2\n",
    "    max_length: int = 256\n",
    "    keywords: tuple = (\"moe.gate\", \".gate\", \"router\", \"gate\")\n",
    "    log_prefix: str = \"moe_expert\"\n",
    "\n",
    "    # 쏠림 판정 기준(원하면 조절)\n",
    "    top1_share_threshold: float = 0.70\n",
    "    entropy_norm_threshold: float = 0.40\n",
    "\n",
    "    # 자동 LR 감소\n",
    "    enable_lr_drop: bool = True\n",
    "    lr_drop_factor: float = 0.5      # lr *= 0.5\n",
    "    min_lr: float = 1e-7\n",
    "    cooldown_steps: int = 200        # 한번 LR 내린 후 N step 동안은 다시 안 내림\n",
    "\n",
    "    # 자동 중단(연속 N회 쏠림이면 stop)\n",
    "    enable_early_stop: bool = True\n",
    "    patience_alerts: int = 3         # 연속 3회 alert면 stop\n",
    "\n",
    "\n",
    "class ExpertAutoControlCallback(TrainerCallback):\n",
    "    def __init__(self, tokenizer, cfg: ExpertAutoControlConfig):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.cfg = cfg\n",
    "        self._consecutive_alerts = 0\n",
    "        self._last_lr_drop_step = -10**9\n",
    "\n",
    "    def _maybe_drop_lr(self, optimizer, step: int) -> Optional[float]:\n",
    "        if optimizer is None:\n",
    "            return None\n",
    "        if (step - self._last_lr_drop_step) < self.cfg.cooldown_steps:\n",
    "            return None\n",
    "\n",
    "        # 모든 param_group에 동일하게 적용\n",
    "        new_lrs = []\n",
    "        for g in optimizer.param_groups:\n",
    "            old = float(g.get(\"lr\", 0.0))\n",
    "            new = max(old * self.cfg.lr_drop_factor, self.cfg.min_lr)\n",
    "            g[\"lr\"] = new\n",
    "            new_lrs.append(new)\n",
    "\n",
    "        self._last_lr_drop_step = step\n",
    "        return float(min(new_lrs)) if new_lrs else None\n",
    "\n",
    "    def on_step_end(self, args, state, control, **kwargs):\n",
    "        step = int(state.global_step)\n",
    "        if step == 0 or (step % self.cfg.every_n_steps) != 0:\n",
    "            return control\n",
    "        if not _is_rank0():\n",
    "            return control\n",
    "\n",
    "        model = kwargs.get(\"model\", None)\n",
    "        optimizer = kwargs.get(\"optimizer\", None)\n",
    "        if model is None:\n",
    "            return control\n",
    "\n",
    "        metrics = monitor_expert_usage(\n",
    "            model=model,\n",
    "            tokenizer=self.tokenizer,\n",
    "            input_texts=self.cfg.sample_texts,\n",
    "            top_k=self.cfg.top_k,\n",
    "            max_length=self.cfg.max_length,\n",
    "            keywords=self.cfg.keywords,\n",
    "        )\n",
    "\n",
    "        # 모듈별 값을 평균으로 집계\n",
    "        vals_top1, vals_ent, vals_kl, vals_tokent = [], [], [], []\n",
    "        logs = {}\n",
    "\n",
    "        for name, m in metrics.items():\n",
    "            prefix = f\"{self.cfg.log_prefix}/{name}\"\n",
    "            logs[f\"{prefix}/top1_share\"] = m[\"top1_share\"]\n",
    "            logs[f\"{prefix}/entropy_norm\"] = m[\"entropy_norm\"]\n",
    "            logs[f\"{prefix}/kl_to_uniform\"] = m[\"kl_to_uniform\"]\n",
    "            logs[f\"{prefix}/avg_token_entropy\"] = m[\"avg_token_entropy\"]\n",
    "\n",
    "            if wandb is not None and wandb.run is not None:\n",
    "                logs[f\"{prefix}/counts_hist\"] = wandb.Histogram(m[\"counts\"])\n",
    "\n",
    "            vals_top1.append(m[\"top1_share\"])\n",
    "            vals_ent.append(m[\"entropy_norm\"])\n",
    "            vals_kl.append(m[\"kl_to_uniform\"])\n",
    "            vals_tokent.append(m[\"avg_token_entropy\"])\n",
    "\n",
    "        if not vals_top1:\n",
    "            return control\n",
    "\n",
    "        avg_top1 = float(sum(vals_top1) / len(vals_top1))\n",
    "        avg_ent = float(sum(vals_ent) / len(vals_ent))\n",
    "        avg_kl = float(sum(vals_kl) / len(vals_kl))\n",
    "        avg_tokent = float(sum(vals_tokent) / len(vals_tokent))\n",
    "\n",
    "        logs[f\"{self.cfg.log_prefix}/avg_top1_share\"] = avg_top1\n",
    "        logs[f\"{self.cfg.log_prefix}/avg_entropy_norm\"] = avg_ent\n",
    "        logs[f\"{self.cfg.log_prefix}/avg_kl_to_uniform\"] = avg_kl\n",
    "        logs[f\"{self.cfg.log_prefix}/avg_token_entropy\"] = avg_tokent\n",
    "\n",
    "        # 쏠림 판정\n",
    "        alert = (avg_top1 >= self.cfg.top1_share_threshold) and (avg_ent <= self.cfg.entropy_norm_threshold)\n",
    "        logs[f\"{self.cfg.log_prefix}/alert_load_imbalance\"] = int(alert)\n",
    "\n",
    "        if alert:\n",
    "            self._consecutive_alerts += 1\n",
    "        else:\n",
    "            self._consecutive_alerts = 0\n",
    "\n",
    "        logs[f\"{self.cfg.log_prefix}/consecutive_alerts\"] = int(self._consecutive_alerts)\n",
    "\n",
    "        # 자동 LR 감소\n",
    "        if alert and self.cfg.enable_lr_drop:\n",
    "            new_lr = self._maybe_drop_lr(optimizer, step)\n",
    "            if new_lr is not None:\n",
    "                logs[f\"{self.cfg.log_prefix}/action_lr_dropped\"] = 1\n",
    "                logs[f\"{self.cfg.log_prefix}/new_lr_min\"] = float(new_lr)\n",
    "            else:\n",
    "                logs[f\"{self.cfg.log_prefix}/action_lr_dropped\"] = 0\n",
    "        else:\n",
    "            logs[f\"{self.cfg.log_prefix}/action_lr_dropped\"] = 0\n",
    "\n",
    "        # 자동 중단\n",
    "        if alert and self.cfg.enable_early_stop and self._consecutive_alerts >= self.cfg.patience_alerts:\n",
    "            logs[f\"{self.cfg.log_prefix}/action_early_stop\"] = 1\n",
    "            control.should_training_stop = True\n",
    "        else:\n",
    "            logs[f\"{self.cfg.log_prefix}/action_early_stop\"] = 0\n",
    "\n",
    "        # 로깅\n",
    "        if wandb is not None and wandb.run is not None:\n",
    "            wandb.log(logs, step=step)\n",
    "        else:\n",
    "            short = {k: v for k, v in logs.items() if isinstance(v, (int, float))}\n",
    "            print(f\"[ExpertAutoControl step={step}] {short}\")\n",
    "\n",
    "        return control"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LFSPlZlJcR-o"
   },
   "source": [
    "## 7. MoE 최적화 학습 설정\n",
    "\n",
    "MoE 학습 자동모니터링과 자동 대응 콜백을 적용했다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Finishing previous runs because reinit is set to 'default'."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m The nbformat package was not found. It is required to save notebook history.\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>train/entropy</td><td>█▁▄</td></tr><tr><td>train/epoch</td><td>▁▅█</td></tr><tr><td>train/global_step</td><td>▁▅█</td></tr><tr><td>train/grad_norm</td><td>▁█▃</td></tr><tr><td>train/learning_rate</td><td>█▅▁</td></tr><tr><td>train/loss</td><td>█▅▁</td></tr><tr><td>train/mean_token_accuracy</td><td>▁▇█</td></tr><tr><td>train/num_tokens</td><td>▁▄█</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>train/entropy</td><td>2.01939</td></tr><tr><td>train/epoch</td><td>0.48</td></tr><tr><td>train/global_step</td><td>30</td></tr><tr><td>train/grad_norm</td><td>0.41797</td></tr><tr><td>train/learning_rate</td><td>1e-05</td></tr><tr><td>train/loss</td><td>2.1022</td></tr><tr><td>train/mean_token_accuracy</td><td>0.55681</td></tr><tr><td>train/num_tokens</td><td>154449</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">expert-shadow-7</strong> at: <a href='https://wandb.ai/kubwai/huggingface/runs/1j5fhonc' target=\"_blank\">https://wandb.ai/kubwai/huggingface/runs/1j5fhonc</a><br> View project at: <a href='https://wandb.ai/kubwai/huggingface' target=\"_blank\">https://wandb.ai/kubwai/huggingface</a><br>Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20251225_134057-1j5fhonc/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.23.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/josh/llm-finetuning/wandb/run-20251225_140013-4iz5o407</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/kubwai/mixtral-moe/runs/4iz5o407' target=\"_blank\">finance-sft</a></strong> to <a href='https://wandb.ai/kubwai/mixtral-moe' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/kubwai/mixtral-moe' target=\"_blank\">https://wandb.ai/kubwai/mixtral-moe</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/kubwai/mixtral-moe/runs/4iz5o407' target=\"_blank\">https://wandb.ai/kubwai/mixtral-moe/runs/4iz5o407</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "금융 도메인 적응 학습 시작...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='63' max='63' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [63/63 21:32, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>2.089800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>2.053200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>2.031300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>2.035100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>1.985500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>2.004900</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "학습 완료!\n"
     ]
    }
   ],
   "source": [
    "from trl import SFTConfig, SFTTrainer\n",
    "\n",
    "training_args = SFTConfig(\n",
    "    output_dir=\"./MIXTRAL-MoE-Finetuned\",\n",
    "    learning_rate=1e-5,\n",
    "    num_train_epochs=1,\n",
    "    per_device_train_batch_size=1,\n",
    "    gradient_accumulation_steps=16,\n",
    "    optim=\"paged_adamw_8bit\",\n",
    "    bf16=True,\n",
    "    logging_steps=10,\n",
    "    dataset_text_field=\"text\",\n",
    ")\n",
    "\n",
    "# W&B 초기화\n",
    "import wandb\n",
    "wandb.init(project=\"mixtral-moe\", name=\"finance-sft\")\n",
    "\n",
    "diag_texts = [\n",
    "    \"금융 상품 설명을 3줄로 요약해줘.\",\n",
    "    \"다음 문장에서 위험 요인을 찾아줘: '이 상품은 원금 손실이 발생할 수 있다.'\",\n",
    "    \"금리 인상이 채권 가격에 미치는 영향을 간단히 설명해줘.\",\n",
    "    \"아래 문장을 영어로 번역해줘: '해당 투자에는 손실 가능성이 있다.'\",\n",
    "]\n",
    "\n",
    "auto_cfg = ExpertAutoControlConfig(\n",
    "    sample_texts=diag_texts,\n",
    "    every_n_steps=50,\n",
    "\n",
    "    # 쏠림 기준(필요시 조절)\n",
    "    top1_share_threshold=0.70,\n",
    "    entropy_norm_threshold=0.40,\n",
    "\n",
    "    # 자동 LR drop + 자동 stop\n",
    "    enable_lr_drop=True,\n",
    "    lr_drop_factor=0.5,\n",
    "    cooldown_steps=200,\n",
    "    min_lr=1e-7,\n",
    "\n",
    "    enable_early_stop=True,\n",
    "    patience_alerts=3,\n",
    ")\n",
    "\n",
    "expert_auto_cb = ExpertAutoControlCallback(tokenizer=tokenizer, cfg=auto_cfg)\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    train_dataset=formatted_dataset,\n",
    "    args=training_args,\n",
    "    processing_class=tokenizer,\n",
    "    callbacks=[expert_auto_cb],   # callback 적용\n",
    ")\n",
    "\n",
    "print(\"금융 도메인 적응 학습 시작...\")\n",
    "trainer.train()\n",
    "print(\"학습 완료!\")\n",
    "\n",
    "trainer.save_model(\"./GMIXTRAL-MoE-Finetuned-Final\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "A7PG86XzcR-p"
   },
   "source": [
    "## 8. 요약\n",
    "\n",
    "이 챕터에서는 **MoE Fine-tuning**의 핵심 전략을 다루었다.\n",
    "\n",
    "1. **구조적 이점**: MoE는 '조건부 연산'을 통해 거대 모델의 지식과 작은 모델의 속도를 동시에 제공한다.\n",
    "2. **Router 보호**: 학습 중 Router가 망가지는 것을 막기 위해 **Router Freezing**과 **1e-5 이하의 낮은 학습률**이 필수적이다.\n",
    "3. **메모리 관리**: 모델 크기가 크므로 4-bit QLoRA와 Paged Optimizer가 거의 강제된다.\n",
    "\n",
    "---\n",
    "\n",
    "다음 챕터는 **Chapter 17: Post-training 기법**으로, OSFT 기법을 다룰 예정이다. 오픈소스 Training Hub 라이브러리를 활용하여 OSFT와 SFT를 구현할 예정이다."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "graph",
   "language": "python",
   "name": "graph"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

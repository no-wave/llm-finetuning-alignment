{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eNynbTwxVkUZ"
   },
   "source": [
    "# Chapter 04: 환경 구축\n",
    "\n",
    "## 1. 학습 목표\n",
    "\n",
    "* Fine-tuning을 위한 최적의 GPU 환경을 확인하고 설정한다.\n",
    "* `transformers`, `peft`, `bitsandbytes` 등 필수 라이브러리를 설치한다.\n",
    "* HuggingFace Hub에 로그인하여 모델 다운로드 권한을 확보한다.\n",
    "* 대용량 모델 관리를 위한 캐시 및 저장 경로를 체계적으로 구성한다.\n",
    "\n",
    "## 2. 하드웨어 요구사항\n",
    "\n",
    "LLM Fine-tuning은 막대한 연산 자원을 필요로 한다. 학습 방식에 따른 VRAM 요구량은 다음과 같다.\n",
    "\n",
    "### 2.1 gpt-oss-20b (20B 파라미터) 기준\n",
    "\n",
    "| 방식 | 최소 VRAM | 권장 GPU | 비고 |\n",
    "| --- | --- | --- | --- |\n",
    "| **Full Fine-tuning** | 160GB+ | 4x A100 (80GB) | 개인이 수행하기 어려움 |\n",
    "| **LoRA (FP16)** | 48GB+ | 1x A100 (80GB) | 고성능 GPU 필요 |\n",
    "| **QLoRA (4-bit)** | **24GB+** | **1x RTX 3090/4090** | **소비자용 GPU로 가능 (본 튜토리얼 권장)** |\n",
    "| **QLoRA (최소)** | 16GB | 1x RTX 4080 / T4 | 배치 사이즈 1, 짧은 시퀀스 길이로 제한적 가능 |\n",
    "\n",
    "### 2.2 클라우드 GPU 옵션\n",
    "\n",
    "로컬 GPU가 없다면 클라우드 환경을 이용할 수 있다.\n",
    "\n",
    "* **Google Colab**: Pro+ 요금제 사용 시 A100 또는 T4 GPU 사용 가능.\n",
    "* **RunPod / Lambda Labs**: 시간당 저렴한 비용으로 A100/H100 대여 가능.\n",
    "* **AWS / Azure / GCP**: 엔터프라이즈급 안정성 제공.\n",
    "\n",
    "## 3. GPU 환경 확인\n",
    "\n",
    "먼저 현재 환경에서 GPU를 사용할 수 있는지 확인한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "L7VVXmILVkUb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========================================\n",
      "GPU 환경 점검\n",
      "========================================\n",
      "발견된 GPU 개수: 8\n",
      "GPU 0: NVIDIA H100 80GB HBM3 | VRAM: 79.2 GB\n",
      "GPU 1: NVIDIA H100 80GB HBM3 | VRAM: 79.2 GB\n",
      "GPU 2: NVIDIA H100 80GB HBM3 | VRAM: 79.2 GB\n",
      "GPU 3: NVIDIA H100 80GB HBM3 | VRAM: 79.2 GB\n",
      "GPU 4: NVIDIA H100 80GB HBM3 | VRAM: 79.2 GB\n",
      "GPU 5: NVIDIA H100 80GB HBM3 | VRAM: 79.2 GB\n",
      "GPU 6: NVIDIA H100 80GB HBM3 | VRAM: 79.2 GB\n",
      "GPU 7: NVIDIA H100 80GB HBM3 | VRAM: 79.2 GB\n",
      "\n",
      "PyTorch 버전: 2.6.0+cu126\n",
      "CUDA 버전: 12.6\n",
      "\n",
      "[nvidia-smi 출력 결과]\n",
      "Thu Dec 25 09:01:33 2025       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 580.95.05              Driver Version: 580.95.05      CUDA Version: 13.0     |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  NVIDIA H100 80GB HBM3          Off |   00000000:03:00.0 Off |                    0 |\n",
      "| N/A   37C    P0            123W /  700W |       4MiB /  81559MiB |      0%      Default |\n",
      "|                                         |                        |             Disabled |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "|   1  NVIDIA H100 80GB HBM3          Off |   00000000:08:00.0 Off |                    0 |\n",
      "| N/A   31C    P0            124W /  700W |       4MiB /  81559MiB |      0%      Default |\n",
      "|                                         |                        |             Disabled |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "|   2  NVIDIA H100 80GB HBM3          Off |   00000000:0D:00.0 Off |                    0 |\n",
      "| N/A   37C    P0             72W /  700W |       4MiB /  81559MiB |      0%      Default |\n",
      "|                                         |                        |             Disabled |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "|   3  NVIDIA H100 80GB HBM3          Off |   00000000:12:00.0 Off |                    0 |\n",
      "| N/A   32C    P0             74W /  700W |       4MiB /  81559MiB |      0%      Default |\n",
      "|                                         |                        |             Disabled |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "|   4  NVIDIA H100 80GB HBM3          Off |   00000000:17:00.0 Off |                    0 |\n",
      "| N/A   31C    P0             71W /  700W |       4MiB /  81559MiB |      0%      Default |\n",
      "|                                         |                        |             Disabled |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "|   5  NVIDIA H100 80GB HBM3          Off |   00000000:1C:00.0 Off |                    0 |\n",
      "| N/A   35C    P0             71W /  700W |       4MiB /  81559MiB |      0%      Default |\n",
      "|                                         |                        |             Disabled |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "|   6  NVIDIA H100 80GB HBM3          Off |   00000000:21:00.0 Off |                    0 |\n",
      "| N/A   38C    P0             71W /  700W |       4MiB /  81559MiB |      0%      Default |\n",
      "|                                         |                        |             Disabled |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "|   7  NVIDIA H100 80GB HBM3          Off |   00000000:26:00.0 Off |                    0 |\n",
      "| N/A   29C    P0             69W /  700W |       4MiB /  81559MiB |      0%      Default |\n",
      "|                                         |                        |             Disabled |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "\n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI              PID   Type   Process name                        GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "|  No running processes found                                                             |\n",
      "+-----------------------------------------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import subprocess\n",
    "\n",
    "def check_gpu_environment():\n",
    "    \"\"\"\n",
    "    현재 시스템의 GPU 상태를 상세히 확인하는 함수다.\n",
    "    \"\"\"\n",
    "    print(\"=\" * 40)\n",
    "    print(\"GPU 환경 점검\")\n",
    "    print(\"=\" * 40)\n",
    "\n",
    "    # 1. CUDA 사용 가능 여부 확인\n",
    "    if not torch.cuda.is_available():\n",
    "        print(\"경고: CUDA를 사용할 수 없다. CPU만으로는 Fine-tuning이 불가능하다.\")\n",
    "        return False\n",
    "\n",
    "    # 2. GPU 장치 정보 출력\n",
    "    gpu_count = torch.cuda.device_count()\n",
    "    print(f\"발견된 GPU 개수: {gpu_count}\")\n",
    "\n",
    "    for i in range(gpu_count):\n",
    "        device_name = torch.cuda.get_device_name(i)\n",
    "        # VRAM 용량 계산 (GB 단위)\n",
    "        total_memory = torch.cuda.get_device_properties(i).total_memory / (1024 ** 3)\n",
    "        print(f\"GPU {i}: {device_name} | VRAM: {total_memory:.1f} GB\")\n",
    "\n",
    "    # 3. 소프트웨어 버전 확인\n",
    "    print(f\"\\nPyTorch 버전: {torch.__version__}\")\n",
    "    print(f\"CUDA 버전: {torch.version.cuda}\")\n",
    "\n",
    "    return True\n",
    "\n",
    "# nvidia-smi 명령어로 현재 상태(온도, 전력 등) 확인\n",
    "def print_nvidia_smi():\n",
    "    try:\n",
    "        # 쉘 명령어 실행\n",
    "        result = subprocess.run(['nvidia-smi'], capture_output=True, text=True)\n",
    "        print(\"\\n[nvidia-smi 출력 결과]\")\n",
    "        print(result.stdout)\n",
    "    except FileNotFoundError:\n",
    "        print(\"nvidia-smi 명령어를 찾을 수 없다. 드라이버 설치를 확인해야 한다.\")\n",
    "\n",
    "check_gpu_environment()\n",
    "print_nvidia_smi()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gjTBjKQzVkUc"
   },
   "source": [
    "## 4. 필수 라이브러리 설치\n",
    "\n",
    "LLM Fine-tuning을 위해 필요한 핵심 라이브러리들을 설치하고 버전을 확인한다.\n",
    "\n",
    "### 4.1 핵심 라이브러리 목록\n",
    "\n",
    "* **transformers**: 모델 로드 및 토크나이징을 위한 표준 라이브러리.\n",
    "* **peft**: LoRA, QLoRA 등 효율적인 파라미터 학습을 지원한다.\n",
    "* **bitsandbytes**: 4-bit/8-bit 양자화를 위한 필수 라이브러리 (GPU 전용).\n",
    "* **accelerate**: 복잡한 분산 학습 및 하드웨어 최적화를 쉽게 처리한다.\n",
    "* **trl**: SFT, DPO 등 LLM 학습에 특화된 고수준 트레이너를 제공한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "fy-dyDFqVkUc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "\n",
      "라이브러리 버전 확인:\n",
      "------------------------------\n",
      "torch          : 2.6.0+cu126\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/josh/anaconda3/envs/llm/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "transformers   : 4.57.3\n",
      "datasets       : 4.4.2\n",
      "accelerate     : 1.12.0\n",
      "peft           : 0.18.0\n",
      "trl            : 0.26.2\n",
      "bitsandbytes   : 0.49.0\n"
     ]
    }
   ],
   "source": [
    "# 라이브러리 설치 명령어 (주석 해제 후 실행)\n",
    "# %pip install -q torch torchvision torchaudio\n",
    "%pip install -q transformers datasets accelerate peft trl bitsandbytes\n",
    "%pip install -q wandb\n",
    "\n",
    "def check_library_versions():\n",
    "    \"\"\"\n",
    "    주요 라이브러리의 설치 여부와 버전을 확인한다.\n",
    "    \"\"\"\n",
    "    packages = [\n",
    "        \"torch\", \"transformers\", \"datasets\", \"accelerate\",\n",
    "        \"peft\", \"trl\", \"bitsandbytes\"\n",
    "    ]\n",
    "\n",
    "    print(\"\\n라이브러리 버전 확인:\")\n",
    "    print(\"-\" * 30)\n",
    "\n",
    "    for package in packages:\n",
    "        try:\n",
    "            module = __import__(package)\n",
    "            version = getattr(module, \"__version__\", \"알 수 없음\")\n",
    "            print(f\"{package:<15}: {version}\")\n",
    "        except ImportError:\n",
    "            print(f\"{package:<15}: 설치되지 않음 (설치 필요)\")\n",
    "\n",
    "check_library_versions()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_qDuBMzkVkUc"
   },
   "source": [
    "## 5. HuggingFace 설정\n",
    "\n",
    "많은 최신 모델(LLaMA 3, Gemma 등)은 HuggingFace Hub의 접근 권한 승인이 필요하다. 토큰을 사용하여 로그인한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "bKO4qQT8VkUd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "HuggingFace 로그인 설정\n",
      "로그인 성공: nowave (Type: user)\n"
     ]
    }
   ],
   "source": [
    "from huggingface_hub import login, HfApi\n",
    "\n",
    "def setup_huggingface(token=None):\n",
    "    \"\"\"\n",
    "    HuggingFace Hub에 로그인하고 상태를 확인한다.\n",
    "    token이 없으면 대화형 로그인을 시도한다.\n",
    "    \"\"\"\n",
    "    print(\"\\nHuggingFace 로그인 설정\")\n",
    "\n",
    "    # 토큰이 제공되면 비대화형 로그인, 아니면 대화형 로그인\n",
    "    if token:\n",
    "        login(token=token)\n",
    "    else:\n",
    "        # 주석 해제 후 토큰 직접 입력 가능\n",
    "        # login()\n",
    "        print(\"참고: 코드로 로그인하려면 login(token='hf_...')을 사용한다.\")\n",
    "\n",
    "    # 로그인 상태 확인\n",
    "    try:\n",
    "        api = HfApi()\n",
    "        user_info = api.whoami()\n",
    "        print(f\"로그인 성공: {user_info['name']} (Type: {user_info['type']})\")\n",
    "    except Exception as e:\n",
    "        print(f\"로그인 상태 확인 실패: {e}\")\n",
    "        print(\"HuggingFace 설정 페이지에서 Access Token을 발급받아 입력해야 한다.\")\n",
    "\n",
    "# 실제 사용 시 토큰 입력\n",
    "setup_huggingface(\"hf_허깅페이스에서 발급 받은 Key Token을 붙혀 넣는다\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pRtwGWSYVkUd"
   },
   "source": [
    "## 6. 캐시 및 저장 경로 설정\n",
    "\n",
    "대규모 모델은 용량이 크기 때문에 기본 경로(홈 디렉토리)에 저장하면 용량 부족 문제가 발생할 수 있다. 넉넉한 공간이 있는 별도 경로로 설정하는 것이 좋다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3xj7Cr5EVkUd"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# 프로젝트 루트 경로 설정\n",
    "PROJECT_ROOT = \"./gpt-oss-20b-finetuning\"\n",
    "MODELS_DIR = os.path.join(PROJECT_ROOT, \"models\")\n",
    "DATA_DIR = os.path.join(PROJECT_ROOT, \"data\")\n",
    "OUTPUT_DIR = os.path.join(PROJECT_ROOT, \"outputs\")\n",
    "\n",
    "# HuggingFace 캐시 경로 변경 (필요 시)\n",
    "# os.environ[\"HF_HOME\"] = \"/path/to/large/disk/huggingface\"\n",
    "\n",
    "def setup_directories():\n",
    "    \"\"\"\n",
    "    프로젝트에 필요한 디렉토리 구조를 생성한다.\n",
    "    \"\"\"\n",
    "    dirs = [MODELS_DIR, DATA_DIR, OUTPUT_DIR]\n",
    "\n",
    "    print(\"\\n디렉토리 설정:\")\n",
    "    for d in dirs:\n",
    "        os.makedirs(d, exist_ok=True)\n",
    "        print(f\"생성/확인 완료: {d}\")\n",
    "\n",
    "    print(f\"현재 HF_HOME: {os.getenv('HF_HOME', '기본 경로 (~/.cache/huggingface)')}\")\n",
    "\n",
    "setup_directories()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gnrlaw5FVkUd"
   },
   "source": [
    "## 7. 모델 로드 테스트 (환경 검증)\n",
    "\n",
    "모든 설정이 올바른지 확인하기 위해 작은 모델(`gpt2`)을 로드하여 간단한 추론을 실행해본다. `bitsandbytes`가 정상 작동하는지도 이 단계에서 확인할 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "JJnu8LzGVkUd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "환경 검증 테스트 (GPT-2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "입력: Fine-tuning is\n",
      "출력: Fine-tuning is a very important part of the process.\n",
      "\n",
      "\n",
      "검증 성공: PyTorch, Transformers, GPU가 정상 작동한다.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "def test_environment_with_small_model():\n",
    "    \"\"\"\n",
    "    작은 모델(GPT-2)을 로드하여 라이브러리와 GPU가 정상 작동하는지 검증한다.\n",
    "    \"\"\"\n",
    "    print(\"\\n환경 검증 테스트 (GPT-2)\")\n",
    "    test_model_name = \"gpt2\"\n",
    "\n",
    "    # 1. 토크나이저 로드\n",
    "    tokenizer = AutoTokenizer.from_pretrained(test_model_name)\n",
    "\n",
    "    # 2. 모델 로드 (GPU 사용)\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        test_model_name,\n",
    "        device_map=\"auto\"\n",
    "    )\n",
    "\n",
    "    # 3. 추론 테스트\n",
    "    input_text = \"Fine-tuning is\"\n",
    "    inputs = tokenizer(input_text, return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "    outputs = model.generate(**inputs, max_new_tokens=10)\n",
    "    result = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "    print(f\"입력: {input_text}\")\n",
    "    print(f\"출력: {result}\")\n",
    "    print(\"검증 성공: PyTorch, Transformers, GPU가 정상 작동한다.\")\n",
    "\n",
    "# 테스트 실행 (오류 발생 시 설치 과정 재확인 필요)\n",
    "test_environment_with_small_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "soJVKe2cVkUe"
   },
   "source": [
    "## 8. W&B 설정 (선택 사항)\n",
    "\n",
    "Weights & Biases(W&B)는 학습 과정을 실시간으로 모니터링하고 시각화하는 강력한 도구다. 필수는 아니지만 강력히 권장한다.\n",
    "- https://wandb.ai 에서 회원가입"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "EiPmkMxuVkUe"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Failed to detect the name of this notebook. You can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "W&B(Weights & Biases) 설정\n",
      "1. https://wandb.ai/authorize 에서 API 키 복사\n",
      "2. wandb.login() 실행 후 키 입력\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mkubwai\u001b[0m to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    }
   ],
   "source": [
    "import wandb\n",
    "\n",
    "def setup_wandb():\n",
    "    \"\"\"\n",
    "    W&B 로그인 안내를 출력한다.\n",
    "    \"\"\"\n",
    "    print(\"\\nW&B(Weights & Biases) 설정\")\n",
    "    print(\"1. https://wandb.ai/authorize 에서 API 키 복사\")\n",
    "    print(\"2. wandb.login() 실행 후 키 입력\")\n",
    "    wandb.login()\n",
    "\n",
    "setup_wandb()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "P-_0Ft5HVkUe"
   },
   "source": [
    "## 9. 요약\n",
    "\n",
    "이 챕터에서는 Fine-tuning을 위한 물리적, 소프트웨어적 기반을 마련했다.\n",
    "\n",
    "1. **GPU 확인**: QLoRA 학습을 위해 최소 24GB VRAM(RTX 3090/4090)을 권장한다.\n",
    "2. **라이브러리**: `peft`, `bitsandbytes`, `trl` 등 최신 Fine-tuning 스택을 설치했다.\n",
    "3. **검증**: 작은 모델 구동 테스트를 통해 환경이 정상임을 확인했다.\n",
    "\n",
    "이제 모든 준비가 완료되었다. 다음 챕터부터는 실제 `gpt-oss-20b` 모델을 로드하고 본격적인 Fine-tuning 과정을 시작한다.\n",
    "\n",
    "다음 챕터는 **Chapter 05: gpt-oss-20b 모델 로딩**으로, 대규모 모델을 4-bit로 양자화하여 메모리에 올리는 방법을 다룬다.\n",
    "\n",
    "---\n",
    "\n",
    "**Chapter 04 생성이 완료되었다. 다음 챕터(Chapter 05)로 진행할까요?**"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "graph",
   "language": "python",
   "name": "graph"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
